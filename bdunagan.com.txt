I worked on a campus magazine, Counterpoint, in college. While brainstorming ideas for a fundraiser, I came up with shotglasses with "fill the void" written across them. It seemed like an appropriate title for a blog. There are a lot of great blogs already, but they don't cover everything; I'd like to fill the void.

However, badges are just as prevalent in current Cocoa applications. They are the small numbers on the right side of source lists, letting you know how many unread emails or new podcasts you have. Yet, I've only found one tutorial for how to create them. While it's an excellent overview, it's written in Java, so it doesn't directly port to Objective-C. Fortunately, it's not difficult, and I'm including the source code from BDBadgeCell.m below for anyone to use (MIT license). I used NSBezierPath to create the oval and a bit of string drawing code to produce the element. Check out the screenshots below.

I included a screenshot of Apple's Mail badges for reference; the font is slightly different, and I'd appreciate any pointers to get the look exactly right. On the left is Apple's Mail badges, and on the right is my BDBadge badges. The two applications are in focus in the top row and out of focus in the bottom row.

Add the following code to your cell subclass. (I subclassed NSBrowserCell.)

iTunes has small horizontal arrow icons integrated into many interfaces, in Music, Podcasts, and the Store. They're in other applications too, such as Instruments and Delicious Library. The HIG briefly touches on these arrows but doesn't name them, so I call them link arrows, if only to distinguish them from arrow keys.

I did a quick Google search for hints on how to recreate these in Cocoa, but only found lots of posts from 2004 on changing their behavior in iTunes. Oddly, that tip was rediscovered a couple days ago.

Fortunately, the link arrows aren't difficult to wire up in Interface Builder. Just drop an NSTableView object into your NIB and then drop an NSButtonCell onto the column where you'd like the link arrow to appear. Configure its Attributes as shown below. Be sure to associate the cell's selector with an action. Populate the table, and the arrows should appear on the right side of the column.

Of course, if you'd prefer using Cocoa and Xcode, you can subclass NSButtonCell and add the following lines to the init method.

One common need is to make dates within the last week more readable, like using "Today" instead of "September 13". Apple Mail and NetNewsWire both do this. When using Cocoa Bindings, the easiest path to this is through an NSValueTransformer. I initially tried to use NSCalendarDate (following this example), but apparently Apple might deprecate that object in 10.6. So instead I went with NSDateComponents.

Below is code from BDDateTransformer. It converts appropriate dates into "Tomorrow", "Today", "Yesterday", and the past five days of the week. Feel free to use this wherever.

I went to the gas station yesterday to fill up my car. It was the normal routine until I squeezed the pump handle and nothing happened. I tried it again, took the pump out, put it back in, squeezed. Nothing. After half a minute of this, the machine beeped and asked if I'd like my receipt. I said yes. It had charged my card $82, the amount the last person at the pump had paid.

I spent five minutes with the woman behind the gas station register, going back and forth about what exactly I did. Finally, she looked at my receipt and looked at her printout of credit card charges. No match. The gas pump machine had given me a receipt, but it hadn't actually charged my card. I had no idea those two processes weren't an atomic transaction.

I started programming in Objective-C and Cocoa in April. To familiarize myself with the language and framework, I wrote an RSS reader, and I had planned to throw that code away as soon as I felt comfortable building a real application. Of course, then I thought it would be fun to release that reader to the public. So, here it is: RssBucket (for 10.5). I'm making the source code available as well (for Xcode/IB 3.1), under the MIT license, so feel free to use it wherever.

My previous Cocoa tutorials about link arrows, badges, and dates were extracted from the application. It has a standard source list, with feeds, favicons, and counts. To the right is a list of the selected feed's entries, with a more human-readable date and a link arrow to open the entry in the default web browser. Below the list is a web view that opens the URL for the selected entry. When you shut the application down, it saves off the feed list in your Preferences using NSUserDefaults. And updating occurs on a different thread from the UI.

RssBucket is very much a sample application. It works, but if you want a fully-functional Mac RSS reader, you should check out the excellent and free NetNewsWire.

Now that the NDA is lifted, I can post a handy iPhone dev trick. Craig Hockenberry posted a twitter a while back recommending you trigger didReceiveMemoryWarning in your app to work out any memory issues. And being able to trigger that warning in the iPhone Simulator, rather than on the device, would make debugging easier.

However, getting your warning to propagate through your objects like a real warning does is a bit tricky. You have to use an undocumented notification message: UIApplicationMemoryWarningNotification. You can wrap it in a method like so.

Putting that method on a timer allows you to trigger a low memory warning throughout your application at a regular interval. For some reason, it doesn't trigger the warning in your base controller, but you can easily add that class as an observer to that notification.

This process should allow you to trigger a didReceiveMemoryWarning notification throughout your iPhone application just like a real warning. The fact that it's undocumented is okay because it's only for testing purposes.
Clay Shirky gave an excellent talk at the 2008 Web 2.0 Expo NY entitled "It's Not Information Overload. It's Filter Failure." Referring to the world before the internet, he said, "the inefficieny of information flow wasn't a bug. It was a feature." We can all relate to the feeling of information overload, but Shirky gave a compelling argument that we've always had to deal with information overload. The problem is the filters we used to rely on are structurally broken for the current era of overload.

Aaron Johnson posted a great analogy: "We need better distilleries, not better harvesters." It's simple to collect more information; we've gotten very good at that. The next step is distilling that information down into something easily consumable. That's where companies like FriendFeed (for RSS feeds) and Xobni (for email) come in. They're trying to solve the problem of filtering. How do we want email prioritized? How do we see the most important items first? While those guys refine their filtering systems, the best solution is to just ask yourself, "What do I actually care about?" and let the rest go. At least that's what I tell myself when I stare at the 173 feeds in my Google Reader.





In 2006, Apple began an excellent set of ads called "Get a Mac". They're an excellent example of a #2 company repeatedly attacking a #1 company. Classic marketing books like Positioning by Ries and Trout highlight how important it is for companies like Apple to directly attack Microsoft, just as Pepsi attacked Coca-Cola and Avis attacked Hertz. Apple has grown its US PC market share to 10%, but Microsoft continues to dominate the rest of the market. It's a smart, brilliantly executed advertising strategy. You can watch most of the ads on Apple's website.

Microsoft is the 800-pound gorilla of the computer industry. And yet, they have done a surprisingly bad job at advertising. Vista was years late in launching, and the best ad they could come up with was "Wow". More recently, they took a different tactic: the Mojave Experiment, where they initially tricked a group of people into thinking they were looking at a prototype OS, before revealing that the OS was Vista. Both seem like wastes of money. Apple needs to advertise to convince the public to buy something other than Windows; Microsoft doesn't need to convince people to buy Windows, just as Google and Amazon don't advertise their main services.

Microsoft does have the capacity for brilliance. Two years ago, an internal marketing video was posted to YouTube, entitled "Microsoft iPod". The video was supposed to make the larger marketing department wake up and start selling elegance, and it was so popular on the web because it succinctly contrasted Microsoft's approach to product marketing with Apple's approach. They're self-aware of their weaknesses, but they haven't changed.

All of which brings me to Microsoft's latest misstep. In September, the company decided to spend $300M on a set of ads called "Windows without Walls". They began with two spots with Bill Gates and Jerry Seinfeld; it was reported that they paid Seinfeld $10M for his role. These ads succeeded in getting a lot of media attention. They were quirky and pointless, and I assume both were intentional. But it's believed that Microsoft prematurely ended this series because of the publicity, and they transitioned to ads directly acknowledging Apple's ads. A real Microsoft employee (who looks remarkably like Apple's PC caricature) starts it off by saying, "I'm a PC, and I've been made into a stereotype," and then a whole slew of PC users say variants of "I'm a PC". The only redeeming quality of the ads is the real people they show. While Apple took a great idea and executed it well, Microsoft had multiple ideas and executed them badly in concert. They directly addressed Apple's ads and tried to defend themselves by getting real people to read from a script. Microsoft shouldn't have acknowledged Apple or its ads at all. Politicians don't address issues the opponent brings up; they change the subject to ones where they shine. Apple now has new ads poking fun at Microsoft's ad campaign.

So I wondered what I would do in Microsoft's position. I have $300M to spend on advertising and well-known products that have well-known issues. Simple: let the people tell defend us. Microsoft had the right idea using real people in their ads. They just executed the idea badly. The company shouldn't try to manage the story. Instead, just let people tell their own stories, and turn the best into ads. Microsoft should have made a game out of it: "$1K for 1K". Anyone can upload a 15-second or 30-second video to MSN Video (since Microsoft probably doesn't want these on YouTube). Anyone can vote on them. Microsoft will pick the best (not necessarily the highest voted) and turn them into TV and web ads. Anyone whose video gets picked gets $1K. I'm sure there is a price point around there to convince Windows lovers to pick up a camera and shoot 15 seconds.

The key is to focus on real people. Let them talk about what they like. Let them acknowledge problems, perhaps as reasonable solutions to difficult problems. Apple's ads can talk about viruses because the Windows install base is a larger vector than Mac OS X's. Some portion of the videos would probably attack Apple, pointing out these realities, like a small market share. The ad campaign would cost 10% of what Seinfeld cost, and yet it would be far more effective at putting a human face on the global brand. Moreover, just as Apple's ads scale well in terms of subject matter, Microsoft's ads could focus on any feature with a video to address it or any product they sell, like Office or Exchange.

Microsoft wants to appear more human. The solution to that is not hiring Seinfeld or handing regular people a three-word script. Give people an outlet, and I think they'll surprise you. Let them be the face of Microsoft.

Frasier Speirs posted a thought experiment last week about Apple giving away Mac OS X 10.6 "Snow Leopard" for free. Mac OS X 10.5 "Leopard" is sold for $129, but it's unclear from Apple's 10-K how many units they've sold. While I don't think that a free 10.6 is likely, I could see Apple choosing to sell the OS upgrade for a very low price point, like $30 for both desktop and server. Anyone interested in upgrading would upgrade for that amount.

Apple only makes 6% (~$2B) of its revenue from software and services, and that includes many products beyond OS X. And the company has $25B in cash now. Wall Street would love to see that money used in a stock buyback, but Jobs clearly stated in the last earnings call that the cash would be used on product development. Using its supply of cash to subsidize 10.6 purchases would certainly be more useful than a stock buyback program. Apple could cut support for 10.4 and 10.5 quicker and invest more resources into 10.6+ features.

Fortune's Apple 2.0 brought up a very interesting point about how Apple might deal with its cash: "Of course, Steve Jobs may have better ideas than Toni Sacconaghi about what $25 billion can do. The last time Apple's stock fell this sharply — plunging from nearly $40 a share in March 2000 to $7.44 in December 2000. Jobs used the cash he had on hand to start a chain of Apple Stores."

Google subsidizes an amazing variety of services with ad revenue. Perhaps subsidizing its operating system would make sense to Apple in its current financial position.

Unfortunately, while my first pass at link arrows functionally worked, it didn't work very well.

If you take the code from the original post, hook up setTitle: in a willDisplayCell: method, and connect the link arrow cell's selector to an action, you get this. The text for the link arrow cell is clearly different from the normal table cell, and clicking anywhere on the link arrow cell triggers the action. Bad.

Let's say you're okay with triggering the action by clicking anywhere in the cell. You could try to use setAttributedTitle: instead of setTitle. However, then when you highlight a row, the text doesn't turn white. Bad.

Perhaps you realize that you can fix the color by simply getting the current color of the original attributed title. My code for this has already turned into the following:

The text color works, both when highlighted and defocused. But, there is still the problem of clicking anywhere triggering the action. Bad.

I iterated through all of these before stepping back and rethinking my approach. The problem was leveraging NSButtonCell. While the cell type worked as a quick hack, it left a lot to be desired. It didn't handle clicks correctly. And I couldn't add an icon next to the text. Subclassing was necessary to get the cell to work just right. Instead of NSButtonCell, I subclassed NSTextFieldCell and composed in an NSButtonCell. That way, the title's font/size/color just worked. To get the objects in the right place in the cell, I simply overrode the cell's draw method and positioned them.

One difficulty was registering clicks. I wanted the user to be able to click on a link arrow to perform an action (like open a feed item in a browser). In 10.5, I found the method - (NSUInteger)hitTestForEvent:(NSEvent *)event inRect:(NSRect)cellFrame ofView:(NSView *)controlView. The resulting custom cell worked well and was extensible in case I wanted to add an icon next to the title. Check out the code below:


So close but not quite. My original post on source list badges contained code that produced almost the desired result. First off, Ken Orr pointed out that the correct font is Helvetica. That fixed the font, but look at the colors in the screenshots.

Clearly, the final image just looks wrong. The badge's text color is bright blue, rather than subtle blue. I couldn't figure out how to identify a defocused NSTableView last time, but tonight I finally found the correct call at CocoaDev. It utilizes NSResponder to see whether the NSTableView is in focus or not. So that enabled me to fix the color issues. Check out the actual implementation below and feel free to use it wherever.


So, I'm just releasing the source code, as it's only interesting to developers. FilePile was so straight-forward to code because Apple includes many standard APIs in the SDK, like NSFileManager. Just as it provides an API to access my Mac's file structure, the class allowed me to delve into the iPhone's file structure. The app provides a unique interface to let the user know where they are in the file structure, illustrated in the screenshot below. Clicking on a file shows a preview of it, either as text or an image thumbnail, and a list of its available attributes; attributes are easier to see in landscape mode. Check out these interesting paths:

One difficulty I had was supporting image preview because of the memory footprint. Previewing images in the Simulator is trivial in terms of memory, but the iPhone's environment required I manage the memory better. It was this path that led me to find the undocumented UIApplicationMemoryWarningNotification notification; using that allowed me to trigger low memory warnings in my application on the iPhone without actually running out of memory. Trying to debug a low memory condition on the iPhone with Instruments is extremely difficult.

Perhaps FilePile will get accepted to the App Store at some point. Until then, the project is available as a zip file and on Google Code.


I posted updated methods for doing source list badges and link arrows in NSTableViews, so I figured I should update the RssBucket code to reflect those. Rather than keep it as a zipped project on this site, I moved it over to Google Code. I also uploaded an updated app bundle.

Feel free to borrow any code from it you'd like. MIT license.
Recently I found myself needing to perform root-level operations. Two simple things. Writing to a directory that's root-owned and not group-writable (drwxr-xr-x root:wheel). And running a shell task with sudo. I had no experience with Mac OS X's security framework before this, so now with the operations working, I wrote up a quick Cocoa application and tutorial to help anyone else encountering these same issues. Apple's documentation and sample code are great, but I had trouble understanding how to use NSTask to perform the operations. Two caveats. I'm using 10.5, so the Xcode project is 3.1 and the security calls might have changed since 10.4 or before. Moreover, while I think my implementation follows Apple's recommended solution, I might be wrong. Someone let me know if that's the case.

Mac OS X's Security framework revolves around authorization. An application must obtain the user's authorization, through the familiar name/password dialog, to perform privileged operations. Privileged operations aren't necessarily root-level ones. They might be application-specific tasks, like viewing payroll information. By requiring authorization, an application can limit the user's ability to access confidential information or tasks. When the operation is a root-level one, Apple cautions developers to take special care to structure the application to minimize security risks. They recommend the code performing the root-level operations be factored out into a separate tool. The problem is the tool needs to run as root, but to run a program as root, the caller must be root. Apple's solution to this problem is AuthorizationExecuteWithPrivileges or AEWP as mailing lists refer to it. The method allows a non-root application to start another application as root. Apple thinks the best way to use this method is to start the tool in a non-root context and then have the tool call itself with AuthorizationExecuteWithPrivileges. That way, the caller application is never root, and the security exposure is minimized. Apple dives into much greater detail in their documentation; read through it before deploying an application with root-level operations. Be sure to check out their sample code: BetterAuthorizationSample and AuthForAll. (For the more ambitious, take a look at the BetterAuthorizationSample's Design and Implementation Rationale.)

I called the project BDAuthorize. It's a simple UI with two buttons: "Create Text File in /Library/LaunchDaemons" and "Duplicate Text File in /Library/LaunchDaemons". That folder matches the ownership and permissions I wanted to test. Under the hood is a helper tool that performs the privileged operations. The application creation is boiler-plate, but I'll go over the helper tool creation.

To create a helper tool in Xcode, I added a "Shell Tool" as a new Target ( "Project" -> "Add Target..." -> "Cocoa" -> "Shell Tool" ). I called mine "AuthHelperTool". I added "AuthHelperTool.m" as a new Objective-C class file, and then I dragged that file onto AuthHelperTool's "Compile Sources" under Targets. Be sure to remove the file from the analogous folder under BDAuthorize. Now the .m file will be compiled into AuthHelperTool. I wanted this target embedded in BDAuthorize's Resources folder, so I dragged AuthHelperTool under Products to BDAuthorize's "Copy Bundle Resources" under Targets. And as I didn't want to compile AuthHelperTool separately from BDAuthorize, I added AuthHelperTool as a dependency on BDAuthorize. To do this, I did "Get Info" on BDAuthorize under Targets, went to "General", and added the tool under "Direct Dependencies".

One subtle issue I ran into was credential caching. To test a sudo call, I tried it in Terminal before trying it in the helper tool. However, sudo caches credentials for a period of time, so my helper tool succeeded for a while and then started failing. To flush sudo's cache, just type sudo -k in Terminal. Flushing allowed me to test sudo calls myself and test the helper tool in a pristine state.

Although I performed a simple copy in this example, I could have easily run a launchd command like "sudo launchctl load plist" or any other root-level operation. The security framework takes a bit to understand, but structuring an application to leverage it is straight-forward.

Feel free to either download the zip file or browse the code on Google Code.
Although I've been using Cocoa Bindings for a bit now, I just recently realized how key-value observing (KVO) works. It observes paths, not data. For example, let's say I have NSString *foo in Bar class. Other classes can get the value with [bar valueForKeyPath:@"foo"]. But if I can set the value with foo = @"123", the change doesn't propagate. Only by calling [self setFoo:@"123"] is KVO leveraged. See Apple's documentation for more information.

Morever, what the getter/setter methods actually do is irrelevant from the observer's perspective. What's important is that whenever the setter is called, any interface elements observing that path are alerted, and they call the getter next. So, as a simple example, below is a class for displaying the date. An NSTimer calls setDate: every second, but that method doesn't actually do anything. Callling that setter method can be a quick way to alert the interface that the underlying data has changed, even if the setter is not the method changing it.

To see this code work, go to Interface Builder and drop in an NSTextField. Hook up the Bindings like you see below.

Below is the result.

Spinning wheels (NSProgressIndicator objects in Cocoa) are a common UI element in, well, all software these days. People like to know if applications are busy doing something and, if so, how much longer they'll be. Web browsers, for instance, have continuous spinning wheels to indicate that they're still downloading the requested page and discrete progress bars to show how much of a file they've already downloaded.

In Cocoa, NSProgressIndicator is easily placed into an NSView using Interface Builder, but embedding the object in a list (NSTableView or NSOutlineView) takes a bit of coding. CocoaDev discusses it, but I thought I'd throw together a simple project demonstrating it, pictured above. It's available on Google Code and as a zipped file (Xcode 3.1).
At work, we're using Apple's built-in PackageMaker in our automated nightly builds to produce installer packages. The command-line version is very handy as we can incorporate it into scripts run every night with launchd, and for complicated package specs, the GUI version can save off a pmdoc bundle (a collection of XML files) that the command-line version can use. Still, we've run into two issues to keep in mind when using PackageMaker.

Coming from Windows, Mac OS X's ability to go "find" files scattered around the hard drive is magic. (No, I don't count COM.) Installers can upgrade an application bundle even if it's not in /Applications. Amazingly neat. And horribly frustrating if you're unaware the installer can do that. We have an application bundle and System Preferences pane bundle with the same name, and for a bit, we were scratching our heads over why the application installer kept "upgrading" the System Preferences pane when we wanted it to just place the bundle in /Applications.

The issue was PackageMaker's default. By default, it relocated the bundle it was installing to wherever a previously installed bundle by the same name was. So, an RssBucket installer would replace RssBucket.app (or RssBucket.prefPane), not just if it was in /Applications but anywhere on the file system. Apple's Console application (/Applications/Utilities/) can show you what the installer package actually did in installer.log; search for "diverting" for relocation issues. Unchecking "Allows Relocation" in the PackageMaker GUI disables this feature.

PackageMaker has another interesting default. When the GUI creates a pmdoc bundle, it caches each component's version in the XML file. It's a strange default because when you use the pmdoc in the command-line PackageMaker, the created installer still has that cached version. So, when used in a nightly build with agvtool incrementing the build, the created installer has the version of the application used to create the pmdoc bundle, not the version of the supplied application bundle. Result: On the third day, the installer fails to upgrade. install.log says, "Application (version 1.0.0.1) was not installed because a newer version (1.0.0.2) already exists." (I can't remember if the second day works because the versions match.)

Simply removing the version from the XML file in the pmdoc bundle solves this problem. The installer instead relies on Info.plist to determine the version. Caching the version is an odd default, but luckily, it's easily fixed.
While writing FilePile, I wanted to use a UITextView in a UITableView, both subclasses of UIScrollView. The file information view needed a preview section and a list of attributes, so the two classes worked well together for previewing a text file and showing its file attributes. However, the UITextView was regularly blank, on the iPhone Simulator and the device. Scrolling the view would cause its contents to appear. This UI bug was especially problematic when I had a text preview that didn't stretch beyond the frame. It didn't scroll, so it wouldn't appear just by touching it; the view was just blank until it was drawn again. See the screenshot below.

The problem is easily reproducible with Apple's UICatalog. Just make the following two changes.

* TextViewController.m:86: change "myTableView.scrollEnabled = NO;" to "myTableView.scrollEnabled = YES;"
* TextViewController.m:146 change "return 1;" to "return 10;"

Launch the project, and click on "TextView". Slowly scroll down the screen, and you'll see that the contents of the UITextViews don't consistently appear. I wrote a quick sample project to demonstrate it. You can find it on Google Code or download it as a zip file. Below is a short movie showing the problem.

I filed a bug (rdar://6429549) with Apple about the issue, but in the mean time, I found a couple ways to deal with it: reuse and setNeedsLayout. UICatalog doesn't reuse the UITextView in TextViewController. Understandable given UICatalog only used the view once, but for these purposes, reuse is a good idea. This change cuts down on the number of UITextViews that are blank. Now when I scroll one of the blank views, that reused view appears correctly from then on, implying the problem was in initialization.

Still, reuse doesn't solve my original problem. I only had a single UITextView in FilePile's file view, and I want it to always show up, without requiring a scroll event. Fortunately, a simple NSTimer in the UITableView's datasource method solves the issue. In - (UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath, just call  userInfo:nil repeats:NO]; when the cell is a UITextView (or UIScrollView subclass). Below is the full method.

With that code in place, here is a movie of the result.

Supporting multiple languages in a Mac app, known as internationalization or localization, is very easy on Mac OS X. Before we dig into the exact process, I'll briefly cover what makes up a Mac app. Although they appear to be atomic items in Finder, Mac applications are actually folders containing various resources. (Right-click on an application and select 'Show Package Contents' to explore one.) In the Contents/Resources folder, there are folders with .lproj extensions. For example, Calculator.app has English.lproj, French.lproj, zh\_CN.lproj, and fifteen others. Each of these .lproj folders contains UI files (NIBs, compiled down from XIBs) and strings (InfoPlist.strings and Localizable.strings that NSLocalizedString leverages) translated into that specific language. By simply changing the language in System Preferences->International, I can switch an app from English to French. Very neat.

When I first write a Mac application, the Resources folder only has an English.lproj folder in it. And the only reason it does is Xcode 3.1.2 defaults to localizing MainMenu.xib and InfoPlist.strings. When I create a new XIB, it's not localized; I can localize it by right-clicking on it, selecting 'Get Info', and clicking 'Make File Localizable'.

And I can click 'Add Localization' to add a French localization for that XIB. (Interface Builder suggests 'French', 'German', etc, but Apple recommends using the two-letter ISO 639-1 spec.)

It duplicates the XIB into French.lproj, creating the folder if it doesn't exist. This process sounds convenient, and it's absolutely necessary as a way to store localization knowledge in the project file. But it's only meant as a first step. If I have ten languages, I don't want to maintain ten copies of every XIB; I want to maintain one copy of every XIB and ten files with localized strings. Luckily, Apple provides this slick workflow through the command-line program ibtool.

Go to Terminal, find or create a MainMenu.xib, and type



That's the obvious part. It seems easy to extract strings with IDs from a XIB. I can send these .strings files to a localizer for translation. Here's the awesome part: The same tool lets me create a new XIB with the original XIB and a modified .strings file. Go through the MainMenu.strings file we just generated, and change all the strings to something different. Go back to Terminal, create French.lproj if it doesn't exist, and type

A new XIB is created in French.lproj with same UI elements as English.lproj but with the new strings. I can use these same commands in scripts to automate extracting the .strings files and re-generating the localized XIBs. Below are my two Ruby scripts for these tasks, both MIT license. Let me know if you have any issues with them.


An interesting datapoint is SproutCore, the web app technology MobileMe is built on, also uses .lproj folders for localization. Isolating UI and localization in this way is a fantastic design pattern, so I'm happy to see it extended to the web world. Frankly, I have no idea how other websites and web apps handle localization. Perhaps they have a similar method.

System Preferences on Mac OS X is a perfect place to put preferences and controls for a background process or framework. Instead of cluttering the user's dock and task switcher with running application or filling their menu bar with little icons, a preference pane allows developers to hide any visual indication of their program until the user is interested in it. On my mac, I have a couple preference panes from third-party developers: BusySync, Growl, and MySQL. All three fit nicely in there; they are background processes or frameworks that I don't want to see unless I want to fiddle with them. Apple has documentation on preference panes and their Cocoa representation, NSPreferencePane.

One issue that quickly arises is debugging. I didn't find it immediately obvious how to hit a breakpoint in my preference pane Xcode project. It turns out to be only a little more complicated than typical debugging: I need to launch System Preferences myself through Xcode after my preference pane is installed to debug it. Here is the step-by-step process.

Let's start by creating a preference pane Xcode project. In Xcode, select "File"->"New Project...". Choose "Standard Apple Plug-ins" and then the "PreferencePane" item. Name the project "SamplePrefPane".

Add the following line of code to the SamplePrefPanePref.m:mainViewDidLoad and set a breakpoint on it:
NSLog(@"It works!");

Next, select "Project"->"New Custom Executable...". In the dialog, give the executable a name (this name is what identifies it in Xcode's source list), and enter the following executable path: /Applications/System Preferences.app.

Then, build the project and install the preference pane, located in the "Products" folder. (Double-clicking the built prefPane installs it.) System Preferences will appear and ask whether to install it for one user (in ~/Library/PreferencePanes) or for all users (/Library/PreferencePanes). I generally select all users. After installing the prefPane, close System Preferences.

Finally, back in Xcode, select "Build"->"Build and Debug". System Preferences should appear with the new prefPane along the bottom. Click the prefPane. Xcode should hit the breakpoint placed at NSLog() in mainViewDidLoad.


On Mac OS X Leopard, AppKit's release notes mention a great NSTableView feature: "Hitting Space will attempt to 'performClick:' on a NSButtonCell in the selected row, if there is only one instance in that row." I'm not sure if this trick was built-in for free in previous OS version, but it's a convenient keyboard action to quickly move through a workflow.

One NSButtonCell is nice, but having the space bar work with two NSButtonCells in an NSTableView row would be fantastic. I didn't think that code should be difficult. I realized I could enable that action through custom code. There were two potential paths: using a custom selector on the delegate or leverage performClick: on the NSButtonCell. I posted a small sample app on Google Code to illustrate both paths.

For the first path, I create a NSTableView subclass and override the keyDown: method to catch the space bar key stroke; then, I send a selector to the subclass's delegate. The delegate identifies which NSTableView called it and does the appropriate action. This path doesn't leverage any bindings the NSTableColumn might have, but it works.

And yet, what I want is to call performClick: on the NSButtonCell within the keyDown: method in the subclass. The release notes indicate that AppKit takes this approach, and I can then leverage bindings. Elegant. Unfortunately, I can't figure out how.

As the sample app shows, I can get as far as sending performClick: to the seemingly appropriate NSButtonCell. But nothing happens. To compare the default NSTableView path and my path, I subclassed NSButtonCell and put a breakpoint in performClick:. Then I removed the second NSButtonCell, so that the trick discussed in the release notes would trigger. I found an interesting distinction. When I call

I use one instance of NSButtonCell. But when I call


I use a different instance. I verified using po in gdb.


The call stack is slightly different. In the second instance, the keyDown: selector leads to _attemptToPerformClickOnFocusedColumn. That internal method finds the right NSButtonCell and sends it the performClick: selector. Somehow, the paths lead to different cells. This second path leads to the correct cell; it fires the column's bindings and updates the underlying object's state, which is then reflected in the button's state. But the cell in my path isn't valid. I even tried playing with its state without any bindings, but the button's state never changed.

Frustrating. Maybe I'm just doing it wrong.
When I wrote the first post on localization, I hadn't discovered the full power of ibtool or the need for all that power. I figured out how to re-generate the localized XIBs from an English version and a .strings file. But soon, I needed to incrementally incorporate changes from the English XIBs, without blowing away the previous localized XIBs. The most common reason is element sizing. Text translated from English to languages like Spanish dramatically increases in length, but the UI elements in the English version are fit to the English text. Luckily, ibtool solves this problem as well.

Apple's documentation has a great explanation for solving this problem, summed up in the code below.

The ruby script from the earlier post is a little more complicated because I need multiple versions of the English XIB. The new process involves a bit of manual work, unless I incorporate SVN into it. However, it's worth it. ibtool lets me push incremental updates into localized XIBs without losing past changes to them.

I considered implementing a more dynamic interface, where a custom NSView subclass governed how the elements were laid out in any given view. Sofa, the talented group behind Checkout, Versions, and Disco, went that direction according to an Apple interview: "We generate all of Checkout's localization resources at build-time and dynamically resize views to fit oversized strings." That approach was tempting, but I wasn't sure how to deal with the many variants of views I had.

One issue I encountered before I found genstrings was not having translations for all the strings I called in NSLocalizedString. Apple has a nice argument to add to an application to point these out: NSShowNonLocalizedStrings. So, to look for non-localized strings in TextEdit, I would type:
/Applications/TextEdit.app/Contents/MacOS/TextEdit -NSShowNonLocalizedStrings YES

Apple has more information about this trick.

Just a couple hints for working in Xcode and Interface Builder.



- (void)doThis:(id)obj1 andThat:(id)obj2;
must be
@selector(doThis:andThat:)
whereas
- (void)doOtherThing;
becomes
@selector(doOtherThing)



Using this design pattern translates to much less code than the traditional boiler-plate approach does. But at first glance, we seem to sacrifice control for conciseness; the view-model connection bypasses the controller. Fortunately, NSTableView implements the NSTableDataSource protocol, and it provides a handy hook: setObjectValue:. This selector is fired after every model update, so by implementing it in the controller, we can find out when a model instance changes. While this hook is generally useful, I find it very nice to handle model dependencies.

There are two types of model dependencies: ones between a single instance's properties and ones between multiple instances' properties. I wrote up a small project (Mac OS X 10.5, Xcode 3.1.2), NSTableView_setObjectValue, to illustrate these. To link a single instance's properties in BDObject, I link isChecked with status.

When I click on a checkbox in the running application, the checkmark toggles and the status column instantly reflects that change. Without the setStatus: call, the view wouldn't know the status property had been updated. To link multiple instances' properties, I implement tableView:setObjectValue:forTableColumn:row: in the controller. After a model instance's property is updated, this selector is fired, and I can update the other model instances to reflect this change. (Remember that the table's dataSource property must be connected to the controller object in Interface Builder.)

Now, when I click a checkbox in the running application, the associated status is updated, along with every other model instance's checkbox and status, linking the instances together. And we didn't need any boiler-plate code to achieve this. NSTableView's setObjectValue: selector elegantly inserts the controller into the view-model bindings path.


Following on my last post, Core Animation on the Mac, I ported the animation code to the iPhone.

  
    
    
  

In this case, I mocked up the view in Mail where the user moves a message to another folder. There is a bar along the top for the icon (a Star Trek one from the Iconfactory) and a short description. Below it is a UITableView list of items. When the user clicks on an item, the icon flies into the item, and then the item pulses. Instead of duplicating Mail's blue pulse, I reused the Yellow Fade Technique from the last post.

The icon animation code is almost the same as the Mac version. Instead of driving the animation through an NSView, I control it through a CALayer; I add animation objects to it keyed on its position and then assign it a new position.

The yellow fade animation is very different. There is no NSGraphicsContext on the iPhone. Instead of re-implementing drawLayer:, I simply give the NSView the appropriate properties and change its alpha value through animation code. The new code is much shorter and takes advantage of Core Animation's repeat and auto-reverse methods.

As with the Mac implementation, this sample project (Mac OS X 10.5, Xcode 3.1.2) is available at BDCoreAnimation_iPhone on Google Code.

While Core Animation is generally amazing, it's especially helpful for giving the user feedback for their actions. Clicking a button can now easily show an animation that tells the user what the button is doing. To illustrate this, I'm going to walk through two simple animations: a flying icon and a yellow fade. Below is a Quicktime movie showing the two animations, and at the bottom of the post is the code behind them.

First, I setup the skeleton application. I have a source list on the left with a couple entries and child entries. I hooked this view up with a simple NSOutlineView and an NSTreeController. On the far right, there is a button to start the animation. See the screenshot below.

Clicking the button fires the selector animateButton:. That method does a couple things. First, it figures out where the button is and where it wants the icon to go, including the two curve points for the bezier path. Next, it calculates the actual bezier path the icon will follow. Then, it defines the animation, setting a duration and supplying the delegate and the path. Finally, it assigns that animation to the relevant NSView and starts the animation by calling setFrameOrigin:. The one important subtlety that is in [CAKeyframeAnimation animationWithKeyPath: @"frameOrigin"];. I found Core Animation to be very finicky about what key path I used to associate the animation with the view; frameOrigin works consistently, so try it if other key paths don't seem to work.

A couple years ago, 37 Signals popularized a user feedback animation called the Yellow Fade Technique. When a user performs an action or the (web) application updates, the new information's background flashes yellow and then fades away.

Clicking the button fires the selector animateButton:. This time, that method does a few different things. First, it creates an NSView and gives it a CALayer; the delegate is set to the controller, so that the controller can draw the layer. Next, it defines the animation. This animation is a bit more complicated. It's actually a group of animations. Since I want the yellow overlay to flash twice, I define four animations: fade in and out once then fade in and out again. Perhaps the code could be tighter, but this approach works. Finally, I assign the animation group to the NSView and start it with an NSView call to setFrame:. Notice that I use frameOrigin for the animation's key path, even though I start the animation with setFrame:. Again, Core Animation is finicky.

I have an additional method defined for drawing the layer's content: drawLayer:. In this method, I draw a dark yellow curved rectangle around the cell frame and then fill the cell frame with a lighter yellow.

Combine these two animations together for a nice icon swooshing into the source list with the source list item flashing yellow when the icon finishes. Check out the project (Mac OS X 10.5, Xcode 3.1.2) at BDCoreAnimation on Google Code.

For a more in-depth tutorial on Core Animation, please check out Bill Dudney's Core Animation for Mac OS X and the iPhone.

If you're going/around, be sure to check out CocoaHeads on Wednesday night at the local Apple Store. Scott Stevenson et al have an awesome lineup of people talking: Delicious Monster, Sofa, 280 North, and Sebastiaan de With (the guy behind Cocoia and Classics). Not to be missed.
I've previously covered how Apple's command-line utility ibtool can be used to automate localization tasks: generating a .strings file, localizing an English XIB, and incrementally updating a localized XIB. It's a fantastic tool, included in the Xcode development installation, and works well for both Mac OS X applications and iPhone apps. I use this automation extensively on Mac OS X (10.5): 14 XIBs across 11 languages means many, many files to keep in sync. However, the heavy use has exposed an annoying bug in ibtool: silent errors.

When ibtool encounters an expected bad character, like an unescaped double-quote, it verbally fails, supplying the message below.

This error is helpful, but it doesn't identify the offending line. To do that, we can use another Apple command-line tool: plutil. (Thanks to Cocoa Musings for pointing this out.) plutil can parse a .strings file and provide the line the bad character was on. It's a very useful program when the .strings file that failed has 3K lines, and the problem is a single unescaped double-quote. ibtool generates these verbal parse errors on all alphanumeric characters and a number of others (" ' : . / - _ $).

Unfortunately, ibtool (and plutil) fails silently on these other characters: 
{ } 
Let me say that again: ibtool fails silently. The program stops incorporating localized strings when it encounters one of these characters, but it does not produce an error. The bug becomes particularly nasty when scaling up to 150 localized XIBs. I only discovered the bug when I found a single XIB incorrectly localized and tracked it back to a set of brackets randomly inserted into a .strings file.

So, I wrote a ruby script, validate_strings_files.rb, to validate .strings files more completely. The script parses through a directory of .strings files and produces verbal errors, with line numbers, for any out-of-place characters. It works well on the 11 languages I deal with. An error that would silently fail in ibtool would produce the following result with the script:
VERIFYING FILE MainMenu.ES.strings
MainMenu.ES.strings (328): @
Below is the code for validate_strings_files.rb.

One important sidenote is Unicode support. The script works with Unicode characters but not universally. I have two lines to allow complete Unicode support. The first works on most languages, and the second is used for UTF-16 languages, like Japanese (JA).

Keep in mind that Ruby 1.9 is required because of its Unicode support; refer to the Hivelogic article for compilation instructions. The r:UTF-16LE:UTF-8 code is from this article on Ruby encodings.

One other error I've encountered is "class mismatch". ibtool produces this error when the class of a UI element associated with a specific ObjectID changes. For instance, imagine two languages were modified independently, and both had a UI element added to them. The first might be an NSTextField whereas the second might be an NSMenuItem; however, the IDs might be the same. The two entries in the .strings files would look like so:

Using the first as the base for localization, incoporating the strings into the second wouldn't make any sense for this UI element. Luckily, ibtool produces the following error when trying to localize those XIBs:

Looking around the App Store, I see most apps customize their UITableViews in a unique way. Flixster embeds movie posters and ratings, in addition to their titles. Tweetie integrates tweets, icons, usernames, and the date. GasBuddy lists service type, amount spent, gallons, and dollars per gallon in each row. Constructing these customized UITableViewCells is possible in code, but leveraging Interface Builder's drag-and-drop interface is far more fun. Thanks to Bill Dudney for talking about one approach to this on his blog and to StackOverflow for covering this topic.

Creating a custom UITableViewCell using Interface Builder is straight-forward.

The XIB is set up, but there are two approaches to instantiating the new customized cell from that XIB. When I was at WWDC a couple weeks ago, I confirmed with one of the Interface Builder engineers at the IB Lab that both work just fine. (He did repeatedly ask if I was using UITableView:dequeueReusableCellWithIdentifier:, just to make sure.)

One approach is to create a temporary UIViewController each time you need a new cell. By setting up the XIB the way we did, the temporary UIViewController has the cell as its view attribute. After we grab a pointer to that, we can release the view controller.

Another approach is to load the NIB file and grab the cell directly, as it's the only top-level object in the NIB.

I added the four localization scripts from previous posts to Google Code. In addition, I added a readme to document how to use them, as they do require some setup to work smoothly. Below is the readme and the script for incremental localization.



I'm pretty happy with the iteration.

When I sync in my iPhone, I love the syncing arrows in the source list. They smoothly turn around and around, letting me know that iTunes is busy syncing all of my data. iPhoto shows the same rotating arrows when it syncs a MobileMe album. Unfortunately, those arrows aren't an NSProgressIndicator Cocoa object; those objects only display the spinning lines for isIndeterminate:YES (see my NSProgressIndicator example from last year). Instead, that nice UI feedback is created through home-brewed animation.

On an side note, comparing the Resources folders in iTunes and iPhoto gives some insight into Apple's approach with each product. (Right-click on any application and click "Show Package Contents" to navigate to the Resources folder.) iTunes' Resources folder holds a mere 118 items, including 19 language folders. Only 118! There are extremely few icons in the folder, implying that iTunes draws almost all of them in code. On the other hand, iPhoto has 1,532 items in its Resources folder. Seemingly every state of every icon in the entire photo application comes from an icon in this folder. Different approaches. The end result is iTunes weighing in at 150MB and iPhoto at 410MB. Perhaps the stark differences are market-driven, as iTunes is predominately delivered online whereas iPhoto is typically pre-installed or comes on a DVD. For this tutorial, I'm going with iPhoto's approach.

Let's make syncing arrows for a sample Mac application. Originally, I thought I should use Core Animation, as it's in 10.5+. However, at WWDC, an Apple Core Animation engineer told me that only the iPhone apps can rotate table cells like that, as only UITableViewCells subclass a view object (UIView). On the Mac, NSCell subclasses NSObject directly, so he said to go with a simple NSTimer and cycle through a set of images. So, that's what I did here.

I want a simple application for this demo: just a list (NSTableView). Inside each row, I want those syncing arrows, rotating at varying speeds to see which interval is the optimal. There are two parts to this UI: an NSTimer and a set of icons in my cell class. To make the code very simply, I leverage Cocoa Bindings to trigger a UI refresh, instead of calling NSView::setNeedsDisplay. The screenshot below shows how to connect the NSArrayController in Interface Builder to the two NSTableColumns in the NSTableView.

The icons are straight out of iPhoto. Go into iPhoto.app/Contents/Resources and search for "sl-status_syncanimation-01.tiff". There are six total. 

First, I initialize the cell's state, including the timer and the icon array.

Next, I simply add the method called by the NSTimer. In there, I update the status icon and increment to the next state.

I posted the code on Google Code as SyncingArrows. Keep in mind that I wrote it in Leopard (10.5) using Xcode 3.1.2; let me know if you have issues trying it.
My last post illustrated how to add syncing arrows to a Mac application, like iPhoto and iTunes have. But what adding that user feedback to an iPhone app? Don't worry; it's even easier, thanks to Core Animation. In fact, there are multiple paths to those rotating arrows: an image array and a simple rotation transform.

In both cases, we'll use a custom UITableViewCell to drive the animation, unlike the model-driven approach we used on the Mac. See my custom UITableViewCell post for a tutorial.


Instead of leveraging UIImageView-specific methods, we utilize the iPhone's layer-backed UIViews and create a CAAnimation, with a rotation transform. To match the array animation, we make the toValue equal to pi radians (180 degrees). Unlike the array animation, we cannot supply a specific argument to have the animation repeat indefinitely, so we just assign a really large value. (Let me know if there is a better workaround.) Finally, we add the new animation to the UIImageView's layer.

The difference between the two animations is quite striking. At a small duration (around .3s), the array animation looks fine without comparison. The animation is almost imperceptibly jumpy but is, again, just fine. However, a side-by-side comparison of it to the rotation transform animation reveals the stark contrast between the two approaches. The latter shows how amazing Core Animation makes the UI; it's magic pixie dust. I committed a sample project (10.5, Xcode 3.1.4) to Google Code. Check it out in SVN, run it, and compare "Array at 1.00s" with "Rotate at 1.00s". The rotate transform wins. As does Core Animation.
Apple has a nice mechanism for storing important app bundle information outside of the application executable: Info.plist. All the applications in /Applications have one; just right-click on any app, select "Show Package Contents", then click on "Contents". The file elegantly houses many facts (version, name, copyright) about your app. And Apple has a straight-forward API to get at that data: [[NSBundle mainBundle] infoDictionary]. The call returns an NSDictionary filled with key-value pairs from the Info.plist file. This workflow allows me to easily add my own key-value pairs to the file and quickly access them in code.

Recently, I wanted to read the key-value pair dynamically. That way, the user could change the value at any time, and within a minute, the app would use the new value. This proved harder. As it turns out, Apple optimized the infoDictionary query, so that it caches the contents at startup and returns that cached version through the application lifetime. If the user modified the key-value pair, that change wouldn't show up until the next startup. Instead of using that builtin method, I had to read the Info.plist file directly. See the code below.

The feature I alluded to in my "Rereading Info.plist" post is a logging preference. It has two requirements: user-configurable (i.e. not code) but not immediately visible. When a user starts having technical issues and calls tech support, they find out how to turn on logging, and with that output, development has a better idea of what the issues are.

A year ago, when I first implemented it, I was averse to adding a secret preferences window, because I couldn't think of a natural place for accessing it. Instead, I added a quick key-value pair to Info.plist. Users wouldn't happen across it; most of them probably don't know app bundles are folders. But the preference was there if needed. So, I shipped that. Multiple times. Recently, I decided this preference needed to be dynamic, for those times when a user wants to enable logging without restarting the application. I set up an NSTimer to read the key-value pair from Info.plist every minute. After finding out the dictionary was cached, I figured out how to manually read it. Problem solved.

And yet, I completely ignored all the signs that pointed to a bad user experience. The workflow for setting the preference was complicated to explain to other developers (who know app bundles are folders). Non-developers asked for the specific steps multiple times. And yes, the Info.plist dictionary is cached, implying it's for general application information, not debug settings. Still, it took two thoughtful comments on my other post to snap me out of my developer haze and think about the feature from a user's perspective.

The goal of the feature was to hide the user-configurable preference from immediate view, not to bury it completely. The Info.plist workflow didn't work, so I started brainstorming. How about a secret preference window? Fine, but where would it go? Perhaps an NSMenuItem, but that's not hidden. Then I realized an alternate NSMenuItem was the perfect fit: hold the option key and "Preferences..." switches to "Secret Preferences...". A user won't stumble across it, but the menu item is a key away. Moreover, an NSTimer isn't necessary; Cocoa Bindings make a preference change instant. That's what I implemented today.

Never be afraid to iterate your design. The product I work on went through many iterations before arriving at its current form. Apple agrees. When I've talked to their UI evangelists, John Geleynse and Eric Hope, both consistently cite design iteration as one of the most important processes for developing award-winning applications (along with product definition statements). Most apps start out looking mediocre; the best user experiences are the culmination of many iterations. Keep iterating.
Google popularized this trend in web forms years ago. When I go to the Google homepage, the page automatically puts the cursor in the search text box, so that I can immediately start typing. In Things on the iPhone, when I start to add a new item, the "New To Do" page automatically selects the Title text field. Again, I can immediately start typing. This autofocus is very easy to achieve: simply call [myUITextField becomeFirstResponder]. Changing the keyboard style is equally trivial: myUITextField.keyboardType = UIKeyboardTypeEmailAddress. Both are attributes of the UITextInputTraits protocol.

For an iPhone app I'm developing, I wanted a toolbar at the bottom of the initial view, but I didn't want it appearing in the other views. I tried simply calling myUINavigationController.hidden, but the effect was a little jarring. UINavigationController has a great wrapper for that method that includes animation: setToolbarHidden:animated:. With animation, the toolbar smoothly slides in and out when going between the initial view and the other views. I inserted the method in UIViewController's viewWillAppear and viewWillDisappear.

Whenever I encounter a strange error message or a similar roadblock while programming, I google for answers. This approach leads to a wide array of websites, almost all blogs. (In fact, most of this site's traffic comes from Google, presumably from the same approach.) As soon as I get to the site, I want to quickly assess the post's relevance and my level of trust. If the site is interesting and useful, I'll trust it more if I come across answers on it again...if I recognize it. When I designed this blog, I tried to add these same breadcrumbs that I look for when I google: relevance, recognition, and trust.

When I click on a Google link, I want to figure out as quickly as possible if the page is relevant. So I look around for a date, an author, tags, and a comment count.

I expect the date to be embedded in the URL, but unfortunately, WordPress's default permalink format is arcane: http://www.bdunagan.com/?p=123. The URL format is displayed to every single visitor, and yet, all this conveys is how many times I've posted before this. It's a sad default when the very next option conveys both timestamp and title: http://www.bdunagan.com/2009/11/05/sample-post/. The visitor can quickly glance at the URL and instantly know the post's age and topic. The default permalink format is really an indication of the blog creator's laziness.

The URL is easy. One step further is the post's meta information. I want to know the author (for a site like TUAW), tags, number of comments, and frankly, the date again (as I might be looking at the homepage where the date isn't in the URL). All that information gives me a better idea of whether or not the post is relevant to my goals. Including that information next to the title makes sense. However, the default WordPress theme hides this information, shown above, at the bottom of the post instead of next to the title and above the fold. Worse yet, comments follow that section, so I can't easily page-down to the bottom of the screen, as it might be tucked away in the middle.

The default theme for WordPress is Michael Heilemann's Kubrick. It's a fine theme, but far too many authors stay with it, making it difficult for new visitors to trust the site and for repeat visitors to recognize the site. (Moreover, many spam blogs use this theme as well, making sites using it more difficult to trust.) Selecting an alternate theme is ridiculously easy; just go to the admin section, click Appearance, find a theme, and click "Activate". WordPress is nicely architected such that nothing more is required. And with the large number of free, built-in themes, selecting a relatively unique one is no problem. Many insert the post's meta information near the title, rather than at the post's end. Creating an identifiable site helps visitors recognize you. Going with Kubrick is just lazy.

A new visitor to this site has no idea who I am. No trust at all. The only thing going for the site is the fact that it showed up in Google as possibly relevant to the visitor's goal. In addition to the content, I put a quick description of myself at the bottom. I put links to Twitter, LinkedIn, and Flickr. Each represents a one-click path for the visitor to find out a bit about me. No guarantee it will be interesting, but it does establish I'm a real person who writes Mac software. I want to establish a modicum of trust with the visitor. It'll help the next time the visitor comes to the site and recognizes it.

Matt Gemmell's blog redesign is a great example of this trust in action. I visited his site yesterday but immediately thought I'd clicked on the wrong link; the theme was completely different. Instead of the graphic-rich white-on-black theme I was accustomed to, the site was text-rich black-on-white with lots of white space. I went over to Instintive Code to see if I'd mistakenly gone to Gemmell's other blog. Nope, no work blog. So I went to the blog's index page and saw the title "Accessible Blog Redesign". Ah, a redesign. Still, there was a brief period when I didn't trust the page I landed on; it wasn't the one I built my trust on.

To drive home the importance of breadcrumbs for trust, visit Clay Shirky's blog. Shirky is an impressive speaker and a great writer. When I read a tweet last spring about his sharp critique on the demise of newspapers, I immediately clicked through to the blog post. But I landed on a standard WordPress site with the default Kubrick theme. Weird, seemed like a guy as aware as Shirky would do something a bit more creative. I looked back through the blog's history. Hmm, created a month ago with only three posts so far. More weird. The blog came from Shirky's domain, but it had no links back to the homepage. And, Shirky's homepage didn't have any links to the blog. At this point, I'm very skeptical about the blog. I actually googled around to make sure other authoritative people linked to the blog articles, which they did. Now, nine months on, the blog still only has six posts and no links. I still find it weird. So many missing breadcrumbs.

Ironically, in writing this blog post, I noticed that my single posts didn't include the number of comments up at the top like the main index does. Fixed that. And I added a nice favicon from Glyphish. Iterate, iterate, iterate.
Most iPhone apps have a splash screen. It's a PNG image called Default.png that gets automatically displayed (if present) when an app starts. Apple uses the image to make an app appear to load faster than it actually does, by making the splash screen a screenshot of the initial view.

A couple days ago, I stumbled across an excellent tip for fading out Default.png using Core Animation. The original version faded the image out by expanding it and lowering its alpha value. It's a great effect, but I preferred a more general method. The code below is for a UIViewController subclass and fades out any subview. It brings the subview to the front, reduces the alpha to zero over a half second, then sends the view to the back. I use it to fade out the splash screen after the app launches and to fade out the root table view (thereby displaying the splash screen) if the user deletes all its items.

A couple months ago, I was using Safari on my iPhone when I came across a blog with an amazing iPhone theme. It had a nice UITableView feel, with badges for comment counts and even an on/off switch at the bottom. And right below that, it said, "Powered by WordPress with WPtouch". I thought, "Man, need to get me some of that."



As I mentioned in my last post, categories are such a nice addition to Objective-C. They allow me to wrap up new functionality in a standard class without subclassing it myself. Below, I extend NSNumber with two handy methods for printing out numbers and bytes in a human-readable version, turning 1,000,000 into 1 M and 976.6 KB, respectively.

On the iPhone, I miss NSHost. CocoaTouch does contain a private version of this Cocoa class, but again, it is private. Recently, I needed to resolve a DNS name to an IP address in an iPhone app. It's a one-liner in Cocoa: [[NSHost hostWithName:@"www.google.com"] address]. Not so on the iPhone without NSHost. So, with a little help from Apple's CFHostSample sample project and their docs on CFHost, I put together this short snippet.

What about the local IP addresses and ethernet MAC addresses? Building off the code from Zach Waugh (developer of QuickPic), I added two methods for getting the set of IP addresses and ethernet addresses. The wireless connection seems to be en0, while the cellular connection is pdp_ip0. Keep in mind that the IP addresses are what the device believes, not what the outside world sees (What Is My IP?), thanks to NATs.

What about names for addresses? Again referencing Apple's CFHostSample, I added a quick wrapper for CFHostGetNames.


Many panes in System Preferences require I authenticate myself to change settings. Apple has a consistent UI for this: the lock icon. Next to it is a little snippet of text telling me why it's useful: "Click the lock to make changes." I click it and authenticate, and it opens, enabling various buttons and fields in that pane. "Click the lock to prevent further changes." I click it again, and it locks, disabling those buttons and fields. This UI and underlying authentication process are included in Cocoa: SFAuthorizationView.

This tutorial creates a simple prefpane with a button and the lock UI, building on two earlier posts about root-level operations and debugging prefpanes. Clicking the button creates an empty file at /var/log/test.txt, an action that requires root-level privilege. The button is grayed out if the lock is locked; it enables when I unlock the lock. The source code for this project is available on Google Code.



There are many good iPhone tutorials on creating a UITabBarController directly through code (a more complex implementation) or from the main XIB. However, I want to separate out the UITabBarController UI elements into a separate XIB and load them dynamically.

Most of the code is boilerplate. The key is creating the XIB in Interface Builder so that it hooks itself up when loaded. First, the code. The main controller needs to have an outlet for the tab class.

When the tab instance is needed, we can load it:

Next, the XIB. We change the "File's Owner" object to the main controller class and connect the new tabBarController outlet to the UITabBarController in the XIB. When the XIB is loaded, the tab bar controller is added to the main controller.

I've posted an example project to Google Code. See the screenshot below.
Recently, I got an email from Skype saying they activated my Skype To Go number. It was early in the morning, and I hadn't finished my coffee. I already had a vague discomfort with Skype over their switch from SkypeIn/Skype Pro plans to US/Country/World plans. Skimming the email, I quickly jumped to the conclusion that Skype had somehow lost my original number (which I acquired in 2005) and replaced it with a new number. Going to Skype, I clicked on "Skype To Go" and saw it was indeed the new number. I clicked around a bit more to see if I had the wrong section. When I didn't find any reference to my original number, I sent a support email to Skype about the problem, asking them how to switch it back. Below is a screenshot of the email:

But of course, Skype hadn't screwed up. I had confused two separate services: Online Number (SkypeIn) where people can call me and Skype To Go where I can call international numbers through my cell phone using Skype's low rates. Skype support replied to me within five minutes, allaying my fears about losing my original number and explaining the new Skype To Go feature. The problem was, like most consumers, I don't read.

Let me repeat that: I don't read. Moreover, I don't care. My sole goal with Skype is to have a phone number with voicemail. All of my interactions with the service are in terms of that goal, making the other features just noise. I skimmed that email because the subject said "activate" and "number", and when it mentioned a number that wasn't my existing one, I panicked. I'm not familiar enough with Skype's product line to recognize the difference between SkypeIn and Skype To Go, and frankly, I didn't even read that part. Less than three minutes passed between seeing the email and emailing support.

It's a marketing quandary. Users have a very limited view of the products they use. They only care about solving the problems they bought the products for, not utilizing all the accompanying features nor hearing about other available products. Developers live and breath their products. They intentionally added every feature to those products and wrote the accompanying marketing material. Developers read and care deeply; users don't do either. As developers, we need to keep the user perspective in mind.

The best antidote: remember you are a user. Everyone has interactions like my Skype story. Reflect on those, and think about your own products in that context. What if users only read every 10th word? What if they continually hit "OK" on "Confirm Delete" followed by cursing? I do both as a user, because I want to accomplish a goal, not perform a task with a specific product. Your users will thank you.


On my MacBook Pro, I have a lot of items in the menu bar: Tweetie, LittleSnapper, Jing, Evernote, BusySync, MobileMe, along with the standard set. Recently, when I needed to make my own, I started wondering what they are. Actual apps or app components? Activity Monitor confirmed that most are separate applications, although Tweetie's is just a component. Good, so how do I make one?

All I had to do is add a single row to Info.plist: set LSUIElement to true. The other code is boilerplate, creating a controller in Xcode and hooking up some outlets in Interface Builder. I created a quick sample project on Google Code under MenuBarItem (screenshot below). Thanks again to Glyphish for an awesome set of free icons, including the pictured coffee mug.

Recently, I wanted to composite together two images, much like Tweetie's retweet UI for avatars: display the main icon in its original size but overlay a second icon smaller and offset. Turns out to be only a couple lines of code using NSImage:

"When the Mac first came out, Newsweek asked me what I [thought] of it. I said: Well, it's the first personal computer worth criticizing. So at the end of the presentation, Steve came up to me and said: Is the iPhone worth criticizing? And I said: Make the screen five inches by eight inches, and you'll rule the world."
- Alan Kay in 2007

"Nothing about the iPad is obviously revolutionary, but it didn't need to be: the iPhone OS and iPhone hardware are already revolutionary.

Apple already reinvented John's mobile computing and my input mechanics and novice usability in 2007 with the iPhone. We've had the truly magical and revolutionary product this entire time, but we take it for granted now, and we've forgotten how awesome it already is."
- Marco Arment from Tumblr and Instapaper

"The iPad as a particular device is not necessarily the future of computing. But as an ideology, I think it just might be. In hindsight, I think arguments over 'why would I buy this if I already have a phone and a laptop?' are going to seem as silly as 'why would I buy an iPod if it has less space than a Nomad?'"
- Steven Frank from Panic

Not exactly filling the void with an iPad post, particularly one that's two weeks late to the party and starts with what smarter people have already said. I blame Things for not being more insistent.

I agree that Apple's iPad is the next step in computing: simplified computing. Yes, it has its shortcomings (inherent to a 1.0 release, which the iPhone had only three years ago), but it keeps with the design decisions that made the iPhone/iPod Touch ridiculously popular.

To illustrate the need, let me tell you about the "Check disc" incident. Over Christmas, my Dad subscribed to the New York Times Crossword Puzzle. The NYT doesn't have an online puzzle component, so he had to download Across Lite, a third party Windows app. He called me after using it for a couple days about a "disc" issue. Whenever he tried to save the day's puzzle, the app displayed, "Cannot write to file. Check disc." It was such a weird error that I wanted to see it for myself. With VMWare Fusion on my Mac, I installed Across Lite and got the same error within a minute. I didn't actually figure out what the problem was (read-only directory?); instead, I simply told my Dad how to use the "Save As..." option to save the puzzle to the Desktop. Problem solved. 

But the larger problem is not solved. The incident is a perfect example of the abstraction problems that current computing environments have. They want to deal in apps and documents and photos, but they still break these abstractions daily. Windows gives me app shortcuts available from the Start menu, but those are a mediocre hack to abstract away the monstrous Program Files folder. Mac apps are nicely treated as atomic objects (even though they are folders), but people still run them out of read-only DMGs, leading to equally confusing error messages. All these semi-porous abstractions lead to a lot of unnecessary confusion.

The iPhone OS enforces the abstraction to a much better degree, and we've seen the results: over two billion apps downloaded on seventy million devices in eighteen months. All these little apps are islands of data, a per-app version of the Newton's data soup platform. The iPad simply extends this OS to a larger surface.

I want one.
It's very easy to detect a shake event in an iPhone app. There are many good posts on StackOverflow about it, but I wanted to consolidate the code into the minimal number of lines. The key is the motionEnded:withEvent: delegate and the view becoming the first responder.

The iPhone OS's accelerometer tells me the exact x, y, and z orientation of an iPhone or iPod Touch. Which is freakin' awesome. To demonstrate how easy it is to leverage, I wrote a bit of code to rotate a UILabel in a simple Utility app. The controller sets itself as the delegate for UIAccelerometer and then responds to the selector accelerometer:didAccelerate:. I started by simply using the exact rotation, but that resulted in a very wobbly text label; so, I added a bit of rounding and animation to dampen it.

Today, I was looking at a UI bug where the value of an NSTextField, populated through Cocoa Bindings, wasn't getting saved. I tabbed over to the field, changed the value, tabbed over to the blue "Done" button and hit Space. Value saved. No bug. So I tried following the steps to reproduce exactly. Click in the field; change the value; click "Done". Ah, the value didn't save. Turns out, bindings don't fire by default for an NSTextField unless it loses focus. To get the bindings to fire, I had to go into Interface Builder and check "Continuously Updates Value" (see the screenshot). With that checked, the bindings fired for both NSControlTextDidEndEditingNotification and NSControlTextDidChangeNotification notifications.

Let's say I want to detect a modifier key. This information is embedded in every NSEvent object as flags in the modifierFlags attribute. But what if my current method isn't passed an event? The answer is not NSApp's nextEventMatchingMask. I explored that option for too long before moving on. The answer is NSApp's currentEvent. With it, I have easy access to the current event that the application is dealing with. To detect the modifier keys, I simply extract the modifierFlags attribute and check it for them.

iTunes has a great user interface affordance for adding actions to text: clickable arrows embedded right next to the text. (Perhaps Apple wanted to avoid an interface filled with blue, underlined text.) I haven't found any formal name for them in Apple's HIG, so I call them link arrows (or jump arrows). Sadly, this is my third post on recreating link arrows. My first post was a first pass at the problem, whereas my second post looked a bit better but not great.

This time, the UI looks right, and clicking works as expected. I wrote up a small sample app to demo the link arrows; see Google Code.

The key is tracking. In the previous post, I used NSCell::hitTestForEvent to detect whether a click "hit" the link arrow image on a mouse down event, but then I immediately acted on it, without waiting for a mouse up event. Jarring and wrong.

I need to track what happens after that initial NSLeftMouseDown event. For this, I use NSCell::trackMouse to track all relevant mouse events until the next NSLeftMouseUp event. Thanks to Apple's PhotoSearch sample app and Rowan Beentje's Sequel Pro code for this much needed direction.

There are a couple corner cases I handle in trackMouse:

These mouse event sequences are what people expect from buttons, and I want people to think of the link arrows as buttons, as iTunes treats them.

Below is the main code from LinkArrowCell.m. The rest is just scaffolding.

Before I went with the trackMouse approach, I looked at subclassing NSCell and overriding startTrackingAt, continueTracking, stopTracking. For some reason, my overridden methods never triggered. Very odd as Apple's NSCell docs bring up the approach, and many developers refer to it working on various email lists like cocoa-dev. Still not sure why this path didn't work, but trackMouse certainly does.

Again, a sample project for this code is at Google Code.
A couple days ago, I was trying to codesign the latest application bundle when I got a very strange error message:

Googling for those exact phrases lead nowhere. Eventually, I came across an excellent StackOverflow comment from Apple employee Chris Espinosa: different OSes. In fact, that was the problem. The application bundle was built on 10.6 and I was trying to use codesign on 10.5. Doesn't work.


The UINavigationItem button is an excellent example of this larger hit area. Try touching near (but not on) the standard "Back" button in any navigation-based iPhone app. It responds. I never even noticed this interaction until I started this post, and that's precisely the goal.

But when Apple doesn't auto-extend the hit area, the developer should. There are two approaches:

I can programmatically grow the area of the button by subclassing UIButton and overriding UIView::hitTest:

For the invisible button, use Interface Builder to create a large UIButtonTypeCustom UIButton, hook up its touchDown, touchUpInside, touchUpOutside events to custom selectors in the controller, and use UIButton::setHighlighted on the original button to mimic a real tap, as described in this spot-on StackOverflow comment. (And don't try setting the button's alpha to zero; the button will no longer respond to touch events.)

Either way, an oversized hit area is especially necessary for smaller buttons, like the 29x31 "Detail Disclosure" button (UIButtonTypeDetailDisclosure) or the 18x19 "Info Light" button (UIButtonTypeInfoLight). Try simply adding the "Info Light" i button to a blank screen, running it on a device, and tapping it; it's frustratingly small.
This week, I ran one of my ibtool ruby scripts to localize an app. During the process, it generated the following error:

The error sounds innocuous. I searched on Google for it and got literally no results. Unfortunately, the result is quite bad: a partially localized XIB/NIB. The error means ibtool has encountered a string in the strings file with an ObjectID that isn't present in the source XIB file. In my case, I had deleted a placeholder string from an NSTextFieldCell element. When ibtool is unable to find this ObjectID in the source XIB, it stops localizing the new XIB. Not a pretty bug (rdar bug #7801770).

To show the confusing result, I reproduced this problem in a new project:

The problem is clearly visualized in the two screenshots below, taken from Changes. Diff'ing the XIB files shows that ibtool stops localizing the new XIB when it cannot find the missing ObjectID. Look at Changes' scrollbar marks.

However, diff'ing the strings files makes it far less clear how ibtool failed. Changes' scrollbar marks are distributed across the entire file.

Yet another reason to always verify the newly-created localized XIBs by comparing strings files... :)
Making template/theme changes to a live blog is a bit frustrating. While mucking around with CSS or a theme header, I occasionally render this blog completely illegible or obviously broken, then I scramble to revert my changes, hoping I remember the original code correctly. Editing is a pain without source control or a staging process and very different from app development. Too different. So, I figured out how to track all my Wordpress changes with git and stage them on a subdomain on DreamHost.

I'm using git and GitHub, but any scm would be fine. I'll run through the steps here. (Keep in mind I use Wordpress's update system for version and plugin updates; I just want to track my own changes.)


Seeing the changed files in git is fantastic, and at this point, I wonder how I got by for so long without versioning my Wordpress changes. But tracking changes isn't enough.

Now, I can easily revert horrible template changes, but anyone hitting the blog during that time will still see it. It reminds me of how the early Facebook team used to directly edit the HTML on the production site when they were adding features. Debugging changes on a live site is annoying and unnecessary. Instead, just use a subdomain. Here are my steps:


Staging changes is so much more comforting. Before, I wondered how many visitors hit my site mid-change (which sometimes took hours) and left annoyed. At least now I can think, "Actually, I designed it that way intentionally..."


While networks are awesome in the abstract, actual implementations are messy. Take multicast traffic. Network engineers can simply disable it, and I'm sure they have perfectly valid reasons for it. But as an uninformed user, what do you do if a multicast-enabled app doesn't work? Is the failure the app's fault, or is the network to blame? Enter Multicast Ping, available as an app download (PPC/Intel 10.4+) and an Xcode 3.2 project on GitHub.

Multicast Ping is a tool for testing multicast connectivity. It tests both directions, sending multicast packets to the supplied address/port while also listening for multicast packets on that same address/port. Run it on two Macs, and the app will let you know if the computers see each other. Both copies should display connections; if they don't, the problem is in the network.

The icon is courtesy of Artua Design Studio via Iconspedia.

There are a couple great tools for testing multicast connectivity: ssmping for Windows and Linux and mctester for python (which means most modern systems). However, I couldn't find any equivalent implementations in Objective-C for the Mac, so I used mctester as a reference to write this Objective-C/Cocoa version.

The code (see below) is straight-forward: two methods for creating send and receive sockets and two methods for sending and receiving the multicast traffic. Take a look at the GitHub repo to see the code in context.

Again, the app is available for download (PPC/Intel 10.4+).

Recently, I thought it would be interesting to add a sidebar to my Wordpress blog. I figured a short list of recent posts, relevant posts, and random posts would help surface information people might find useful. Here's a quick rundown of how to add these with the PHP code below:


My friend and I were throwing around ideas for iPhone apps, and one of his suggestions was a timer for meetings. Time is money, and we've all experienced meetings that waste both. Launch it, and the app starts counting up, displaying the cumulative cost of the meeting. I submitted it to the App Store in February under Dollar Clock as a free app, and it's had 700 downloads to date.

Dollar Clock is very simple. Just pick the number of people and their pay, and the app starts counting. It remembers when it started, so you can quit the app and relaunch it later. And it tweets, via Tweetie and Twitterrific: "$394 at a 5-person meeting #timeismoney".

**Screenshots**

Apple includes a handy commmand-line tool called agvtool for updating Xcode project version numbers via Terminal. Run the tool with a build version or marketing version, and it modifies a couple fields:

Google "agvtool" and you'll inevitably come across two great posts: Apple's Chris Hanson covering the tool's goal and Red Sweater's Daniel Jalkut discussing its automation. They go into much greater detail.

Keep two caveats in mind when using agvtool:

Quick, open up your Mac's Downloads folder or your Windows Desktop. Do you remember where all those files came from? I have fifty items in my Mac's Downloads right now, and I forget where twenty of them came from. My favorite is "Inst 8.1.184.1.zip". What was that...

Name your downloads. Think about people downloading files from your website and then remembering them next month. It happens; they're as busy as you are. Be sure to name those files, so that they remember what the files are and why they cared. Remember, you are a user too.
In writing Multicast Ping for the Mac, I needed a two-way communication pipe between the interface (parent process) and the privileged tool (child process) and a clean method for terminating the tool through that pipe. NSTask provides setStandardInput and setStandardOutput for just this reason. However, I cannot launch a privileged tool with NSTask; instead, I have to use AuthorizationExecuteWithPrivileges with its lower-level file handle mechanism.

With a bit of help, I implemented a two-way pipe at a high-level and utilize that pipe to cleanly terminate the child process with a simple EOF signal. Apple's BetterAuthorizationSample sample project touches on this communication pipe but does so at a low-level and only one-way. Matt Gallagher from Cocoa With Love also uses a pipe in invoking other processes when he creates an Open File Killer app, but the pipe is still only used one direction. Thanks to both those tutorials as well as a post from Caius Theory for helping me along. See my GitHub multicast_ping repo for the code in context.




In porting Dollar Clock from the iPhone to the iPad, I switched from using a flip view (using UIModalTransitionStyleFlipHorizontal) to a popover view (UIPopoverController). But a strange thing happened: the UITableView in the popover lost its initial selection, after viewDidAppear was called. Regardless of what I did, the row was always deselected, and indexPathForSelectedRow would go from returning the correct path to nil. Turns out I needed to reloadData before assigning it an initial selection.


A week ago, I replaced the Flickr photos in my blog's header with iPhone OS apps: Retrospect Touch and Dollar Clock. Since I hadn't updated Flickr since Christmas, the iPhone/iPad apps seemed more appropriate, and I was curious to see if the switch affected the download numbers. Here's the before and after:

Today, I checked the numbers in iTunes Connect for Dollar Clock. At first, I only looked at the total downloads, in the left graph, where the hash mark notes the header change. Clearly, the header change increased the downloads. Then I looked at the purchases and updates separately, in the right graph, and the pattern was less clear. During the week after the header change (the bar after the hash mark), more of the downloads were updates, not purchases.

During the same week I changed the header, Dollar Clock 2.0 was posted to the App Store, and many people downloaded the update that week. The total downloads indicates my header change had a significant effect, but the breakdown reveals confounding variables. Perhaps if I had more variables, I would find that the header change had absolutely no effect.

Or maybe I just wanted an excuse to use Tufte's small multiples. :)

When Mac applications are compiled, Xcode has a setting to include or exclude debugging symbols: the "Strip Style". When set to fully strip, the bundled executable (AppName.app/Contents/MacOS/AppName) contains very few symbols, but when all the symbols are included, the executable is a wealth of information. Unfortunately, Xcode's default Release configuration includes all the symbols, leading to shipping applications with lots of symbol information, including method names and even source files. These are accessible with nm. Let's see what we find for Retrospect Client (from my company):


While interesting in isolation, the information is richer in aggregate. With a quick ruby script, we can rank the apps in /Applications by various metrics:


Apple's products are not in either list because they strip out the symbols for every major application. However, they apparently missed App Store's Application Loader 1.1 (72), codenamed StarGazer:

Over half of the apps in my /Applications had no symbol information. The developers stripped it out, as described by Apple's documentation on symbolizing crash dumps.

As I mentioned earlier, Apple's default Release configuration does not strip out symbols. In fact, it leaves them all in, regardless of what "Strip Style" (STRIP_STYLE) is set to. The culprit is "Deployment Postprocessing" (DEPLOYMENT_POSTPROCESSING). When not enabled, Xcode ignores the setting for "Strip Style". Apple describes this "Deployment Postprocessing" option in Xcode Build System Guide: Build Configuration and further in Xcode Project Management Guide: Building Products.

I wrote a quick sample application in Xcode and used the default Release configuration to generate the following dumps from nm:



To generate the statistics for the apps in /Applications, I wrote a short ruby script:


If the tool nm and its wealth of information seems impressive, class-dump is amazing. Feed it an executable and get back a complete list of class names, internal variables, and methods, even as a set of header files, thanks to its deep understanding of Objective-C and Mach-O. Apple's product might be stripped of their symbols, but class-dump can list their internal structure.

I created a template application, added a method (shouldShowInClassDump), and built it with all the symbols stripped out. Running class-dump on the resulting Release executable revealed the following:

To see how applications compared in aggregate, I wrote a simple ruby script to walk through /Applications and sum up the number of lines of headers from class-dump and the number of @interface declarations. Both are crude measures of complexity.


Apple's tools are clearly complex, with iPhoto and Aperture leading by almost a factor of two in header lines, but apps like Panic's Coda hold their own. And Retrospect topping the final list is obviously an indictment of my coding abilities.


Billions of app downloads through a mobile application store was not a certainty or even speculated. Apple had no idea the App Store was going to be such a runaway success. By an account on Daring Fireball, there was a heated debate simply over the phone's OS: Mac OS X or something else. And even after CocoaTouch was built, Apple didn't ship the App Store and iPhone SDK for a full year. The popularity of third-party apps was not guaranteed.

But then Apple launched the App Store, and everyone saw just how popular it was. The App Store transformed the device from an outstanding Apple product, like Apple TV, into a revolutionary mobile device. Apple's interest was sealed at that point. Think about Apple's 2004 Macworld press release, three years after it launched Mac OS X: "Mac OS X Users Approach 10 Million. More Than 10,000 Native Applications Now Available." Compare that to Apple's recent iPhone 4.0 press release, three years after it launched the iPhone: 85M iPhone OS devices, 185K apps, 4B downloads.

However, the Mac lacks a comparable experience. We've seen how much confusion the Mac's current installation process causes. DMGs and zipped app bundles are better than Windows applications' arbitrary installer workflows, but still, non-technical people simply do not understand what's going on. Which is fine, they shouldn't need to know about the man behind the curtain. They're just normal people.

The computer industry has been blind to their needs for decades, and experiences like the App Store are revolutionary improvements. The App Store proves that there is a huge pent-up demand for software that's easily purchased and installed. At no point in using the App Store does someone wonder how to find an app, how to install it, where the Downloads folder is, or what a DMG is. The abstraction of the App Store is complete.

Of course Apple is thinking about a Mac App Store. Bringing the App Store's distribution model, payment model, and user experience to Mac OS X is a natural progression. And Apple's next OS upgrade, 10.7, seems like a fitting place to debut a Mac App Store. It won't lock out all other software, like some suggest. Why bother? The Mac platform is already established. The concerns over battery life and content quality are moot. They're just layering on an additional layer of simplicity, much like Mac OS X layered a beautiful user interface and tool set on top of BSD Unix. Valve's Steam is an App Store for games. Facebook is an App Store for friends. Again, it's all about abstraction.

And from a business perspective, it's all about sales. Hardware sales. What drives sales? Killer apps. Apps that convince people to buy the hardware. What drives iPhone OS sales? The App Store. It delivers killer apps. It's a meta-killer app. A Mac App Store would surface the best Mac apps through iTunes to millions of people, who use either a Mac or a PC to sync their iPod or iPhone or iPad. It would provide potential customers thousands of reasons to purchase a Mac.

Consider the present alternatives. Apple Downloads lists around 6K apps. People can find it from menu bar under "Mac OS X Software..." and, until recently, the Downloads link on Apple.com. The site is available but not blatantly obvious. Beyond that controlled environment, there are Softpedia (72K apps), VersionTracker (23K apps), and MacUpdate. Their goals are ad revenue, not user experience; hence why my Multicast Ping was automagically added to Softpedia. And there is the open web. Sure, potential customers can search around to fill their needs or find apps by word of mouth. But even ignoring the search issues, the installation workflow is far from ideal. Finally, there is Bodega, the third-party Mac app trying to solve this very problem and doing a really good job at it. Their software is clean and inviting, and their 100K+ downloads support the theory that there is a need. Nonetheless, Bodega doesn't drive Mac sales. Apple needs its own Mac App Store to do that.

Yes, people have speculated about a Mac App Store since shortly after the iPhone App Store debuted, and the idea has seen renewed gossip since Apple converted ADC to a flat $99/year membership, removed the Downloads link from their site's header, and explicitly said it will never be. I remember when Jobs said no video iPods.

So that's my prediction: Mac App Store through iTunes in 10.7. But let's say I'm wrong. Apple decides not to extend its billion-download idea onto the Mac. What does that say about Apple's interest in the Mac? I'd guess Charlie Stross is right, and Apple views the Mac as a dying platform.

I doubt that, though.
Matt Drance posted an interesting thought the other day about the average consumer rejecting bad user experience. When helping out my parents or other non-technical folk, I've never found that. At least, not exactly. Average consumers do not think about user experience. They don't think about workflows, calls to action, marketing copy, or corner cases. Consumers just use stuff. When something goes wrong, it's their fault. They internalize any problems. They blame themselves.

When consumers have a great user experience, they don't notice. They don't wonder how you designed the product so well. They just use it and move on, but that's a good thing. They shouldn't feel bad while using your product. Don't make users feel bad because eventually they'll choose a better user experience as a coping mechanism.

User experience: Because your users internalize your product's failures.
Apple has a great Technical Q&A document for embedding a URL in an NSTextField. However, after implementing their approach, I found two issues:

Recently, I wanted to store a key/value pair in Info.plist or in preferences (NSUserDefaults) and know if the key existed and the value if it did. I started by using boolForKey, but this method doesn't distinguish between NO and nil. Instead, I switched to valueForKey. Here are six lines of code to detect the three values:

Somehow, my iPhoto library contains duplicates. Lots of duplicates. I tried Brattoo Propaganda's Duplicate Annihilator, but while it took care of many, many photos, there are some photos and quite a few movies duplicated. Ruby to the rescue!

I wrote a 50-line Ruby script to list out the duplicate files. The script uses SQLite3 and SHA-1 digests to identify multiple copies of each file in a directory. The end result was 2K duplicate files and 20 GB of freed disk space. Woot.

Here's the script:

Today, I needed a debug log for Wordpress. Luckily, the Wordpress Codex details just a couple lines of code to turn on error logging. This method is especially nice for hosted environments like DreamHost, where there's no access to php.ini. Simply add the following to wp-config.php:

Unfortunately, NSDate doesn't have a "round" selector like roundToNearestQuarterHour. But it does have NSDateComponents available. Just a couple lines of code using that handy class, and I can get the desired result: a date in Cocoa, rounded to the nearest 15 minutes. Just add the code below to the nearest NSDate category.

When I start up my Mac, I have a number of apps launch, including Jing, Evernote, LaunchBar, and BusySync. Each registers itself as a "Login Item", listed in System Preferences under Accounts. Developers have different names for this option, like "Launch on Startup" or "Launch at Login".

In 10.4 and older, Mac OS X enabled this feature through AppleScript, and Apple has sample code for that in LoginItemsAE. In 10.5 and 10.6, the API is in LSSharedFileList.h, buried in the LaunchServices framework:

Frameworks/LaunchServices.framework/Headers/

I played around with this API a bit today and came up with a couple wrapper methods to abstract away the details: isLaunchAtStartup and toggleLaunchAtStartup:. Feel free to use this code.


Introducing Remind Me Later.
Global hotkeys are a very handy method for letting the user access your application without a mouse or command-tab. Cultured Code's Things uses it very effectively to quickly type in new tasks via control-option-space. I recently implemented it for Remind Me Later for quickly creating events with a simple key stroke: command-option-R. The application does not have to be in the foreground. It doesn't even need a window. It just needs to be listening.

Frankly, the hardest part is figuring out the correct keycode. The best place on the Mac seems to be Events.h, in HIToolbox.framework (cd /Developer; find . -name "Events.h").

Ever wonder why Command-A on the Mac doesn't always select all of the text? The keystroke is actually a hot key for the "Select All" menu item under Edit for Mac applications. Interface Builder presets the menu item's key equivalent to Command-A. If the menu item isn't there, Command-A isn't hooked up.

Certain applications, like Google Notifier and Remind Me Later, don't have an Edit menu, but we only need a bit of code to create a hot key like Command-A for "Select All". (Ironically, I found an example of this code in Google Breakpad, but Google Notifier doesn't use it.)


I updated Remind Me Later today to include calendar selection. In version 1.0, the app would always add events to the first calendar in iCal. Now in version 1.1, you can select any iCal calendar, including CalDAV ones like Google Calendar.

One caveat is CalDAV delegate support. iCal allows you to add a Google Calendar account and then include secondary calendars as delegates. However, iCal does not include these delegates when other applications ask for the calendar list (via Apple's CalendarStore framework), so the calendars don't show up in Remind Me Later's calendar list. I posted an easy workaround on the app page.

Dollar Clock 3.0 is available on the App Store for iPhone and iPad. This new version supports external displays via Apple's VGA Connector. Just plug in a projector and launch Dollar Clock: time is money, on the wall.

I made a couple other additions as well:

As always, Dollar Clock is free. Download the new version!
So, you stumble upon this Remind Me Later thingy. You download the app and run it. Nothing happens. You look around the computer screen. Nothing. You run the app again. Still nothing. A couple minutes later, you notice the bookmark icon in your menu bar. Not the best user experience.

For version 1.2, I've added UI hints. The menu bar pulses blue when launched to make its location clear. When you add an event, the icon pulses blue when all goes well and red when something goes awry. Hopefully, these small visual hints will help people understand the app a bit quicker.

Remind Me Later is a very simple app, but even the smallest apps need design work. Also, it's free. Give it a try.

Remind Me Later 1.3 is now live on the Mac App Store and this blog! First and foremost in this update is the improved Launch, Help and Support. The first launch experience of previous releases has proven to be an unmitigated failure, but I'm pretty happy with this new workflow. And in case people have trouble, the Support links in Preferences should help users contact me more easily.

In addition, I added a number of features:

	Durations: "4pm to 5pm"
	Delays: "in 1 hour"
	All-day events: "Monday through Friday"
	"Add and Launch iCal": Cmd-Enter to add an event and open it in iCal
	Configurable global hotkey
	Fixed the clickable icon issue on Leopard (10.5)

Download this new release from the Mac App Store or this blog!
Remind Me Later 1.3.1 is now live on the Mac App Store and this blog! This release is simply to fix a couple issues, most of all the alarm problems people have reported.

	Cleaned up the calendar selection in Preferences
	Supports local date format: "02/14/11", "14-02-11", "2011.02.14"
	Supports "next [day]" like "next Friday"
	Fixed various alarm issues

Download this new release from the Mac App Store or this blog!
Remind Me Later 1.3.3 is now live on the Mac App Store and this blog! This release is simply to fix a couple issues, most of all the alarm problems people have reported.

    Supports times without punctuation, like "0900" and "630p"
    No longer schedules event in the past
    Fixed day of the week parsing ("my wedding" =&#62; "myding on Wednesday")
    Fixed issues with 10.5 support
    Fixed iCal custom alarm sound support
    Added support for BusyCal

Download this new release from the Mac App Store or this blog!
In the week leading up to Apple's WWDC keynote last Monday, everyone seemed convinced that Apple's big announcement, iCloud, would be a cloud-based streaming service. (In fact, NPR reported that after the event before amending the article.) But music streaming would be such an odd service from Apple. They make money on hardware. All their other businesses fuel their hardware business. How would music streaming fit into that story?

As it turned out, iCloud is not music streaming. It's a wish list of features to fix what's broken about iOS. Think about it from Apple's perspective. Their online service has never been a success. MobileMe offers an excellent interface to mail, contacts, calendar, and photos; iDisk has never been exceptional; Find My iPhone was their first hit because it leveraged their unique hardware. But at $99/year, only Apple fanboys (like me) and a minority of non-techies opted to pay for the bundle of services. MobileMe just wasn't very compelling compared to the multitude of free services from Google, Yahoo, and Microsoft. In that respect, MobileMe was a fresh coat of paint on .Mac, but the underlying business simply wasn't exceptional.

I wonder if Apple's executives sat down last year to brainstorm a better online service. You can imagine the sort of wish list they would come up with. Anyone who owns an iPhone or iPod understands the frustrations with syncing. 

Out of these frustrations, you can see a service forming. It would be a massive service but a compelling one. The iOS ecosystem has been hampered by its own success, and this service would solve many of its current problems and fuel further growth. And that's the key: fueling hardware sales.

So, Apple announced iCloud. Lion and iOS 5 are evolutionary steps, but iCloud is revolutionary. As Ben Brooks put it, iCloud is Apple's magnum opus. Kudos to Apple for understanding their business and its problems so well and for attempting a comprehensive solution. I wish more companies were as self-aware and motivated.
Recently, I wanted to know the difference between the Debug and Release configurations in Xcode's Build Settings. Here are the essential flipped bits, turning a Release config into a Debug config:

I've talked about thinking like the user. I've talked about feeling for the user. But honestly, developers aren't users. Especially when it comes to their own apps. Developers know exactly how their apps work. They wrote it. They made a hundred decisions that led to their app's user experience. Why would they encounter problems with their own mental model? But users inevitably will. My advice: just handle it. Developers should listen to their users and transparently handle other mental models.

Remind Me Later had this problem. When people downloaded the app from the Mac App Store, the app went to their /Applications folder, and its icon was added to their dock. So, the user clicked the dock icon. Remind Me Later's dialog appeared, because on launch, I show the dialog. However, some users associated the dock icon click with the dialog's appearance, because they didn't notice the menu bar icon appear. People complained.

My first pass at a solution was a pulsing menu bar icon. When someone clicked the dock icon to launch the app, the app's menu bar icon pulsed blue four times. I thought people would notice this. Wrong. People still complained.

My second pass acknowledged the root problem. Some people don't understand the dock. They don't get the menu bar. They just want the stupid dialog to appear. The first click on the dock icon makes the dialog appear; that means the second click should. Luckily, Cocoa's NSApplication provides that exact delegate method:

Now in Remind Me Later, clicking the dock icon makes the dialog appear. Every time.

I'm not alone in coming to this conclusion either. Those popular productivity apps, LaunchBar and Alfred, both support that interaction. Click on the dock icon; the main window appears, every time. However, I'll bet both of them added that workflow after someone complained. On the other hand, QuickCursor provides no help when I click the dock icon. The first click puts the icon in the menu bar, but I might not notice that. The second click does nothing. (Keep in mind that QuickCursor is a brilliant app, along with the rest of Jesse Grosjean's Hog Bay portfolio. This post was written in WriteRoom.) Every dock icon click should open the menu in the menu bar. That interaction should help the user associate the menu bar icon with the app.

So, developers, just handle it. Because your mental model isn't the only mental model.


This blog's sidebar got a little fancier today: sparklines!

Inspired by infosthetics's google adsense sparklines post, I've added a couple interesting streams of data to the sidebar, both from this website and from my iOS App Store apps. Sparklines are an excellent visualization for conveying a large amount of information in a small space. Here's what I display now:

I'll walk through how I get the data from those sources into sparklines on my blog.

First, I needed to get the data from its original location to my blog's server, in a form Javascript could easily find and read. Using Ruby, I extract the data from its service and then write it in JSON to a local file on my web hosting server.

The site data comes from Google Analytics. Getting data from Google is a bit complicated due to its adoption of OAuth. To access Google Analytics information, I wrote a Ruby script leveraging Garb, a great gem from Viget.

The app data comes from Appfigures. Their API access is based on credentials, but SSL is always required. Below is the Ruby script I wrote to fetch that data.

Second, I needed to visualize the data. I've been looking for a lightweight graphing library ever since I wrote my post about measuring design changes. For that chart, I used Excel and Acorn. It was slow, painful, and manual. No fun at all.

I asked a friend at LinkedIn for recommendations, and he pointed me to Raphael, an excellent open source tool written in Javascript. I used a higher-level version of the tool called gRaphael.

Infosthetics's sparklines used color to give context to the data: black bars for weekdays this month, dark gray bars for weekends this month, light gray bars for weekdays last month, and a single red bar for today. Small but data-rich. I tried to bring a little of that context to my sidebar's sparklines by coloring the weekends in red but the weekdays in gray. Below is the Javascript code that reads the generated text files and creates the graphs:

These scripts are also available through my GitHub repository.

I was poking around ~/Library/Application Support/ on my Mac and stumbled across the XML database for Cultured Code's Things. Apparently, the first to-do I entered in Things was about Things.

While I've emptied the trash quite a few times since I began using Things in 2008, I still have 1.8K to-dos in the app, mostly in the logbook. I thought it would be interesting to plot them by month, using gRaphael.

Each month is a bar, and each January is a red bar. While I average around 50 to-dos each month, I added three times that number in July 2009, undoubtedly wedding tasks. Below is the Ruby script I wrote to process Things's XML library and write out an HTML page with embedded gRaphael instructions:

This script is also available through my GitHub repository.
Apple makes it easy to remap certain keys. The left Command key on my MacBook Pro's keyboard never worked well ergonomically, so I remapped the Caps Lock (⇪) key to Command (⌘). I've been happy with it ever since.

Frankly, I physically removed the left Command key after that to prevent my muscle memory from using it. That worked quite well.



I wrote a Ruby script using Nokogiri to parse ~/Music/iTunes/iTunes Music Library.xml, extracting out tracks with their time and play counts using SAX callbacks. Then I used the excellent d3 to visualize the data as a squarified treemap.

Here's what I came up with: 2,700 tracks played, 28,000 plays, and ...

100 days in iTunes is about 10% of my life, given that my iTunes Library file is three-years-old. Hans Zimmer's scores to "The Dark Knight" and "Inception" represent nearly 20 days, and a single song fills 10 days or 1% of my life. I guess variety kills my focus.

Also interesting is how much I listen to podcasts:

Googling around, I found remarkably few people delving into iTunes as a dataset:

I did stumble upon Planetary for iPad. The free app visualizes your iTunes library as a universe, where each artist is a solar system, each album is a planet, and each song is a moon. The app varies the size of the moons based on play counts. It's a fantastic app from Bloom, a company founded by people from Stamen.

I've included the Ruby source code below. It reads in the iTunes XML file and outputs a JSON file with the track information and an HTML file with the d3 Javascript code embedded.

This script is also available in my bdunagan GitHub repository.

Remind Me Later 1.4 is now live on the Mac App Store and this blog! This release is long overdue, so thanks for everyone's patience. Many people contributed excellent suggestions over the past months, and I included quite a few. In particularly, thanks to the BusyMac folks for helping me integrate with BusyCal better. While there are many new features in 1.4, let me single out three of them: calendars, alarms, and todos.


Below is a list of all the new features in 1.4:

    Type in a [calendar]
    Type in an alarm like @15m, @1h, or @3d
    Type in a "todo"
    Better BusyCal integration
    Things integration with iCal To Dos
    Full set of alarm preferences
    Unclutter the menu bar by hiding the icon
    Menu bar icon correctly inverts when clicked
    Global hotkey works correctly for Shift, Control, Option, Command keys
    Support for "+N{m,h,d}" delays
    Specify a default event duration in Preferences
    Lion (10.7) compatible

Download this new release from the Mac App Store or this blog!

Dollar Clock 4.0, now available.


Dollar Clock 4.1 is now available on Apple's App Store! Included in this release:

    More pictures!
    More messages!
    More random!
    Also, you can tap a message to move on.

I shipped Dollar Clock 1.0 more than a year ago as an experiment. I wanted to submit an app to the App Store, and my friend and I came up with tracking how much money meetings waste. To date, the free app has had several thousand downloads on iPhone and iPad, a moderate success for a mediocre side project with no marketing.

The first iteration of the app was simple. That was my design goal: really simple. There were only two views: the money counting and the settings. I limited input to two pickers for people and average salary. I designed the input this way to force the user to estimate the numbers, rather than be exact. I wanted people to use the app to estimate meeting waste.

The app was simple but lame. It lacked personality. Just look at it. I'm bored already. I even shipped 2.0 and 3.0, and both were such lackluster releases that I don't remember what they included. Something about iPad support and VGA support, I think. And that's my point. Even the developer didn't care about the app.

Recently, I decided Dollar Clock needed some love. The app needed a personality. What goes with a depressing clock counting monetary waste? Interesting images and uplifting messages.

I gave the app a voice. It iterates through a set of Flickr photos, each with a number of messages associated. Here are a couple examples:

Furthermore, I was wrong about input. The most common request for the app is more customization for salary input. My original problem with this was complexity. I didn't want to make the salary input hard, for people to use or for me to code. But I like this new version. People click the "Add Salary" row. They can type in number for people and a salary and save it. It's just another salary row. Simple but flexible.

Dollar Clock is available on Apple's App Store, free for iPhone and iPad. Try it out! Let me know what you think.


Dollar Clock 4.2 is now available on Apple's App Store! Included in this release:

    More motivational messages and pretty pictures
    Even a picture of your meeting from space. Yes, we have the technology.

Remind Me Later 1.4.1 is now live on the Mac App Store and this blog! This version includes time zone support, such as "Meeting 7pm EST", along with a number of fixes for user-reported issues. Thanks to everyone who has contacted me about problems they're seeing. Please continue to do so. 

Download this new release from the Mac App Store or this blog!
I recently had an issue with my SSH connections timing out quickly. At Starbucks, I had no issues, but at home, the connection timed out faster than usual.

First, I wanted to see how fast, so I wrote a short bash script that sleeps for increasing periods of time. I found that the connection timed out after five minutes.

I tried creating a ~/.ssh/config file and telling the connection to stay alive: TCPKeepAlive yes. No luck, because the default time period is two hours.

Instead, I ignored the TCP connection's state and told the SSH client to send a null packet every twenty seconds to keep the connection alive. The SSH client times out if it doesn't receive a server response after ten packets.

Thanks to Andy Skelton for making this tidbit googleable.
There have been a number of excellent articles describing how to restore various older technologies to Xcode 4 (10.4 SDK, 10.5 SDK, GCC 4.0, PPC). Thanks to mecki for doing such a good job detailing the steps. The only trouble is those steps didn't work for me in one case: PowerPC support. Luckily, it's easy to restore.

Here is the short C code I compiled to test the process:

I compiled the C code with LLVM GCC 4.2 (Xcode 4's default compiler), specifying the PowerPC architecture with 10.6 SDK. Then I used lipo to check the binary's architecture.

I installed Xcode 4.1 on my 10.7 system and then copied files from Xcode 3.2.5 on my 10.6 system into /Xcode3 on my 10.7 system. Here are the files I included:

Here are the shell commands that I used to finally get LLVM GCC 4.2 to compile with PowerPC:

One last hurdle was Xcode 4's build settings. I had to specify "ppc" not only in "Valid Architectures" but also in "Architectures", along with $(ARCHS_STANDARD_32_64_BIT).


When I can control the input, I can simply use percent encoding to escape the ampersand (%26). But for dealing with malformed input, Rack needs to rewrite the POST data before processing it.

Thanks to Pivotal Labs for the crucial bits of code.

MySQL defaults to "latin1" as its character set, but at some point, most people want to migrate to "utf8". I realize that there are dozens of posts about how people handled this, and yet, not a single one of those worked completely for me.

I wanted MySQL to use "utf8" for the character set and "utf8_unicode_ci" (not "utf8_general_ci") for the collation, and I wanted it all to work on RDS with ActiveRecord on Rails and Rack.

Here are my steps:

  use mysqldump to extract the old data as "latin1"
  use sed to replace "latin1" with "utf8" in the dump file
  create the new database with the right parameters: character set utf8 collate utf8_unicode_ci
  use mysql --default-character-set=utf8 to pipe the converted dump into the new database

Here is my code:

To verify the character set and collation, you can always query the MySQL variables:

For those using Amazon's AWS RDS for their MySQL instance, you have to create a parameter group with "utf8" values. I'd guess you could just modify the current parameter group then apply it, but I haven't verified that.

ActiveRecord supports an :encoding option in its parameters for ActiveRecord::Base.establish_connection. The option tells the connection to execute SET NAMES as soon as the connection is established, thereby telling the server what the character set the client wants.

However, I also wanted to specify the collation. When I added :encoding to ActiveRecord::Base.establish_connection, collation_connection (from the MySQL variables, not connection.collation) remained as "utf8_general_ci". Some people have indicated that you can specify :collation in database.yml for the establish_connection call, but that never worked for me. I think MySQL bug #34980 prevented it. Others indicated that you can simply add ActiveRecord::Base.connection.execute("set collation_connection='utf8_unicode_ci'") at the bottom of environment.rb for Rails; that also never worked for me. To specify the collation in Rails, I used a before_filter in application_controller.rb. See my code below.

I pieced my steps together from the following helpful links:

I really like using the Ruby whenever gem for managing cron jobs in Rails. But recently I needed to ensure there was only one instance of a cron job running. If it took a bit too long, I didn't want another instance starting up in parallel. Enter flock.

Using ActiveRecord in Rack is one line of Ruby code, and setting up database logging outside of Rails is just as easy. As a minor extension of this excellent tidbit, you simply instantiate the Logger object with a file path. Here's the code:

Here are a couple lines of Ruby to bring Logger into rake tasks. I use them for logging cron jobs that are Rails rake tasks. I tried this post, but it didn't get me all the way.

It's straight-forward to log slow queries in MySQL both on your local machine and on an Amazon AWS RDS instance. I'll walk through both, although MySQL has good documentation on slow query log and log destinations.

There are many blog posts about enabling slow query logging to a file within the MySQL data folder, but very few point out that, for files outside that folder, you have to manually create the log file and set its permissions correctly with touch and chown.

AWS does not allow shell access, so there is no way to read a local log file. Instead, RDS logs the output (log_output = TABLE) to the built-in table: mysql.slow_log. Here are the RDS command line instructions for setting up the parameter group, modified from Inventables' excellent write-up.

Recently, I needed to run rake test on a Ruby gem with both Ruby 1.8.7 and 1.9.2. I wanted to figure out why Travis CI was failing for the gem. Not a problem with rvm, even on Lion (10.7).

To install a version of Ruby, use rvm install:

You can check that it's installed with rvm list:

To use Ruby 1.8.7 for the Terminal session, use rvm use:

Seriously, use rvm. It's awesome.

Over a year ago, GitHub updated their wiki system to be powered by git. It was an awesome upgrade. Now I can check out a wiki, just like source code, and edit it in TextMate, just like source code. Really, really nice...except I can't search.

Luckily, GitHub released Gollum. Gollum is a local version of their wiki software, wrapped up in a Ruby gem. It supports the same interactions that you get on a GitHub wiki. And, it supports search.

To set up Gollum, just open Terminal and follow the instructions below, using @mojombo's jekyll as an example project.

Gollum is now running at http://localhost:4567/. You should see the search bar at the top. Here are two screenshots comparing Gollum to GitHub:

Still, remembering to run gollum and open localhost:4567 is a bit of a pain.

With 37 Signals' Pow web server, you always reach your wiki locally at http://wiki.dev/. Pow is ridiculously nice for keeping long-running Rails and Rack process running in the background. After I installed Pow using their awesome one-line instructions (curl get.pow.cx | sh), I used these instructions to integrate Gollum:

First, add config.ru to the base of your wiki repo with the following text. (Use the jekyll wiki repo from above. No need to commit the file. Just add it to .gitignore.)

Next, symlink the project, so that Pow knows about it. (Any folder name is fine, but "wiki" is simpler.)

Finally, touch tmp/restart.txt to tell Pow to restart. (I'd add tmp/ to .gitignore as well.)

Contrary to the linked post, I didn't need to add anything to /etc/hosts. Pow just worked. Gollum 1.3.1 is fine, despite the comments in this ServerFault post. Just open http://wiki.dev.

A brilliant addition to Gollum is sidebar support. I committed _Sidebar.mediawiki to the wiki repo, and Gollum began displaying it next to each page. The sidebar is a really easy way to add a set of useful links to all pages.

Keep in mind that if changes aren't committed, Gollum doesn't see them. For example, I added _Sidebar.mediawiki to the repo folder and refreshed the browser page. No sidebar. Took me a bit to realize Gollum still didn't see it. You have to commit changes for them to exist to Gollum.

GitHub spells this out in its , but it's worth reiterating. Gollum sanitizes the input with @rgrove's sanitize gem, removing any malformed HTML or security issues like custom CSS. This sanitization converts red into red.

To change that setting locally, just find lib/gollum/sanitization.rb and add 'style' to the end of the ATTRIBUTES list. Your local version will use the custom CSS, and the GitHub version will silently strip it out.

I switched to a GitHub wiki from a MediaWiki instance. Converting was easy: click Edit for each page, save the contents as Page.mediawiki, repeat, push to GitHub. Done.

GitHub is able to render MediaWiki markup due to @nricciar's handy WikiCloth Ruby gem. The latest release is 0.7.0, but it has a couple issues. I submitted a couple fixes in a pull request today, and @nricciar already merged them into master. To use the latest WikiCloth sources as a gem for Gollum, follow these steps:

Of course, GitHub will add searching on the website eventually. They're prodigious coders, and Issues already has search. I'm sure they're getting to it. Until then, just open http://wiki.dev/ and search.
A while ago, I found a great Stackoverflow answer for sorting a hash in Ruby:

But recently, I wanted to take it a bit further and recursively sort all the hashes in an object, even if they are inside of an array. Here is a snippet to do that:

Ever lose an SSH connection due to a long-running job? Start using screen. It's an extremely handy Linux app that lets you create and manage terminal sessions. Much like you can VNC into an already running remote Mac UI and RDC into an already running remote Windows UI, screen allows you to SSH into a server and reattach to an already running terminal session. Awesome. Here are some handy shortcuts:


**Update**: 

Back in April, Panic shipped a beautiful SSH app for iPhone and iPad: Prompt. Quite a few comments on that announcement blog post concerned EC2. Many people wanted to SSH into their Amazon cloud, but they couldn't figure out the keypair situation. A day later, Joe Cheng covered steps that worked for him. Unfortunately, his steps didn't work for me. Here is what I did:

KEYPAIR	keypair-name	12:34:56:78:90:12:34:56:78:90:12:34:56:78:90:12:34:56:78:90
-----BEGIN RSA PRIVATE KEY--
I recently switched over to Google Apps at work, and one obvious move was migrating everyone to Google Talk/Jabber. Seemed easy enough. I was sure there would be a checkbox in the Domain Dashboard's Settings for that. I just wanted every domain user to have everyone else show up in Gmail Chat, Google Talk, and Jabber. I eventually translated this goal into this search: "automatically invite and accept all shared contacts". Nothing.

Google Apps does not provide that level of support for domain-based chat. The service doesn't even let domain users see a list of the other domain users. The Google Apps Directory (analogous to Exchange's global address list or GAL) is only available from search and auto-complete and from their API. ("Contact Sharing" is enabled by default.). Their Contacts app is just not as advanced as the rest of their awesome services. Which is fine. They do provide an API and an app store: Google Apps Marketplace. Marketplace has several highly-rated Contacts apps (like SherpaTools and Shared Gmail Contacts), and Jabber provides a mechanism to invite users. Problem solved.

Still, figuring all that out wasn't fun. I read many help articles, forum posts, and API docs before I was able to map my problem onto Google's solutions. Stepping back, here are three lessons to think about:

Google Apps is composed of apps and APIs. Ideally, both would be full-featured. Unfortunately, development resources are often far more finite than Product Managers and customers can bear. You have to prioritize some features (a full API) over others (a full Contacts app).

In this case, Google very clearly likes APIs. For years, Google Apps has provided an API for internal people (Domain Profiles) and for external people (Shared Contacts). While the Google Apps' Shared Contacts UI needs a bit of love, the API and app store allows other companies to write third-party apps to solve this problem. I'm sure the Google Apps team is working on revising the contact manager, but they're also busy shipping features like Google+ support.

Moreover, I could accomplish my goal. I enabled "Automatically accept chat invitations between users" under Settings/Chat (Premier only), and then I wrote a quick Ruby script to invite coworkers with Jabber:

Design decisions are choices. The Google Apps team chose a complete Contacts API over a full-featured Contacts app. Customers appreciate finite resources even less than Product Managers. The key is to keep them in the loop.

Yes, there is a way to solve my chat invitation problem and to manage Shared Contacts. No, I didn't know what I was looking for. Searching through Google Apps' forum site, I found dozens of conversations about how woefully inadequate the contact manager was. Companies were resistant to switching from Exchange to Google Apps solely because they would miss so many nice contact management features. These thread span from mid-2009 to last week.

However, I didn't find a single reply from a Google employee, pointing the thread in the right direction. Google Apps' Help states their position plainly enough: "As an alternative to using the APIs, explore the Google Apps Marketplace for products that can help you manage Contacts through a user-friendly interface." But all those customers need an official voice to guide them. The Google Apps team chose APIs over apps, but it hasn't been telling anyone on its own forums. I only figured out what I should be searching for by reading those dozens of posts.

You might have a great service, a great API, great documentation, and great forums. But if you're not in the forums, answering questions, pointing out documentation, how will your customers understand how to solve their problems? It's your job to translate their problems into your solutions and to set expectations.

Google Apps is an interesting service. It is a clear example of how difficult it is these days for corporate services to keep up with consumer ones, as Google is behind both of them. When Google+ debuted, a number of Google Apps customers were upset that the feature wasn't immediately available on their Apps accounts as well. But Google Apps is a different service. It's geared at businesses but used by a wide variety of people with wildly different expectations. The team needs to manage expectations better.

The Google Apps team is trying. In March 2011, they revealed their release schedule. Moreover, customers can now choose whether to get features immediately or on a scheduled cycle, and they can find out what's next. Frankly, flipping through the schedules makes me appreciate how many services Google Apps covers now. Think about all those services they added a year ago. I bet it was a monumental task.

And yet, when I started my search for auto-invites for chat, I expected Contacts to be a full-featured app in the feature set, rather than a third-party tool. I resisted using the third-party tools, and it took me a bit to accept that the Google Apps team really wanted me to head in that direction, especially since they didn't indicate it very clearly. My expectations were wrong.

Think about what expectations your customers have. Give them a forum to vent. You'll quickly find out what they expect. Then do your best to manage those expectations.
Recently, my team needed to migrate from Atlassian's JIRA to Mozilla's Bugzilla. I expected this process to be uncommon but not unheard of. Not so. Atlassian has a nice tool for migrating Bugzilla data into its JIRA system, but the one googleable conversation I could find about migrating JIRA to Bugzilla was in 2006. 

I'll describe the process I used to migrate around 6K bugs.




An alternative to this export is using curl to export issues individually. This process includes comments and involves lots of quick JIRA queries. As an added bonus, I can avoid any XML SAX state logic that a single large XML file would have needed, so I can focus on transforming issues into bugs in isolation. Sweet.



At this point, we have all the tools and the data we need to tranform JIRA issues into Bugzilla bugs. I committed the full jira2bugzilla.rb Ruby script to a GitHub repo. It doesn't work out of the box, as there are quite a few instance-specific variables, but the script provides a nice base. I'll touch on a couple points:


Again, see this GitHub repo for the full jira2bugzilla.rb script. I simply run ruby jira2bugzilla.rb in the directory with all of the JIRA XML issues.

At this point, we have converted all the JIRA issues into Bugzilla bugs. Next, we transfer them onto the Bugzilla server into a subdirectory of the bugzilla installation, like bugzilla/bugs/. To import the bugs, I ran the following one-liner. It finds all the XML files, sorts them alphabetically, and feeds each one by one into the Bugzilla import script.

The script imported 6K bugs in two hours. However, I didn't include attachments. When I tried to import a single bug with a 5MB attachment, it took around thirty minutes. With multiple gigabytes of attachments, I opted to not include them. 

Most importantly, check the log. The above one-liner pipes all of the log information to bug-import.log. Run tail -f bug-import.log while it's running to make sure the process is working. Grep through the output afterwards for terms like "Bad" or "Error" to ensure all the bugs were imported. With 6K bugs, most imported correctly, but a few did not.

I just wanted to highlight that migrating away from Atlassian's JIRA is not a reflection of their product. I was simply tasked with making it happen. Frankly, Atlassian has been doing an amazing job at steadily growing into a vertically integrated company. They received $60M in VC funding in 2010 to grow their business, and they've scooped up businesses like BitBucket and SourceTree. Just a couple weeks ago, they announced that their main products will now be available as cloud services. Atlassian seems to have a long-term strategy that they're executing very well.
A while ago, I followed this blog's advice and added a little color to 'git log'. This original 'git log' command displays useful tidbits like 7c UUID, relative date, description, tag, and user.

But today, I wanted a bit more: an short absolute date and lines changed. Seeing the number of lines that changed per commit gives a better idea about the magnitude of the changes. Changing the date format was easy; adding the number of lines that changed was harder.

Below is a gist for the Ruby script I wrote. Instructions on setting it up as a Bash alias are embedded in it.

No, not the iOS Personal Hotspot. I meant the other direction. Here is how to share your Mac's internet connection with your iPhone or iPad.

Simply go into System Preferences and click on "Sharing". In "Sharing", select "Internet Sharing" and click "Wi-Fi Options..." to configure the network name. Then, just check the box for "Internet Sharing".

Very useful in some instances.


Amazon's AWS S3 is awesome. However, there are times when the API makes certain tasks a bit hard. Today, I wanted to delete a bucket with 100k files. AWS does not support deleting a bucket with files inside; you have to delete the files first.

First, I tried aws/s3. It supports :force when deleting buckets, in case they have files in them. That call was taking a long time, so I poked around the code. Unfortunately, the gem didn't make some magic API call. It just looped:

Next, I tried to find out how geemus's brilliant fog gem handles this need. It doesn't, because providers don't.

Finally, I gave up that one API call dream and looked for what other people do. They use pagination and threads, and I ended up forking SFEley's s3nuke script from GitHub. In the process of porting his solution from RightAWS to Fog, I discovered that fog handles auto-pagination internally. No need to mess with is_truncated or :marker. Quite nice.

This Ruby script deleted an S3 bucket with 100k files in 15 minutes.


Xcode 4's "Build and Archive" feature is extremely convenient. With it, I can build a distribution version of an app and archive it away, so that later, in Xcode Organizer, I can click on that instance and submit it to the App Store. Brilliant for individual developers. But at my work, we wanted a bit more: automation and multiple IPAs from a single binary. Both are easy with xcrun, part of Apple's Developer Tools. (Thanks to Stackoverflow for the pointer!)

At work, we wanted a standard set of keys and certificates to use for official builds. I went through the standard Apple Developer Center process of creating a certificate signing request with Keychain Access to generate the local keypair, and I created a distribution certificate with that signing request and loaded that into Keychain Access. Then in Keychain Access, I exported the certificate as distribution.cer and the private key as distribution.p12. Xcode needs these two files to correctly use the provisioning profile.

One small hiccup was "login" versus "System" in Keychain Access. I easily imported those two exported files into the "login" keychain in Keychain Access. However, Keychain Access would not import the private key into the "System" keychain and gave the following error: "An error has occurred. Unable to import an item." I needed to use the "System" keychain, as our build system runs with elevated privileges. To get around this error, I simply needed to import the certificate and private key via Terminal instead:

Don't forget to change the "Access Control" for the private key to "Allow any application to access this item" for "System.keychain". Otherwise, you'll get a build error about "user interaction is not allowed".

At this point, I had a copy of the certificate and private key in "login" and "System" in Keychain Access. I simply needed two provisioning profiles based on that certificate: an ad-hoc profile and an app store profile. I generated both in the iOS Provisioning Profile, Adhoc.mobileprovision and Appstore.mobileprovision, and imported those into Xcode.

With the certificate, private key, and two provisioning profiles loaded up, automating the build was straight-forward. In fact, by selecting the correct default configuration (Xcode 3.2.6), I can simply run Xcodebuild with no arguments to generate an App Store distribution app bundle. 

Automated builds are nothing new or novel. There are some great suggestions for how to structure your Xcode configurations to simplify automation. What I really wanted were two files at the end: App.AdHoc.ipa and App.AppStore.ipa, ready to be tested ad-hoc or submitted to the App Store. Enter xcrun.

Apple distributes xcrun as part of its Xcode Developer package. The command line utility will take an app bundle and convert it into an IPA file with the supplied provisioning profile. The key part is the supplied provisioning profile can be different from the original one. Here is how I call it:

With this command, I can run a build and then generate two IPA files: one for ad-hoc distribution or one for App Store submission. All with the same binary. The community and the tools have progressed a bit since furbo's "The final test" in 2008.

Apple added support for over-the-air (OTA) distribution in iOS 4. It's the best way to distribute builds to QA, much better than syncing through iTunes. These instructions worked perfectly for me. You don't need an Enterprise iOS account to do this; I think that simply lets you get around managing UDIDs. Seriously, it's easy and awesome.

Apple's iOS Mail has a universal inbox on iOS 4. Very handy in general, but its unread badges complete the feature. I frequently glance down the list of email addresses I have and know exactly what the state of my inboxes are. I never have to wonder how many new emails I have. I just check the numbers.

Recently, I needed to duplicate that feature for work. I found two existing solutions:

I ended up forking DDBadgeViewCell. It was concise, and I only needed to make a couple changes to it. Specifically, I wanted the badges to look exactly like Mail's badges, in terms of alignment, colors, and shadows. I'll go through my changes here, but you can check out my GitHub fork as well. See the image for the resulting look:

I wanted to duplicate Mail's look and feel, so I needed the exact colors that Mail uses for its badges. xScope made it easy.

If you look closely at Mail's badges, you'll notice the top part of the arc looks indented. The shadow gives the label a feeling of depth; the badge looks almost letter-pressed. To duplicate that look, I simply drew a darker oval one pixel higher than the original before drawing the real one.

DDBadgeViewCell does an excellent job when there are three or more characters in the string. But as I wanted to display numbers, I needed to special-case the code a bit to get the pixels correctly aligned.


On the iPhone, it's quite normal to have an app slide a sheet up to create something or change settings. But on the iPad, that same view should be displayed in a popover. In a universal app, the key is shared code and XIBs for both approaches.

The code below is all that I needed to have a Settings sheet appear correctly in both an iPhone and iPad app. I simply identify the device type and then either present a popover controller or a modal view controller, using the same XIB. Clean code. Consistent interface.


Rails 3.0 handles routing errors (ActionController::RoutingError) differently than Rails 2.3. The middleware addition means the error is passed to ActionDispatch, instead of ApplicationController. A bit ago, I actually removed the catch-all route at the bottom of routes.rb because I thought the ApplicationController::rescue_from handled any routing errors. Not true. Rails 3.2 doesn't resolved it either.

There is an open GitHub issue for it: "Can no longer rescue\_from ActionController::RoutingError". (In fact, it's old enough to be an issue migrated from Lighthouse.) There are several solutions posted inline in the issue's comments. JosÃ© Valim from the Rails core team suggests simply adding the catch-all route back to routes.rb; others suggest overriding ActionDispatch::ShowExceptions::render\_exception. All seem focused on rendering the 404 page manually. However, I wanted to make rescue_from work. My solution is the catch-all route and raising the exception manually.


At WWDC 2009, Eric Hope talked about crafting the best iPhone apps at his iPhone User Interface Design session (the one where Cultured Code likened public speaking to kryptonite). The first step was coming up with an Application Definition Statement. It should define what your app is and what it isn't. The statement should include a differentiator (your app's strength), a solution (the problem you're solving), and an audience (the people who care), or as my Ruby-esque notes read, "#{differentiator} #{solution} for #{audience}". The goal of the statement was to focus your app. Mac apps could have a long list of features, but the iPhone was a far more constrained environment. Hope's example was iPhoto. On the Mac, it views, edits, shares, prints, and what not; on the iPhone, it shares, nothing else.

That talk was almost three years ago. Things have moved forward. The iOS environment is less constrained, particularly with the iPad. Apps can do more, and iPhoto is a great example. In March, Apple released an iPhoto for iOS that contained editing abilities.

Retrospect for iOS has followed a similar path. What started as a very simple app has grown up a bit.

I wrote Retrospect for iOS 1.0 in December 2009, just for fun. For the Application Definition Statement, I came up with "a serious tool to monitor multiple Retrospect servers". No control, just monitoring. I focused on activities. Viewing activities for a Retrospect server gives administrators a high-level view into what's going on. If everything looks good, no action required; if not, they should launch Retrospect for Mac to dig deeper. Our designer hand-crafted a set of awesome icons, and we submitted it to the App Store.

The app was a moderate success. It was downloaded around 5,000 times and received a glowing review from Economist writer Glenn Fleishman (a long-time Retrospect user), who praised its "outstanding distillation". The app didn't do much beyond viewing activities, but it did that task well. Still, it didn't replace Retrospect for Mac, and it didn't support the iPad. Just as Apple updated its definition of what iPhoto for iOS was, I recently decided it was time for Retrospect for iOS 2.0.

What did the first version lack? Control. Complete monitoring. Better server support. iPad support. That's the list I came up with in December 2011. I decided the app should display everything that the Mac version did: Activities, Past Backups, Scripts, Sources, Media Sets, Devices, and Reports. I even added badges to each report. I added full support for the latest Mac and Windows Retrospect servers and partial support for a couple versions before those. And I finally enabled control within the app: running scripts. I submitted v2.0 in February.

Close but not quite. While most of the new features were significant improvements, one failure erased all of those gains: syncing. Retrospect stores quite a bit of information, and when I expanded the list of items displayed on the iOS app, it translated into syncing times that were 10x to 100x longer. The UI prevented the user from looking at a server until it was synced, so the user might launch the app but then wait a long time before being able to monitor the server. Moreover, the iPad UI did not take advantage of the space. It looked like an iPhone app at "2x" but without the pixelation.

With v2.1, I fixed those two issues. Syncing is now done by item type. Users can view activities before media sets finish syncing. I also added split-screen support to the iPad UI to make better use of the space available.

This latest version of Retrospect for iOS is now available on the App Store. If you use Retrospect and have an iOS device, give it a try. It's free.



I didn't plan on 18 months of silence on bdunagan.com, but it seems to have worked out that way.  Things have been quite busy at Retrospect, Inc., where I work. Lots of changes.

Also, if you don't backup your computer (or your business), well, you should. Try . It works.

> To move forward, you must backup.

I started bdunagan.com in 2008 using Wordpress. Made sense. Wordpress was a solid blogging system that I could self-host. The anti-spam comment system, Akismet, was exceptional. There were lots of plugins to extend functionality, like syntax highlighting and related posts. I liked the control it offered, compared to other sites like Tumblr, and by self-hosting, I could tweak the PHP when necessary. Wordpress worked well.

By 2010, I found myself editing the live site a bit too often. I switched to using a staged site—slight overkill but whatever—and managing the system with Git. I could make changes on the staged version, test them out, commit and push them, then switch to the production site and merge them in. I could also backup the Wordpress database using "mysqldump --skip-extended-insert" to see individual row changes in the diffs. I wrote a post about the . Using Wordpress with the Git safety net worked better.

Five years on, I'm tired of Wordpress. It's a complete content management system, designed for people who don't code, and I'm not the target audience. I want something closer to the metal.

### Enter Jekyll

(Not that Jekyll is new. 's first commit was two months after I started using Wordpress. I'm a late adopter.)

Jekyll is optimized for coders. It installs as a Ruby gem, leverages other gems to provide modular features like markdown support and syntax highlighting, and compiles text files into static files that you can host wherever, including @mojombo's company—GitHub. Clean and transparent.

Migrating from Wordpress to Jekyll was remarkably easy.  converted my local Wordpress db into a set of markdown files under _posts. I replaced the old markup for syntax highlighting with the new markup and de-htmlify entitles like . I also updated Jekyll's config.yml to use my Wordpress-style permalink, so that existing Google results continue to work. For a theme, I went with . It vaguely matched my Wordpress theme (a variant  on the popular DePo Clean theme), but scribble was far cleaner and more responsive.

Overall, it took a day of futzing with Wordpress and Jekyll and then another day of futzing after I got the content into Jekyll.

Onward.

I use WriteRoom to focus on writing. As its creator, Jesse Grosjean, bills it, WriteRoom provides distraction-free writing. I like it a lot. And I didn't have any issue with it, until I switched to Markdown.

I opened a new file. I saved it as "test.markdown" on the Desktop. Then I saved it again. Error.

As the dialog says,

> The document "test.markdown" could not be saved. You don't have permission.

To shorten the painfully long saga, it's a  with sandboxing. Moreover, it's not just a bug for WriteRoom;  had this issue as well, although they fixed it. The error messages in Console.app suggest the issue is misuse of :

I tried a number of hacks to resolve the problem. I registered ".markdown" and WriteRoom with Launch Services. I poked around WriteRoom's Info.plist. Nothing fixed it.

So, I bought Byword. That fixed it, in WriteRoom. Seriously. Not sure what Byword changed, as the Launch Services plist is the same, but it indicates the problem is extension registration, not API misuse. I can save Markdown files with either Byword or WriteRoom.

I am writing this in Byword, though.

On March 31, 2008, I joined Retrospect. It was six years ago today.

Six years. I was at MIT for five. I was at WildPackets for four. Six years is a long time for me. My coworkers laugh at that. Many of them have been working on Retrospect for more than a decade and almost two for several of them. Retrospect has been around since 1989.

My actual employer has changed quite a bit. In 2008, I worked for EMC. In 2010, EMC sold Retrospect to Sonic Solutions, which in turn got acquired by Rovi the same year. Then in 2011, Rovi spun off Retrospect into Retrospect, Inc. That's where I've worked since November 1, 2011.

I joined Retrospect as a Senior Software Engineer. I wrote the user interfaces for Retrospect for Mac, Retrospect for iOS, Retrospect Client for Mac, and Retrospect Client for Windows. Along the way, I rewrote retrospect.com. In August 2013, I became Vice President of Strategy. I still work on the various user interfaces, but now I manage Product Management and our overall strategy as well.

Retrospect has had a turbulent past. But things are going quite well now. It's a great place to work. We're small enough for everyone to know everyone. We work from home three days a week. And we are responsible for a product that has been around for twenty five years.

Above all, we decide our own direction.

The Retrospect codebase has been around since 1984 (counting DiskFit) when Dantz Development was founded. Thirty years of code.

Our original version control system was naturally Retrospect. When our Windows development team spun up in the 90s, they used Visual SourceSafe; we made sure to back that up as well, which saved us multiple times. In 2000, both the Mac and Windows teams switched to CVS, and in 2008, I migrated the various code repositories to SVN.

In 2013, I migrated Retrospect from SVN to Git, in the form of GitHub Enterprise. A year later, we still comment on how much better Git is, and GitHub Enterprise has proven itself to be worth every penny.

SVN is old. It was an excellent replacement for CVS, but it's old. Branching isn't cheap. Repos aren't distributed. History isn't local. It was time for a new version control system. We chose Git, mainly because the CEO and I were already familiar with it and with GitHub.

All included, we had 27 SVN repos to convert. I used nirvdrum's , which worked very well. The main repo took 10 hours on a MacBook Pro SSD. It had 33k commits, dating back to 2000. (There was no vss2cvs like we have cvs2svn and svn2git.)

First, I dumped the SVN repos on the SVN server and transferred them to my laptop.

Next, I collated the authors list for each repo.

Finally, I wrote a Ruby script to manage svn2git. I ran it manually for every repo, so that I could check the state of the new Git repo and log before moving onto the next conversion.

I included these snippets of Ruby and Bash code to help others along, but none of them just work. Each repo migration is unique to the codebase and engineering team. So are the issues. Luckily, I only encountered three problems:

* *quotes in commits*: svn2git had an  with escaping quotes and double quotes from commit messages. I fixed the issue with the code from the pull request.
* *bad commits*: Three repos had a bad commit around 2001 that resulted in the following error message: "Filesystem has no item: Working copy path 'branches/dantz' does not exist in repository at /usr/local/git/lib/perl5/site_perl/Git/SVN/Ra.pm line 282". I tried running svn2git on the first set of commits in the main repo then rerunning it after the bad one; the second run took 80 hours to fail. I ended up splitting the repos on these commits to deal with them.
* *line endings*: We had to change 30% of our files to normalize the main codebase to LF. We added * text=auto to .gitattributes.

The entire process took a week. I spent the first few days figuring out the best migration strategy. The actual conversion only took a day, so the team held off on commits until it was done.

Then, I ran git push --all to GitHub Enterprise.

GitHub Enterprise is GitHub running on an internal server. It works just like Github.com. Same feature set. It costs $250 per person per year, sold in packs of twenty. We migrated to GitHub Enterprise a year ago, and we've been extremely pleased with it.

We chose GitHub Enterprise because the CEO and I were already familiar with GitHub. (I migrated our website to it six months earlier, and we work on it together.) In particular, we found pull requests and inline discussions immensely useful. Pull requests encapsulated features, and discussions happened inline and in context. We didn't need to exchange long emails with code pasted in. Everything happened in GitHub.

That's what we were missing: social coding. Git provided cheap branching, distributed repos, and local history, but GitHub added a set of tools that made code discussions seamless. Our development team tried code review tools in the past, but none lasted long. The tools were not integrated into our existing workflow, and they were awkward to use. GitHub doesn't feel awkward. Pull requests for feature branches seem obvious, and discussions are natural. And the 800 emoji that it supports make them fun. We frequently post " it!" on pull requests. In fact, during our recent release, we merged our 500th pull request. Pull requests are what we paid for.

The second-order feature set is where GitHub gets better: Contributions and Pulse. These give everyone on the team a high-level view on every developer and project, and we've discovered interesting patterns in both views. I get that all this data was already there, even when it was locked away in svn log, but I never took the time to visualize it. The News Feed lets me see what's going on right now. Pulse aggregates that over a month. Contributions aggregates it over a year. The data visualization is automatic.

Below is my own Contributions page:

If you're still on SVN, think about migrating to Git. If you use Git, consider GitHub Enterprise. Both proved worthwhile investments for our team at Retrospect.

No commute. No distractions. Just you and your computer and maybe your cat. If you're like me, you've read about the many virtues of telecommuting, maybe in Remote Work by 37 Signals or GitHub's blog posts, and you think it sounds like a great gig. Well, it is, but there's an asterisk. Reading about telecommuting and living it are a bit different.

I work from home. Every day. I live in Cambridge, England, while the Retrospect team is located in San Francisco, California. I moved to England three years ago. Since then, I've only visited the office a handful of times. All other interaction is online.

Remote work is a great gig, but it does take effort and discipline to make it work. Here are three things to keep in mind.

Commuting is exhausting. I've used both public transit and car: forty-five minutes on subway (BART), two hours on train (Caltrain) and subway, and an hour by car (680). All three variants were exhausting. They sapped hours from my day and required constant but mindless attention. Mass transit included. Riding a train or subway resolves the every-second-requires-attention problem of driving, but other issues crop up to prevent extended working or relaxing, from remembering to get off at the right stop to overhearing loud phone conversations to simply finding a seat. Not fun.

With telecommuting, you can work at home. My current office is in my house. It works well, with no travel time. Some prefer a different environment, like a coffee shop. I used to work at a Starbucks on my occasional work-from-home day in San Francisco, but the commute wasn't negligible. Driving took ten minutes; parking took almost as long; finding a table was hit or miss; wifi involved setup. All that time adds up.

Measure your commute in minutes from end-of-morning routine to at your computer working.

Offices are distracting. You get into work, chat with colleagues, have an ad-hoc meeting, go out to lunch with more chatting, grab coffee, chat more, have another ad-hoc meeting, have a scheduled meeting, and sometimes you are at your computer. Things just come up, every day. I worked in a cubicle for six years before the move to England, and I rarely had more than an hour without interruption. Uninterrupted time is essential for complicated mental tasks. For this individual contributor work, telecommuting removes the distractions that plaque office environments and lets people focus. Being remote lets me focus.

Your remote environment needs to minimize interruptions and distractions. I aim for two to three hours of uninterrupted work before lunch and after lunch. My situation is a bit unique with the rest of my team offset by eight hours, so their 8am is my 4pm. After 4pm for me, interruptions become inevitable. I consider this ideal: a solid amount of time for focused productivity with a couple hours left over.

You need those extra hours because not all distractions are bad. Ad-hoc discussions are good. Hallway chats are good. Getting lunch and grabbing coffee with your team are good. These activities build better products and better teams, and none of these are natural for remote workers. Instead, you have email, IM, voice, and video chat–all good distractions in moderation. One reason 37 Signals built Campfire was to create a more cohesive distributed team; that's why GitHubbers use it. Chat rooms make distributed chats far more natural. I have daily video chats with my team in San Francisco, and I would feel much more disconnected without them.

Minimize distractions but only in moderation.

Use good tools. You want to work from home? Get a comfortable chair, a nice desk, a good computer, an external monitor, and a pair of headphones. Without good tools, you'll be less effective, and you won't like work as much. I have a large IKEA desk and a Herman Miller chair. Yes, Herman Miller is expensive, but I can sit comfortably in the Mirra for ten hours a day.

Your company should use good tools as well. Retrospect uses GitHub Enterprise for source control, Bugzilla for issues, Mediawiki for internal documentation, Google Apps for communication, and Heroku for the website. Each deserves a bit of detail:

* **Git and GitHub Enterprise**: As I said in a , these tools are excellent and ideal for telecommuting.
* **Bugzilla**: JIRA is more common these days, but the team has used Bugzilla for more than a decade and is very comfortable with it.
* **Mediawiki**: Mediawiki centralizes all of our internal documentation (including MRDs and PRDs) into a versioned diffable repository, and watch lists alert people to changes over email.
* **Google Apps**: It costs $50 per person per year, and that would be worth it for just Gmail. We would pay the same for just Google Hangout as well. We also use Google Calendar for all meetings.
* **Heroku**: Heroku makes it trivial to have a Rails website. It costs more for sufficient resources, but updating the website is literally pushing a branch to a remote Git repo. They take care of the rest.

Retrospect is a small company, and we get to decide which tools work well for us. Without these tools, remote work and distributed communication would be much more difficult.

Remote work is not a panacea. If you work from home on the weekend, it's remarkably similar. Just think of it as every day. There are many advantages. No commute. No distractions. You can sit on a nice chair next to your dog. But collaboration and camaraderie are hard. There are no hallway meetings or chats over coffee. Telecommuting takes discipline and balance to get those hours of focused productivity while maintaining good team communication. Good tools help.
Commits should always be logical, atomic changes, but parceling out your working copy into logical commits is often a struggle. Adding individual files to a certain commit is straight-forward, but adding just a subset of changes in a given file while not losing the other changes is more difficult.

In the past, I had two strategies. Sometimes, I copied a file from Xcode into TextMate, reverted unrelated changes in Kaleidoscope, committed the logical piece, then pasted the original file back into Xcode. Bit of a pain, and it didn't scale well to many files. Other times, I counted on Undo (Command-Z) to remember what Kaleidoscope clobbered. Undo didn't always remember what I thought it should.

Recently, I came up with a cleaner strategy using git stash apply:

1. Make many overlapping changes and decide to commit a number of chunks.
2. git stash – This saves your working copy as a temporary commit.
3. git stash apply – This restores your working copy while keeping the temporary commit.
4. Revert all unrelated changes and commit the first piece.
5. git stash apply – This restores the original working copy again and silently merges in your matching changes.
6. Repeat (4) and (5) until your commits match the working copy.
7. git stash pop – If your commits match your original working copy, the stash will be dropped without issue, and your working copy will remain empty.

  

I found some time last weekend to update Dollar Clock. Version 5.0 includes:

* Hourly or yearly salaries
* Post to Twitter or Facebook
* Support for iOS 7 and iPhone 5
* Currencies and Settings in 11 languages: English, German, Spanish, French, Italian, Japanese, Korean, Portuguese, Russian, Chinese (Simplified), Chinese (Traditional)

I use Terminal for Git on my Mac. I know there are well-crafted app alternatives like GitHub for Mac and SourceTree, but Terminal works quite well. Except for branch names. Until this week, I had a terrible workflow for switching branches:

1. git status – Check the working copy just in case.
2. git branch – List all branches to get all branch names.
3. Use my cursor to select a new branch's name. Copy.
4. git checkout – Type that then paste the branch name.

Step 3 is bad. I use the cursor and interrupt typing. I could attempt to type the branch name, but with long descriptive ones that include bug numbers, it gets quite frustrating to be off by a letter or number a couple times in a row. And as I discovered this week, there is a better way: Git auto-completion.

I already had Terminal aliases like gs for git status and gl for git log (with a ). But somehow, I missed the convenient, drop-in  that allow you to auto-complete commands and even branch names in Terminal for bash, tcsh, and zsh.

Here's how to integrate the bash version:

1. Download the .
2. Name it ~/.git-completion.bash.
3. Add source ~/.git-completion.bash to ~/.bash_profile.
4. Relaunch Terminal.

Now you can tab to auto-complete branch names. The first time I did this, I saw far more branch names than I was expecting, branches that I thought I had deleted. Well, I had, but I hadn't pruned. Run git remote prune origin to clean those up, and you're good to go.
Retrospect.com is around 400 pages, including the main site, knowledge base, blog, and documentation. Our software supports six languages–English, German, Spanish, French, Italian, and Japanese–and the site does as well. We wrote it in Rails three years ago, but the initial version–then 200 pages–had separate pages per locale, using the page.en.html.erb . Any HTML change needed to be duplicated across six files, and localizers introduced the occasional HTML bug.

When the site began to grow, separate language files became unsustainable. But with no prior experience, the best solution was not obvious to me. I'll go through what failed and what scaled.

I started very simple:

Yes, the completely naive approach. I just wrapped t() around everything. t() is shorthand for I18n.translate(), and  is the Rails gem for internationalization. Wrapping a phrase works; wrapping a paragraph does not. The difference is a paragraph includes periods, and I18n considers periods to be delimiters. t() treats the phrase argument as a key, and periods scope that key. t() looks up the key in config/locales/*.yml

Here is a simple example: localizing a welcome message.


In this example, t() looked up "homepage.welcome" in en.yml. It found the "homepage" block then the "welcome" key and returned its value. Changing the locale changes the YAML file used during lookup.

Back to the original example, wrapping phrases with t() does not scale because of the period delimiters. "That Didn't Work" appears in the HTML because Rails looks up "I wrapped t() around every phrase" then " That didn't work" then "". None exist in en.yml, but Rails has a coping strategy to make me look a little less stupid: capitalization. It ignored the final empty string, takes the final non-blank part of the period-delimited key, and returns the capitalized version of it. Hence "That Didn't Work".

That was my issue. I needed a solution beyond Rails' toy examples, something that scaled to thousands of strings across hundreds of pages. I did experiment with changing default_separator for I18n, but that hack didn't resolve the consolidation issue. I needed all the strings from the localized pages to be values in a lookup.

Of course, there was no simple solution. For a very tedious week, I migrated 10k strings into their respective localized YAML files across hundreds of pages. For each, I created a delimited key based on their page, like "products.win.features.platform_support".

However, I did not use a single locale file. Instead, I created a folder structure under config/locales to separate the sections of the website, like so:

Thanks to a , I added the following line to application.rb to ensure Rails read in the entire folder hierarchy:

With all of the localizations stored in config/locales, I could reduce the site to only one file per page. Site changes went from complicated six-fold operations to easy tweaks.

Presentation and content were separate.

After switching to YAML files, localization was straight-forward as well. We used to send our webpages to a large localization service. The translations would take weeks, cost quite a bit, and often introduce HTML bugs. With the site content in YAML files, I switched us to Gengo.

Gengo translates content within a few hours at low cost. The highest rate it charges is still a fourth of the cost of our previous localization service, and their turnaround is within the day. Gengo doesn't support YAML files explicitly, but it allows text to be excluded from translation via "[[[three brackets]]]". I wrote a short Ruby script to reformat our YAML files into a form they accept and translate the outcome back.

Gengo is simply amazing. Compared to our previous service, it has saved us thousands of dollars and months of waiting. Other translation services, like Transifex and PhraseApp, use Gengo as their backend to professional translators. I highly recommend them. Getting a string translated for a couple dollars in an hour by a professional sounded absurd to us before we discovered Gengo.

Consolidation is solved. Localization is solved. But we still need each site visitor to get the right language for a given page. The correct language requires the correct locale.

We chose a simple javascript-driven dropdown at the top right of each page in the header. A visitor can quickly switch to their own locale when they arrive at the site. More importantly, the locale sticks. (Sounds obvious, but many sites we've seen fail at this, quickly reverting back to English while we navigate around.)

Let's walk through how a German would navigate the site:

1. Google leads them to http://retrospect.com.
2. They want to switch language and see the world icon at the top.
3. They hover over it, see the menu, and select "Deutsch".
4. They're redirected to http://retrospect.com/de.
5. They click on "Mac" and go to http://retrospect.com/de/products/mac.

We could have stored the locale in their cookie, but we preferred making it an explicit part of the URL. Shared links retain their locale, and testing is simpler. To create links for every locale, we use the following code:

Like I said above, the locale stays with the visitor. We originally included :locale => I18n.locale in all of our URL/path calls, but that didn't scale well as the site grew. Instead, we added the following code to include that key/value pair automatically in every URL:

We encountered a number of other issues during this process.

**Safe HTML**: For security reasons, Rails doesn't render HTML in text unless explicitly told to by text.html_safe. However, t() supports "&#95;html" appended to a given key in the YAML file for its value to allow HTML, so "&#60;strong&#62;this&#60;&#47;strong&#62;" becomes "**this**" for "homepage.this&#95;html".

**Locale fallback**: I18n has a configuration for falling back to the default language: config.i18n.fallback in application.rb. (Use config.i18n.default_locale to define the default language.) If a translation for a certain language is missing, t() looks up the same key in the default locale and uses it, so the site never looks wrong, just untranslated. (And like I mentioned in the first example, if t() fails to find the key in the default locale as well, it capitalizes the final key and returns that.)

**Language-specific CSS**: Phrase length can vary quite a bit between languages. Sometimes, every language works with a certain layout except one. To accommodate different lengths, we reduce the size of the longer languages in CSS.

The final issue we had with this approach was overall translation management. With 3k strings, we inevitably forgot to translate strings from the default locale. When we did, I18n wrapped the missing translations in a span with the class translation_missing.

However, this mechanism trusted our testing to identify missing keys. Moreover, members of our team fluent in other languages (like our European sales team) were not always satisfied with the translations on the site, but there was no place to see all the translations for their respective language.

To solve both problems, I wrote some Ruby code to collect every key, identify those with missing translations, and display both sets on a special page of the website. (Thanks to  for the core logic, although  is a good up-to-date alternative.)

The end result is a page at /missing_keys that lists every key in English with their translation and every untranslated key with what languages are missing:

Not all of our pages are fully localized with t(). When we converted our user guides from PageMaker and InDesign to HTML in 2013, consolidating the languages was too daunting, and we left them as separate language-specific HTML files (i.e. chapter1.en.html.erb). However, we added t() calls at the top to specify their titles.

We use both approaches together.

As I said at the beginning, Retrospect.com is around 400 pages today with 3,000 strings across 80 YAML files. Using Rails and its I18n gem allows us to extract our localization out of the HTML and into YAML files for simple site changes and easy translation, and a bit of custom code allows us to manage the workflow.

Migrating 10k strings was incredibly tedious though.
For Retrospect, we host around 80 tutorial videos on YouTube. I mirror the videos on the site as well under . When I added them to the site, I used Atom feeds (YouTube API v2) to gather the list of videos. On Tuesday, those ceased to work (a bit before their official  date), so I migrated the site to .

Migrating was not straight-forward. No more Atom or RSS feeds. All queries now need to be authenticated, understandable as YouTube will now be able to track usage and enforce quotas.  issues multiple types of API keys, including a server key and a browser key. The server key makes the most sense for my usage; the server fetches the video listings on launch and caches them.

However, the server key requires an IP restriction. We run retrospect.com on Heroku, with dynamic IP addresses (static IP addresses require a paid add-on), and I had no luck finding a CIDR address range for Heroku apps. The alternative is the browser key. It is supposed to be used by a web browser and requires a referral restriction. So, I created a browser key and inject a referer.

YouTube API v3 is different from the Atom feeds, so I'll walk through how to accomplish typical queries with curl and ruby. (Google also has an excellent  app to try as well.)



I haven't worn a watch in a decade. That might change with whatever revolutionary magical device from the future the iWatch turns out to be. Apple will unveil its wearable in four hours. But for the last five years, my only wearable device has been a Fitbit.

I pre-ordered one of the first ones back in June 2009, and I used it consistently for the first half of 2010. I walked 450 miles with it–875k steps burning 640k calories. Then I stopped. The novelty had worn off, and wearing it had become a chore. The first iteration of the device had three fatal flaws for my long-term use: remembering it, syncing it, and charging it.

**Remembering** – Seems obvious, but I needed to remember the device. When leaving the house, I had to remember my phone, my keys, my wallet, and then my Fitbit. A fourth completely unnecessary thing, just for a fancy pedometer. Same for running. I needed to remember to wear it and to sync it and to charge it.

**Syncing** – The first Fitbit had no Bluetooth. To sync it, I snapped it into its USB dock and plugged that into my Mac. Forgetting that meant no data.

**Charging** – That Fitbit's battery life was several days. I suppose 5-day battery life was a feature, but in practice, it was infrequent enough to forget to do it but often enough to find a dead device every other week. It charged while plugged into the USB dock, so at least syncing and charging were the same action.

I wanted to like Fitbit, but the data just wasn't that interesting for all the tasks I had to remember to get it. So I stopped remembering.

Luckily, I wasn't the only early adopter, and Fitbit sold enough of the original tracker to make other models. I purchased a Fitbit Zip in December 2012, and I've used it consistently since then. It solved all three of my issues: attaches to my keys, wirelessly syncs to my phone, and lasts three months on a $2 removable battery. In the last 18 months, I've walked 4.5k miles-9.2m steps burning 1.9m calories. Yay.

But that's it. Just "yay". Fitbit's data is interesting like the weather is interesting. Check the temperature by popping outside or launching the Weather app. Decide how far you've walked by guessing or launching the Fitbit app. I've never used Fitbit's other features, like tracking caloric intake or sleep patterns. The data doesn't affect my routines. Fitbit is still a novelty to me.

That's the question for Apple's iWatch. How does it go beyond a novelty? I use my Mac, my iPhone, and my iPad every day, not for the sake of using them, but to do things. The current assortment of smart watches still seem geared toward early adopters, who are interested in the device itself rather than doing things with the device.

Apple has repeatedly come to market years after competitors with products that fundamentally change those market. There were computers before the Apple, music players before the iPod, phones before the iPhone, and tablets before the iPad; yet all of those markets are now dominated (at the consumer high end) by Apple's products.

Those products solved problems. What will iWatch solve? I'm looking forward to the reveal in four hours. 


Apple unveiled its watch on Tuesday, dubbed simply Apple Watch. The announcement marked the culmination of years of rumors surrounding the company's interest in the category. Given the level of innovation in this first version, the company had indeed been actively working on a watch for years.

I love watching Apple work. There is no other company that combines such different disciplines and directs so many people to produce so few products for so much profit. Beneath the polish of the keynotes, sophistication of the hardware, and elegance of the software is undoubtedly a chaotic jumble of schedules, lists of known issues, and legions of frantic overworked employees. However, unlike other companies (like Amazon and its Fire Phone), Apple focuses on the product, not the process, and the products are a pleasure to use.

I started following Apple in college, back in 1999, and I finally bought my first Mac, a 12" PowerBook, in 2003, along with an iPod. I loved both. Those were such nice upgrades from my Gateway desktop and my Rio MP3 player. My Rio held ten songs. Ten. And my desktop was a big beige box. Apple's PowerBook and iPod felt like devices from the future. My 11" MacBook Air still feels like that. Each product feels incubated, like Apple missed several internal deadlines to get the experience right.

That is Apple's core competitive advantage: patience. It has billions of dollars, thousands of employees, and experience with hardware and software, but so do other companies. Unlike other companies, Apple says, "No, it's not ready yet," and tinkers more. In the keynote, Tim Cook noted that they experimented with pinch-to-zoom as a zooming gesture but hated it. Their solution was the Digital Crown, reminiscent of the iPod click wheel. The new device again feels like Apple took its time.

Apple Watch does not need to be a runaway success. The iPhone and iPad have enjoyed tremendous success, but Apple's iMac, Macbook, and TV dominate their categories as well. With Apple Watch, Apple is choosing another category where it can use its unique skill set to produce something compelling, something "personal". Focusing on the style and fitness markets positions the device to a set of people who are already interested and willing to pay a premium for a premium product, and Apple's massive ecosystem of companion iPhones and apps will help justify an expensive smartwatch to a broader set of people in time.

Think back to the original iPhone and iPad. In 2007, the original iPhone had no third-party apps, but it put the web in your pocket. In 2010, the original iPad had few apps for producing, but it was extremely nice for consuming. Those first generations were optimized for niche audiences, early adopters with disposable income who enjoyed premium products. Successive generations of iPhones and iPads have had far broader appeal, consistently adding hardware and software features to expand their reach.

Now look at the Apple Watch. This initial version might appeal to a relatively small set of consumers today, willing to pay $349 for a smartwatch, but imagine what the watch will do in 2019. The iPad has been out an equivalent number of years, and the iPad Air feels like a device from the future.

Apple Watch's evolution will be fun to watch, and this first iteration looks worth the wait. The Messages' "Heartbeat" option exemplifies that patience. Apple built a stylish watch that could take your pulse and tap it back to someone else, a phone to communicate through, and a software stack to reduce the entire experience to a tap–to share your heartbeat with someone you love wherever you are.

A great example of Apple in 2014.


 is the first app I install on any Mac. On every computer, at some point, I think, "Why is this slow?" It might be CPU. It might be RAM. It might be network. It might be disk. iStat Menus gives me the answer, at a glance.

iStat Menus began life as iStat Pro, a free Dashboard widget for Mac OS X Tiger (10.4). In 2007, the widget transitioned into the OS X menu bar with , and in 2010, it became a paid app with 3.0. Since then, it's been updated numerous times with the latest OS X support, moving to 4.0 in 2012 and  in August 2014. The team originally shipped widgets under the moniker iSlayer, but they started up Bjango as an iOS app company and migrated all their apps to it in 2010. Marc Edwards, one of the team, went into detail about the history (and the future) of the company in a .

See the screenshots below for the app's battery, memory, network, and time menu bar items.



I joined Retrospect seven years ago today. It was luck to have landed here and . And we've been busy for the past year.

We shipped two paid upgrades and five free updates. New features. Improved features. Better performance. Better stability and resilience. All for two platforms in six languages, while working from home three days a week. An excellent year.

But I'm really looking forward to what's next. We have some great stuff in the works.

I use  a lot. All day, every day. How much? Apparently, 10,000 times in the last two years. A dozen times per day.

For years, I treated Alfred as a straight UI replacement to OS X Spotlight. I switched to it from LaunchBar. I liked that Alfred launched in the middle instead of in the menu, and I preferred the way it displayed results.

Only this year did I started exploring Alfred's deeper features. I added a custom workflow and custom web searches. The workflow searches a specific directory for matching text files and opens the selection in Atom. The web searches are shortcuts for GitHub pull requests, GitHub commit hashes, Bugzilla bugs, and Salesforce cases, as well as a number of other internal company searches. Command-space and type "pull 123" or "bug 1234", and I'm there. Fast and easy.

I know I only use a small subset of Alfred's current feature list, but for now, I'm happy with my custom web searches.
For Product Management, two requirements documents define a product: the outward-looking Market Requirements Document (MRD) and the inward-looking Product Requirements Document (PRD). The MRD describes the problem, and the PRD describes the solution. These documents provide a complete view of the product: what it is and why it is. They ensure everyone involved is on the same page.

I've read about other companies doing away with them. I doubt it. Every product needs something defining what it is and why it needs to be built. Code doesn't count. Call them what you want. You need these two documents.

Every significant Retrospect release has an MRD and a PRD. In 2013, we redesigned our documents. I'll walk through why and how for form and function.

Like most companies, Retrospect originally used Microsoft Word documents for requirements documents. It's the standard format for document, so it's understandable. But it had downsides. The product manager would write up a draft and send it out to a group. Each person in the group would track changes in the Word document, comment on various points, and send it back. The product manager would then walk through each reply and edit the original to answer those questions. The editing proces was cumbersome and opaque, and it was difficult to compare drafts when successive versions were sent out. Irksome.

We wanted to solve four problems (like a meta-requirements document):

* **One copy**: There should only be one place to find the document, and people need to edit it in that location.
* **Versions**: The document should be transparently versioned with support for multiple authors.
* **Comparisions**: The versions should be easily comparable, so that anyone can see what changed and who changed it.
* **Image Support**: The document should support embedded images. Word supported this; the new system should as well.

All of these can be boiled down to transparent and useful. We wanted to remove the small bits of friction in our requirements document workflow. That friction made it harder to find the latest version of the document, update the document, see others' updates, and remain on the same page.

Our solution was a wiki. Wikis provide all four qualities: one copy, versions, comparisions, and image support. And with links, we could easily connect the MRD, the PRD, and the per-feature Engineering specs. We already had a Mediawiki instance, so we leveraged it. One thing that came for free was automatic update alerts through Mediawiki watchlists.

What we lost in the migration was the universal language of Word. It's esoteric, but people get used to it because everyone else uses Word. The Engineering team was comfortable with Mediawiki; the Sales and Marketing staff was not and still aren't, two years in. But no medium is perfect. Choose a form that reduces the friction to a minimum for your team. Think about the path of least resistance between product ideas and people's feedback.

I imagine one reason companies abandon requirements documents is their content. I've read quite a few that had a remarkable amount of cruft–superfluous content that didn't contribute in any meaningful way. Looking through a decade-old Retrospect PRD, I found a vision statement, a confidentiality agreement, a revision history, and a company logo.

Sections like "Vision Statement" are unnecessary business-speak, a professional veneer for content of any quality. You might as well play buzzword bingo with the remainder of the document. Points for "synergy" and "best of breed". Larger companies find these sections necessary. We're not that big.

When we redid ours, we removed cruft. Every section needed to be useful. Here is what we use these days.

**Retrospect MRD**

* *Purpose*: What problem are we trying to solve? Keep it simple and short.
* *Audience*: Who has this problem? Specify interested current customers and type of potential customer.
* *Appeal*: Why does the audience want this problem solved? Include customer interviews and feedback.
* *Competition*: Who else is already solving this problem and how? Link to details in a separate document.
* *Notes*: A record of decisions and on-going discussions.

**Retrospect PRD**

* *Features*: A list of features and use cases that solve the MRD's problem.
* *Release Details*: Various topics such as version number, documentation changes,  supported languages, and system requirements.
* *Notes*: A record of decisions and on-going discussions.

Useful documents. No cruft. This is what we do.


I didn't have a BookBook when I got my iPad Air. I was upgrading from the original iPad in its original iPad case. The iPad Air felt awesome, like a device from the future. It was so ridiculously thin. I loved holding it.

Of course, within a week, the novelty wore off, and I just had an iPad. That's when I wanted a case. I was afraid I'd drop it or drop something on it. Thin as it was, I didn't want to test its durability. I bought a .

The BookBook is one of those rare products that is more useful than its marketing suggests. Yes, it protects the iPad in case I drop it. Yes, it looks like a book, so people won't steal it. Yes, it looks good. I knew those when I bought it.

What I didn't know was the BookBook could be a vertical iPad stand for FaceTime. In the Twelve South marketing material (see above), the BookBook is on its side. While the viewing angle is nice, the camera points upward. But place the BookBook upright, and the camera points straight. A perfect angle for a family talking to another family.

That's how I use it, every week. My family is in the United Kingdom, and our extended family is in the United States. Sometimes, my family has dinner with the BookBook on the dining table. Other times, it's on bookshelf in the family room. Being able to stand the iPad straight up means the entire room is within view of the camera, and the entire room can see the iPad. I don't need to hold the iPad or prop it precariously. It just stands there, and my family can easily chat with our extended family, courtesy of FaceTime and BookBook.

The BookBook is a case and a stand, built for a video chat with the entire room, not just a person at a desk. It's pretty too.
I joined Retrospect eight years ago today.

I started as a software developer, working on Retrospect for Mac. I transitioned to Vice President of Strategy , and this year, I became Chief Operating Officer, overseeing Sales, Marketing, Product Management, Customer Service, Finance, and Operations. It's really neat to work on a product that's been around for three decades and with a team who has seen one or two of those decades.

This month at Retrospect, we shipped , one of the largest releases in our history: cloud storage support (to Amazon S3, Google Cloud Storage, and others) as well as performance improvements across Mac and Windows in six languages.

A decade ago, at EMC, our team was twice as large, but we didn't do simultaneous Mac and Windows releases or multi-lingual releases. Now, we use off-the-shelf tools like  and Amazon S3 along with homegrown tools like our automated cross-platform test suite to ship more faster. We're able to add a steady stream of new features, performance enhancements, and bug fixes to both Retrospect for Windows and Retrospect for Mac.

I use  at home. The server runs on an old 2008 MacBook Pro laptop, and every day, it backs up my entire household: 6 computers with 5 TB of data. And with the new release, every document from every computer is transferred off-site to Amazon S3 nightly, secured with AES-256 encryption. All for the price of three dozen Starbucks lattes. (As someone who drinks two lattes a day, I go through three dozen quickly.)

You cannot find a comparably-priced product with such a rich feature set, and I'm happy to be part of the team that ships it.

Yes, yes, multi-touch is amazing. Unless you're my grandmother, and you're just trying to use the mouse that came with the computer to get your email. She was constantly getting frustrated with how Magic Mouse responded to her touch rather than her click, so I disabled the multi-touch features. Easier said than done. There is no checkbox for that in System Preferences. Here are the Terminal commands I used:



I discovered Tea Forte's Flora Tea at Coupa Cafe, a popular coffee shop in Palo Alto. That was in 2003, thirteen years ago. It was the first tea I found that was caffeine-free, that tasted just as good over ice as brewed hot, and that I never got tired of. I never bothered finding a second.

But in 2015, Tea Forte discontinued the flavor. I tried half a dozen other herbal teas, but none of them matched Flora. So of course, I had to deconstruct the tea and make it myself. I've been drinking my own version for the last year.

Flora was described as "a botanical bouquet of lush tropical hibiscus petals with a swirl of cinnamon and the sweet, invigorating flavor of licorice."

Luckily, not many ingredients: hibiscus, cinnamon, and licorice. I still had a couple canisters of loose leaf Flora to experiment with, so I ordered a variety of these from Amazon. While I've settled on an easy set of ingredients, they weren't obvious at first.

* **Tea Infuser**: I use the OXO Good Grips Twisting Tea Ball. It's a good size for these ingredients.
* **Hibiscus**: I order a 1kg bag of whole hibiscus flowers from Amazon. With the OXO infuser, I don't need to chop these; I just stuff them into the ball. While a large bag is a good price, it costs time. I realized early on that around 10% of the flowers had cobwebs, and so I started picking through the entire bag to sort those out. More time-consuming but it gives me piece of mind.
* **Cinnamon**: I originally thought I needed cinnamon chips (little pieces of cinnamon). Not true. I now get cinnamon sticks from the local grocery store and cut them up into .5cm pieces with a sharp knife.
* **Licorice Root**: Ironically, while I tried to get cinnamon chips, I first purchased uncut licorice root, thinking I could cut it up myself. Don't. Licorice root is very tough to cut. I go to Amazon and find a 100-200g bag of "Licorice Root Cut" or "Licorice Root Pieces" from a trust-worthy vendor.

What I put in one OXO tea infuser:

* 5 bits of cut licorice root
* 5 bits of cut cinnamon stick bits
* As many hibiscus petals as I can stuff in

(I keep the ingredients separate until I make each tea.)

When I tried to recreate the taste of Flora, I picked out the individual elements and weighed them. These are the approximate portions. At first, I compared the tastes of the two versions. After a couple days, I accepted that the my own concoction would never taste like Tea Forte's, and I stopped worrying. I still have the canisters of Flora, but they just gather dust. Every day, I have my own version.

I do miss Tea Forte's pyramid infusers though.

Salesforce doesn't support territories for Leads in its Territory Management or Enterprise Territory Management features. (Our company hasn't upgraded to Salesforce Lightning, so I'm not sure whether the new version includes this feature.)

However, you can set up sales territories as a read-only custom formula. As a formula, the field is always up-to-date, a stark contrast compared to the territories in Territory Management, which require you click "Run Rules" every so often. (We use Territory Management for Opportunities and Accounts, and every so often, we realize the territories aren't correct. We have to click "Run Rules" to fix them.)

Let's walk through how to set up a custom formula:

1. Go to "Setup" > "App Setup" > "Customize" > "Leads" > "Fields".
2. Click on "New" under "Lead Custom Fields & Relationships".
3. For the field settings, use "Formula" for "Data Type", an appropriate label for "Field Label", and "Text" for "Formula Return Type".
4. For the formula, I translate "Country" into regions. Because custom formulas are limited to 5,000 characers, I couldn't use a CASE statement. Instead, I used , with a colon separator because CONTAINS matches substrings. You can use any separator that doesn't match the argument. See my version in the GitHub Gist below.
5. To verify your version, use "Check Syntax".


By default, Salesforce's "Contact Lookup" only uses the name to search. To enable search for a Contact's other fields, like email address, you need to enable "Enhanced Lookups".

Here's how to find it: "Settings" > "Customize" > "Search" > "Search Settings". Then mark "Enhanced Lookups" for "Contacts".

Enabling "Enhanced Lookups" adds an "All Fields" option to the "Contact Lookup" dialog.

Our Support team asked me about searching for Contacts by email earlier this year, and figuring out how to enable it was not straight-forward.
For my company's email campaigns, we use a customer's US time zone to determine when a particular email gets sent out. I added it as a custom formula to Accounts.

Let's walk through how to set up a custom formula:

1. Go to "Setup" > "App Setup" > "Customize" > "Accounts" > "Fields".
2. Click on "New" under "Account Custom Fields & Relationships".
3. For the field settings, use "Formula" for "Data Type", an appropriate label for "Field Label", and "Text" for "Formula Return Type".
4. For the formula, I translate "Billing State" into the time zone. See my version in the GitHub Gist below.
5. To verify your version, use "Check Syntax".

Three years ago, I became VP of Strategy at Retrospect, and I found we didn't know enough about our product sales. We used Salesforce, but it didn't provide the level of granular reporting we needed to analyze how Retrospect was selling according to a number of metrics. So, I wrote my own: Datacube. (Retrospect's icon has always been a cube.)

## Building Datacube

Datacube needed to analyze sales in a number of different ways:

* **Revenue**: Measuring by revenue is always useful.
* **Units**: Retrospect varies in price per release and per region based on currency exchange rates, so measuring by units is a useful gauge for comparison.
* **Product Edition**: Our product SKUs change every product release, and we needed release-agnostic insight into how products were selling.
* **Product Level**: Retrospect has a desktop version and a number of server versions, so we wanted to separate those levels.
* **Product Type**: We sell a set of products along with add-ons to those products. Differentiating between those sales is significant.
* **Purchase Type**: Customers can buy Retrospect new, upgrade Retrospect, or renew their subscription to Retrospect. We wanted to see the individual trends for all of these.
* **Time Period**: Side-by-side comparisons of arbitrary time periods are essential for understanding product launch sales, as our launches are not at the same time each year.
* **Market**: We sell Retrospect all over the world, so we needed to see how any edition performed in any market.
* **Drilldown**: We needed to see the individual sales that made up any given number, both to verify its accuracy and understand anomalies.

Datacube handled all of these. It ingested our sales information from Salesforce, analyzed the sales across the above metrics, and let me compare across time periods. For any interesting data point, I could drill down on it to the individual transactions. In the end, Datacube was three thousand lines of Ruby-on-Rails code, and it helped us understand trends in our business and focus our efforts.

However, building a tool has its trade-offs, and Datacube had many limitations. The most significant was buy-in: Sales was never comfortable with it. While Product Management used Datacube, Sales continued to use various Excel templates, leading to two ground truths. Never good. Finally, our VP of Sales pushed me to look at Salesforce again to see if we could somehow use it to replace Datacube and Excel, so that everyone was on the same page.

## Replacing Datacube

When I started building Datacube, I had very little experience with Salesforce. I didn't realize how easy it was to extend objects with custom fields to add missing metrics or how to leverage Reports to analyze those metrics. Salesforce could do everything I built into Datacube with minimal effort.

Extending Salesforce was straight-forward. I worked with Sales and added those original missing attributes ("Product Level", "Product Edition", "Product Type", "Purchase Type") to the Salesforce Product object, and I helped Sales set up Territories to identify markets around the world. Drilldown is already available in Reports via "Show Details".

Comparing time periods was a bit more complicated. One quick solution is to open multiple browser tabs, load various time periods into each, and flip back and forth. To compare years in the same report, I created a custom formula for the year of an opportunity to drop in the column grouping. To compare months in the same report, I used "Close Date" > "Group Dates By" > "Calendar Month in Year", with the year field as a row grouping. Comparing custom time periods took more effort, but it's similar to the previous two methods.

Datacube helped us understand our business far more than we had before. Incorporating those metrics into Salesforce meant everyone was working off the same data and analysis.
Salesforce Reports provide support for a wide range of queries. After selecting the main object to report on, I can add complex filtering, search cross-linked objects, and include attributes from related objects, while seeing the results in the standard Salesforce Reports view.

Compare this to searching in Ruby using Restforce. I can search the attributes of one object in code. No related object attributes or complex filtering or cross-linked objects without more code, and on top of that, no visibility through the website. Seeing the results in Salesforce is crucial, both for at-a-glance data verification and for sharing the report amongst a sales team (who don't write code).

Fortunately, with Salesforce Analytics API, we can bring the power and visibility of Salesforce Reports into Ruby. (Note though that the results are limited to 2,000 rows.) I wrote the following class to fetch a Salesforce Report and package it up into a Report object with name, filters, and rows. Feel free to use it.


In , I discussed using custom date ranges over multiple years to compare different product releases in one Salesforce Report.

At Retrospect, we wanted a report for tracking the current release with a comparison to previous releases, while hiding other sales data from outside those relative ranges. As our release schedule varies, we couldn't just track a particular month's sales compared to the previous years; we needed to compare the March 17, 2015 release, the March 1, 2016 release, and the March 7, 2017 release and only view the first N days of each release. Moreover, we wanted to leverage the rest of the custom attributes we created for deep sales analysis, like product edition, configuration, or fulfillment type.

Let's walk through exactly how to set up the report. We need to add two Opportunity fields and then use them in a report.

### Release Name

The first field is a text formula for the name of the release. Our most recent one was "2017 - Win 12, Mac 14". This field will be used in header of the report. The formula is structured so that any opportunity that falls within a given date range is assigned the correct release name to group it; any opportunities outside all of the dates is assigned a blank. See below:

### Days Since Release

The second attribute is a number formula for the number of days since a release. This field will be used in the filters for the report. The logic is the same as above, but instead of returning text, we return "CloseDate - DATE(YYYY,MM,DD)". Like above, if the opportunity falls within the date range, the relative date from the release is returned, and otherwise, we return a very large number.

### Salesforce Report

Now we put those attributes together. Create an Opportunity Report with a "Matrix" format with no details. Drop "Release Name" into the header, and add "Days Since Release" with a number for the day as a filter. You can then use any standard or custom attribute on the left side to slice up the resulting data. Below is a screenshot of the filters and headers for our report:


Last fall, we redesigned . The original look and feel had evolved over several years, ever since we were spun off from Rovi in November 2011 and became Retrospect, Inc. But it had the same basic look, and it needed a makeover in two respects:

* Responsive Design: 17% of our traffic was from mobile devices.
* Modern Look: The site felt outdated.

Let's walk through each.

There are a number of elements to responsive design. A great resource on the various principles and implementation is :

* Ethan Marcotte's Responsive Design: Patterns and Principles
* Scott Jehl's Responsible Responsive Design
* Ethan Marcotte's Responsive Web Design
* Karen McGrane's Going Responsive

We started with the basics:

* Migrating "px" to "em"
* Migrating "em" to width percentages where possible
* Using media queries for different views of the same HTML depending on the screen size

@media screen and (max-width: 980px) {
  // CSS for wide screen
}
@media screen and (max-width: 400px) {
  // CSS for mobile screen
}

Next, we created a set of responsive CSS building blocks to roll out across the site:

* Lists that are horizontal when there is space and vertical when there isn't
* Boxes that dynamically shift depending on the width of the page
* Titles and text that changes font size based on the screen

Using a set of building blocks helped us maintain a common look and feel across the site while minimizing one-off CSS code. We built our own, but there are great off-the-shelf frameworks available like Thoughtbot's .

Throughout the redesign process, we needed to update the content of the site, ranging from small bits of text to incorporating a sliding set of logos to a complete rewrite of the homepage. We always circled back to what visitors expected from a particular page.

For example, our previous homepage highlighted Retrospect's recent features, but in the redesign, we focused on what differentiates Retrospect from other data protection products. In talking to people inside and outside of the company about the homepage and looking at Google Analytics, we realized people who land on the homepage don't know what Retrospect does, so conveying our core feature set was more important than our latest feature set.

The final element was text-rich graphics. They weren't responsive, and moreover, they were frustrating to maintain for a site with six languages. If there was text in a graphic, there were six versions of that graphic. All of our site's localization runs through I18n with a massive 27,000 lines of localized text. I cover this in-depth in . By migrating the text into HTML and overlaying it on text-free graphics, the text and image can be responsive as well as easier to manage for localizations.

The end result of these mini-projects was a responsive, fluid website that worked well on a desktop, tablet, and mobile device.

Our previous site lacked visual impact. While it did convey information, the lack of imagery and a clean color palette gave it a bland look. Retrospect is a small company, focused on protecting the data of small businesses around the world. We wanted our website to reflect that.

First was the color scheme. We debated changing the primary color scheme to a blue palette and even a red one, but after putting together several mockups, we opted to remain with a black and white palette. Not gray though. On the previous site, every page had a gray background with a rounded white box containing the main content. This layout was common years ago, like on Apple's site, but it looked dated now. The layout also didn't help our responsive redesign, as the gray background and box took up valuable real estate on a mobile device. By removing the gray background, the content was able to fill the available space on a given screen size, and the site felt cleaner.

(Yes, the irony is not lost on me that this blog retains the gray background and rounded white box for content. A project for another day.)

Next was a new font. Our previous site defaulted to Helvetica Neue. Helvetica is popular enough to have an entire film created about it, but we wanted a thinner font. After looking through a number of options in Adobe's TypeKit, we settled on Acumin Pro. It's very thin and clean at large sizes but still readable for normal text.

Finally, we looked for new stock art. Stock photos help communicate the essence of a site without needing to hire a professional photographer, models, and locations. Our two favorite stock photo sites are  and . Finding appropriate stock art feels like searching for a needle in a haystack, as Shutterstock now has 130 million images, but finding the right fit is worth far more than the price of the image. For us, we came across . The office photos focused on a small team getting things done around an office. They felt like a great match for how we wanted our website to look, so we use them in a number of places around the redesigned site, including the homepage.

Together, the refocused color palette, new font, and new set of stock photos bring an immediate visual context to each page and draw in the visitor.

Dantz Development, the original company that created Retrospect, started in 1984. Most of the current team are alumni from it. When we started this project, we used the Wayback Machine to see what the original Dantz website had looked like through the years.


Below are examples of our current site:




I joined Retrospect nine years ago today. Nine years is eons at a San Francisco software company. Just look at  of Silicon Valley companies. And yet, I'm still the new guy. Most of my coworkers have been here for double that or more. Our Director of Technical Support has been with the company (in its various incarnations) since 1994. I wasn't even in high school in 1994. In fact, Dantz Development (our original corporation) shipped Retrospect 1.0 in 1989. I was nine.

That's a long time to be protecting people's data.Recently, I found a certain `.strings` file was producing an error for Retrospect's iOS app in Xcode, specifically:

This error is common enough to have a number of excellent StackOverflow answers. In their cases, there was a text error in the file, like a missing quote or semi-colon, and most referenced `plutil -lint path_to_strings_file` as the best way to identify the incorrect line.

The confounding factor in my environment was that the Retrospect for Mac project, which referenced the exact same file, built successfully. I tried (unsuccessfully) to resolve the problem by switching encodings and even downloaded HexFiend to poke around in the file. All useless.

As it turned out, the root issue was the same as everyone else's: I had a quote missing. However, Xcode (7.3.1) behaves differently for iOS projects and Mac projects: the iOS one failed explicitly on the error and the Mac one failed silently. The Mac app simply wouldn't see any string translations beyond that offending line.
In 2011, I wrote about . Six years later, it's still the only app I use but now on my iPhone. Recently, I was on a tour bus careening around the streets of Porto, Portugal, while kicking off scripts on an EC2 micro VM (which are amazingly priced at ~$5/mo). Prompt just works.

In the last post, my recommended steps involved syncing your iOS device with iTunes and dragging the keypair into "File Sharing" to get it onto your iOS device. Clearly, that doesn't cut it anymore. Apple has migrated away from iTunes completely. Luckily, you can easily use Apple's Notes application:

1. On your Mac, open Notes.
2. On your Mac, open the keypair file in a text editor.
3. Copy the contents into Notes.
4. On your device, open Notes.
5. After the file syncs, copy the keypair out of Notes.
6. Launch Prompt, click on the gears icon for Settings, then tap "Keys" and "+" and "Paste from Clipboard".
7. Connect to EC2.
In 2011, Retrospect was spun off into Retrospect, Inc. For seven years, we had operated as a small group inside of a larger company, but at that point, we were on our own. We had just shipped Retrospect 9 for Mac, and we had a lot of ideas for what we wanted to do. One of them was changing our release cycle.

Our previous release cycles had been quite variable. We maintained both Retrospect for Windows and Retrospect for Mac, but we never shipped them simultaneously. Both versions supported many languages, but we never shipped those simultaneously. We added features to both products, but we rarely added the same features in both places. We always planned for regular releases of both products, but we let the schedule dictate the final dates.

All of those release decisions had their reasons, but now we had an opportunity to rethink them. We wanted to change all four: major releases of both products in all languages with feature parity, every year.

Our first release was in November 2012: Retrospect 8 for Windows and Retrospect 10 for Mac. We released both products simultaneously in all languages with our big feature, Instant Scan, baked into both. We slipped the next release from November 2013 to March 2014 while we were putting the finishing touches on block-level incremental backup, but we've shipped every March since then, with performance increases, cloud backup, scalable data protection, and hundreds of smaller improvements and fixes.

The constraints of a standardized release cycle have helped every department. Sales gets a consistent release of both products every year, instead of wondering when a particular feature will be in which product. Marketing can focus its energy on a single launch, rather than juggling multiple releases with different features in different languages. Support helps customers with the same feature set in both products. Finally, Engineering can triage new features and bug fixes within a known timeframe. We used to have many Engineering discussions over whether to include yet another feature in a release because the release dates were always malleable. Now we discuss in which release to include the feature.

These are problems that every software company deals with, and when we started making these changes, these results weren't guaranteed. But having made the switch, we're incredibly happy with the new process. By standardizing our release cycle, we accepted a set of constraints that streamlined many internal processes and made the company far more efficient at delivering features consistently across multiple platforms and multiple languages.
Salesforce includes two different types of forecasting:  and . We use neither.

We tried to use "Customizable Forecasting", but it only allowed six months into the future, which didn't fit with our yearly per-month sales targets. Moreover, we couldn't split those quotas into different components, like sales targets per platform or purchase type ("New" vs "Upgrades"). Neither Salesforce module supported such customization. Instead, we designed and implemented our own–Granular Forecasting–with three requirements:

* Leverage Salesforce Reports and Dashboards for real-time progress at a territory level
* Split sales targets by platform and purchase type and territory
* Allow non-technical person to collate targets in a spreadsheet with a repeatable import process into Salesforce

The resulting workflow lets our Vice President of Sales create the yearly sales targets in Google Sheets. I download them as a CSV and then run a Rails rake task to import them into Salesforce as opportunities with the stage set to a new name called "Target". Each opportunity included a set of "products" which represented the platform and purchase type for that target. For instance, the "January 2017 - France" opportunity contained 6 products for the combinations of platform (Mac/Win) and purchase type (New/Upgrade/ASM).

By including the targets in the normal workflow of opportunities, we're able to use Salesforce Reports for granular forecasting. This custom approach to forecasting won't work for everyone, but by ticking off the design requirements, we have a forecasting workflow that works well for us.
I joined Retrospect ten years ago today. A full decade and more than a quarter of my life. And yet, working on Retrospect doesn't get old. There are always more sources of data to protect–Retrospect Email Protection shipped this month–and more stories about how Retrospect saved someone's business. Just check out the Retrospect homepage for twenty stories and counting.

This month, we made backup even more affordable with Retrospect Solo, which protects a single computer, two email accounts, and any number of external hard drives with local and cloud storage for $49. And the same product and feature set scales up to Multi Server, which protects hundreds of servers, desktops, and laptops for $4k.

And it's a free 45-day trial.
Ten years ago today, I started this blog. It was a couple months after joining Retrospect, and I created a WordPress blog to collect various technical notes and thoughts.

In the last decade, I've written 160 posts–16 per year or more than once a month–and the site has received 431,529 pageviews, with the largest referrer being Stackoverflow at 6%. See the below screenshot of Google Analytics from 2008 to 2018. The spike is when Lifehacker reviewed . That free app was a huge hit, racking up around 80,000 downloads.

Here are the top five most popular posts with their pageview percentages:

1. 16% – 
2. 7% – 
3. 6% – 
4. 5% – 
5. 4% – 

Of course, the most popular posts are useful technical how-to guides, not thoughtful posts. Here are my top five favorite posts and their pageview percentages:

1. 0.04% – 
2. 0.42% – 
3. 0.05% – 
4. 0.11% – 
5. 0.01% – 

Given the recent rate of posts, I doubt I'll match 160 in the next decade.Having issues with cached CSS or javascript on macOS Safari during web development? No need to use "Clear History". In the menu bar, go to "Develop" > "Empty Caches".


I joined Retrospect eleven years ago today. It sounds like a long time, but it doesn't feel like it. I've had different roles at the company, and shipping software doesn't get old. Always new features to add and existing ones to improve. More types of data to protect in different ways. Retrospect 1.0 shipped in 1989, so Retrospect has been continuously evolving for 30 years now.

This month, we released our first web application: Retrospect Management Console. It lets you see the state of your backup environment from anywhere in the world, a big step up from being tied to an installed application on the local network. And it's free for basic monitoring with Retrospect Backup 16.

If you don't have a backup, you should get one.
For the last seven years, our team at Retrospect, Inc. has pushed Retrospect Backup forward with a host of features, like cloud backup, distributed management, and remote backup, and we're thrilled to announce the next step in our growth. Retrospect is joining forces with Drobo and Nexsan as a subsidiary of StorCentric!

StorCentric provides world-class and award-winning storage solutions for prosumers, SMBs and enterprise customers. Between its Drobo and Nexsan divisions, the company has shipped over 450,000 storage solutions and has won over 100 awards for innovation and solution excellence.

With StorCentric's resources, we'll be able to push Retrospect Backup forward even further with new features and support for more platforms, and our customers and partners will continue to receive the same top-notch service from our excellent Sales and Support teams.

We're really excited to be part of the StorCentric family, so we can continue improving Retrospect Backup.Recently, I had a bug where I reassigned `params` in a Rails controller but only for development mode. However, when I pushed to production, I found `params` would become nil after passing this unexecuted code.

As it turns out, this is a feature of Ruby: "The local variable is created when the parser encounters the assignment, not when the assignment occurs". ()


I joined Retrospect twelve years ago today.

A month ago, I would have written about the StorCentric acquisition and our growth since then, but as the Coronavirus pandemic has spread across the world, the Retrospect team has been focused on our families and ensuring our customers can protect their business data.

If you don't have a backup, you should get one. Your data needs love too. Retrospect is offering free 90-day subscription licenses to any Retrospect Backup product to ensure every business can get protected.

Stay safe, everyone.During the first iteration of our cloud monitoring service, we deployed software that sent malformed data to it, so we needed to correct the data on the server, as the data came in. The JSON itself was misconstructed for certain objects, so we needed to fix the issue before Rails parsed it.

In `application.rb`, I added the following line to ensure our middleware was called before anything else, including Rack and Rails middleware:

ActiveRecord in Rails makes it easy to collect items using SQL commands, but if you want to also add attributes that are not database columns, this is what to do:


During the summer, I organized a low-key internship program for Retrospect Engineering. We had four participants, varying from a freshman computer science major to a senior business student specializing in data science to someone looking for a career change.

Given the wide range of backgrounds but little experience with software engineering, I outlined two goals: building up professional software engineering experience and shipping a product.

The interns stepped through five tasks during their first two-week sprint:

* **Participate in Scrums and Sprints**: Show up on Zoom every day and give a brief sprint status for yesterday and today. Retrospect Engineering have used daily scrums for years and started sprints recently. Scrums are short and keep the team on track.
* **Learn Git**: Create a repo. Do a commit. Use git status and git diff to see changes. Git is the default source control system of the day, and understanding the power of source control and Git in particular is a fundamental part of professional software engineering.
* **Learn GitHub**: Clone a repo. Do a branch and a commit. Push and create a pull request. Comment and merge. Pull requests revolutionized Retrospect Engineering's code commit workflow by making code review easy and necessary.
* **Use Visual Studio Code**: Install Visual Studio Code. Install a couple popular extensions. Use to edit a repo. Microsoft Visual Studio Code is an excellent free cross-platform source code IDE, and we use it internally for our Rails and Nodejs web development projects.
* **Search Stackoverflow**: Click around the popular topics and search for anything. If you have a question about a language or encounter an error, Stackoverflow probably has the answer.

By completing these tasks, the interns could go back to school with a better toolset for software engineering.

A couple years ago, I built a macOS tool for Retrospect Support to collect system information and logs into a folder that a user could zip and upload to the Support team. We've always wanted a Windows version but never found the time. I picked a cross-platform Support Tool, built in Node.js and Electron, as a simple product that the interns could ship.

With a basic set of tools for building software, I forked `electron-quick-start` as the foundation for the Support Tool, and the interns began learning about Javascript, Node.js, and Electron. The Retrospect Engineering team provided a lot of guidance and troubleshooting, as all of these tools were new to the interns.

The project took them about two months, but a couple weeks ago, the interns completed a first version of the cross-platform Retrospect Support Tool. We now link to it from the  at the bottom.

Retrospect Support is already using this diagnostic tool to deliver a better support experience to our customers.
I joined Retrospect thirteen years ago today.

Retrospect has been protecting data at homes and businesses for 32 years. We've been shipping since 1989, before the web became popular, the dotcom bubble started, and most of today's huge tech firms were founded. Retrospect continues to back up and restore petabytes of data because it works. We estimate that Retrospect protects around 100 PB of data.

Staying relevant for three decades as a small company in a crowded market with much larger competitors means staying close to our customer base, understanding the workflows that make sense for them, and translating those into features that a small Engineering team can build. We can't solve every data protection problem out there. We have to choose the relevant ones for our segment of the market and then ensure everyone internally and externally understands the value of those solutions.

If you don't have a backup, do your future self a favor and download Retrospect.
The sophistication of ransomware is staggering, particularly the variants that gain administrator access and then use it to delete backups on-premise and in the cloud. To combat this vector, major cloud storage providers have rolled out a locking mechanism called Object Lock or Retention Policies that enable customers to lock objects for a specific period of time, and not even the administrator can modify or delete the objects during that period.

Retrospect Backup 18 launched last week with ransomware protection. It creates immutable backups by integrating with Object Lock on Amazon S3, Wasabi, Backblaze B2, and MinIO. Customers can specify how long they want their backups to remain locked, and Retrospect will set the associated retention policies for those backups in the cloud.

Moreover, Retrospect uses ProactiveAI's policy-based scheduling to predict when a backed up file will be out of policy, include it in the next incremental backup, and delete out-of-policy backups that are no longer required, ensuring point-in-time restores for every immutable backup while reducing storage space. Customers can also leverage Retrospect's filtering ability and backup transfer policies to perform complete system backups on-premise and then transfer only the critical documents from that backup into an immutable backup in the cloud. Immutable backups offer a secure workflow for protecting any data that is too important to be lost to ransomware.

Microsoft Azure and Google Cloud Storage provide only per-bucket/container retention policies, so we have white papers solutions for them as well as step-by-step guides in the link below.

See retrospect.com/ransomware.

Retrospect Backup 18 and Retrospect Virtual 2021 include a long list of other features, including security reporting, geo tracking, cloud data protection, and broad platform support updates like VMware 7.0 and Windows Server 2022 Preview.

Cloud data protection is a particularly nice addition. Retrospect Backup 18 can protect cloud storage, backing it up on-premise or to a different cloud storage provider and integrating with the rest of the toolset, like filtering and scheduling. We use it internally for protecting our cloud storage, including all of the website assets but excluding our daily software build folder, with incremental on-premise backups.

Along with our product launch, we redesigned our website at retrospect.com. The last major redesign was in 2016, detailed in . We iterated on that design, such as adding testimonials to the homepage above-the-fold. But the site was due another redesign, and we're really happy with the results.

The Retrospect Engineering team did an outstanding job this release. We juggled multiple products during the release cycle, with moving timelines and shared features, but as usual, the team hit the deadline with a solid release that will help customers protect more data in better ways.
Order processing is one of those unglamorous corporate tasks that is tedious, repetitive, and absolutely essential. Processing an order as fast as possible enables Operations to quickly fulfill the order for the customer as well as start the clock on the invoice terms, typically NET30. At Retrospect, our online store handles fulfillment automatically, but for channel sales, orders are processed by Operations.

For channel sales, our channel distributors send us orders in a variety of formats, including CSV, Excel, and PDF. Operations must take those orders and add them to Salesforce with a variety of items to check. For instance, if there is an existing pipeline opportunity, Operations needs to use that and fulfill it. Operations also needs to check for existing accounts or contacts in case they are current customers. Customers can have support contracts, so those need to be updated when new orders are processed. Operations must check all of these items and more when processing an order to make sure Salesforce is a clean and accurate representation of our customers and opportunities.

Introducing errors through inaccurate data (like typos) or duplicate data (like multiple Salesforce accounts) costs time later for Operations to fix the fulfillment and for Sales to clean their Salesforce accounts.

Five years ago, we had no automation. Operations had a full-time person dedicated to order processing. It took them 30 minutes per order. Some days, they didn't process all of the orders that had come in. Because there were always more orders to process, they didn't always check for an existing account, contact, or opportunity, introducing duplication and pipeline confusion. Sales had to clean up their accounts periodically.

In 2016, the VP of Sales talked me through these issues during a 1-on-1 and asked if there was anything we could do about it, like some amount of automation. He had already asked Operations to stop duplicating Salesforce data, but duplicate data still crept in. Given Sales' level of frustration, I took a couple days and wrote a v1 of a tool, and it looks about the same today.

It was a single page, hosted on our website behind a sign-in wall, and it imported orders, showed the accounts and contacts with Salesforce links if they already existed, and let Operations type in the correct names if they could find it in Salesforce.

After the information was accurate, click "Create Prospect Opportunity", and the page would show a Salesforce link to the imported order as an opportunity.

Over the last five years, I've continued iterating on the order import page based on Sales feedback to automate a growing list of tasks:

* Look up and use an existing pipeline opportunity
* Auto-correct the top 250 resellers and distributors to their Salesforce account names
* Process CSVs, Excels, and PDFs in various formats
* Link to Salesforce searches to find missing accounts or contacts
* Color code the results: green is good with everything linked, red is bad with extra attention needed to see what's missing
* Change support contracts based on the new order
* Upload multiple files to attach to the order
* Add per-distributor instructions for which email addresses to email for what order fulfillments

In total, I've spent around a month building out the tool.

The original goal of the tool was to reduce Salesforce duplication and Sales frustration, and the tool certainly did that. Sales no longer needs to periodically clean their accounts like they used to.

However, the unintended consequence was the drastic productivity boost in order processing. Manually processing an order used to take 30 minutes. Now, it takes 5-6 minutes–a 5x productivity boost.

The tool automated the mundane but necessary workflow for order entry and processing, let Operations focus on other tasks, and let Sales spend more time selling, all while reducing the time to fulfill orders for customers.

As a time estimate, the order processing tool cost $15k for Engineering (1 person-month), but in the first year, it saved $100k: $50k for Operations (10 person-months) and $50k for Sales (3 person-weeks for each sales rep). An $85k ROI in one year, and we've been using it for five years.

I think the productivity boost justifies the internal tool in this case, and the initial version only took a couple days, with iterations after that based on feedback. Still, there was still an opportunity cost to building it. At a different company, $100k per year might not have been enough to justify 1 person-month of Engineering. It did for Retrospect at the time.
Optimized algorithms enable Engineering teams to deliver a significantly better, dynamic experience for customers, given the right choice of algorithms for the problem space. However, advanced AI algorithms or machine learning techniques aren't necessary to vastly improve many solutions for customers.

For Retrospect Backup, one core problem is resource scheduling, specifically when to back up a data source, accommodating environments from a couple computers to a large business with hundreds of different data sources with different availability patterns, bandwidths, and data sizes. The goal is to protect every resource within the backup policy that it's assigned (i.e. every day).

Let's say you have a main desktop computer and a laptop. The Retrospect engine is running on the desktop, and the Retrospect client agent is running on the laptop. When does the Retrospect engine back up your laptop? The ideal solution is to start the backup when your laptop becomes available. That is exactly what Retrospect does.

The Retrospect engine waits for agents to contact it to schedule them for backup. Retrospect client agents send out a network message when the computer connects to a network, and the Retrospect engine is listening for that. When it detects a new client, the engine checks if the computer is out of policy, and if so, starts a new backup.

This type of resource scheduling is unique in the backup industry. Whereas other backup solutions schedule backups based on a particular time, like every day at 10am, Retrospect can schedule backups for any time that a data source is available within a specific time window, such as anytime between 9am and 5pm.

Moreover, Retrospect can back up more than just computers. It can protect physical servers, virtual machines, desktops and laptops (endpoints), external hard drives, network attached storage (NAS) volumes, email accounts, and cloud storage. The wide variety of resources translates into a dynamic set of scheduling problems.

We created this resource scheduling feature, named "Backup Server", over fifteen years ago to handle the sporadic nature of endpoints coming and going from the network, and we've steadily expanded it to accommodate more data sources, renaming it to Proactive Backup and most recently to ProactiveAI, as it's significantly smarter than its predecessors.

This type of resource scheduling has a number of attributes to utilize for optimization:

* Data source type
* Data source availability
* Data source importance
* Last backup date
* Last backup size
* Last backup duration
* Last backup performance

Compared to other backup solutions, the previous iteration of Retrospect's resource scheduling algorithm--a simple first-in-first-out (FIFO) strategy--worked well. However, the sheer diversity of our customer base exposed the algorithm's weaknesses:

* **Start Time**: The next backup date was scheduled based on the previous backup's completion date. This nuance translated into time skew, where the first backup started at 10am and finished at 11am, the second started at 11am, and so on until there is a day when the backup is scheduled to start after the backup window and thus skips a day.

* **Priority, Availability, and Size**: Servers are generally always available for backup, but endpoints come and go. Desktops might be shut off at the end of the day or have energy-saving mode enabled. Laptops typically leave the network each day. NAS volumes are always accessible as are email accounts and cloud storage locations. All vary drastically in how important they are to a business and how much data they typically hold.

    We needed an algorithm that made initial assumptions about priority but adjusted based on historical data without significant hysteresis.

* **Wake-On-Lan (WOL)**: Computers support an energy-saving feature called Wake-On-Lan or WOL. The computer can be asleep to save on power consumption but still listen on its network interface for a special network packet telling it to wake up, enabling it to save on electricity but also be available for tasks like backups.

    Computers (especially older ones) take time to wake up, so Retrospect needs to wait at least three minutes after sending the WOL packet to see if the computer is awake, keeping in mind that Retrospect does not know if the computer is even on the network. The computer will eventually go back to sleep based on the energy settings.

    The algorithm sent a WOL packet to every applicable computer, moved the computer to the secondary list, and came back to the list after waiting at least three minutes checking other sources for availability. This process led to unfortunate situations where Retrospect sent WOL packets to a subset of computers but did not get back to them until they had returned to sleep mode and Retrospect would send more WOL packets, leading to energy waste and fewer backups.
    
* **Parallelization**: Despite allowing up to 16 simultaneous executions, the algorithm ran on a single thread, so when it polled each agent to check availability, the engine might wait for five minutes, because of the Wake-On-Lan support.

All of these drawbacks led to a suboptimal experience for certain subsets of customers, particularly those with larger environments.

In one instance, a corporation with 700 laptops that were configured with Wake-On-Lan attempted daily backups with a single Retrospect engine. Beyond being a far larger environment than a typical backup engine handled, the resource scheduling algorithm simply could not scale to that level.

The Engineering team researched various algorithms to preserve the core approach to dynamic scheduling while resolving the above shortcomings. We considered clustering by data source type, ranking by size, and prioritizing by bandwidth or past availability. Ultimately, we settled on a far simpler algorithm that focused on the last backup date and last backup duration as the main inputs.

Retrospect runs this algorithm, named ProactiveAI, for each of its 16 available execution units (slots) for all of the backup scripts (policies) on that Retrospect engine. Let's walk through it.

1. **Verify backup window**: Retrospect can back up every hour, every day, every Sunday, or any other schedule. As soon as ProactiveAI sees a new backup window (i.e. a new day), it will attempt to back up the sources.

2. **Verify an execution unit is available**: ProactiveAI only runs when an execution unit is available.

3. **Prioritize by next backup date**: For all available or potentially available sources, Retrospect divides them into buckets for what day they are scheduled to be backed up next.

    Using a future date might seem strange, but it can be in the past as well. This sorting algorithm ensure Retrospect prioritizes initial backups and then overdue backups. Think of it as last backup date combined with the script's schedule. As an example, Script A with weekly backups and Script B with daily backups would calculate the next backup date differently.

4. **Prioritize by last time checked**: When Retrospect reaches out to a source, it marks that time in its configuration. ProactiveAI uses this time to ensure it doesn't re-check sources that it already checked but couldn't find, so that the script can process the entire list of sources before circling back.

5. **Prioritize by the last backup duration**: Now that Retrospect is down to sources within the same day of priority, ProactiveAI sorts them based on the last backup duration. Sources with faster previous backups will be backed up sooner than sources with slower previous backups.

    As a real-life example, incremental backups of email services are fast, so those would be prioritized over a longer server backup. Because of this sorting, Retrospect will protect more sources throughout the day, but if a long server backup does not happen on a given day, its backup will be automatically given higher priority because its next backup was the day before. Conversely, a long daily server backup will not consistently crowd out shorter backups for endpoints or other data sources.

    The Engineering team experimented with using the mean or median of more data points for each source, but the resulting sort order was too prone to hysteresis. In other words, if Retrospect included more past data, including backup durations that were anomalies, the future prioritization continued to be affected for longer than we thought was useful.

6. **Default to prior order**: If there is no duration, ProactiveAI uses the prior order. For instance, if it's the first set of backups, they will occur as sources are available.

7. **Connect to the next source**: Retrospect will attempt to back up the selected source. If it's not available, Retrospect marks that time and moves on. If Retrospect times out and the client and script have Wake-on-LAN (WOL) set, Retrospect sends a WOL packet, waits three minutes, then tries to connect again. If that connection times out, Retrospect marks the sources as unavailable and moves on.

8. **Record next backup date**: After a successful backup, Retrospect marks the next backup date for the source and moves on. As discussed earlier, this future date varies based on the script's schedule.

Focusing only on last backup date and duration limits the assumptions that the algorithm needs to make about the specific environment.

The algorithm can ensure first backups are prioritized and then subsequent backups are ranked by likely duration, using only the last occurrence to avoid hysteresis. Changing how Retrospect handles Wake-On-Lan eliminates the chance of waking up a computer but not connecting to it, and per-execution unit scheduling allows up to 16 scheduling algorithm to run in parallel. Ignoring backup times means Retrospect never misses a day for a backup.

Finally, customers can solve questions of priority by creating multiple scripts, letting the application handle that rather than complicating a single script's schedule with it.

Computational advances have made advanced AI algorithms like machine learning and deep learning much more accessible as tools, but Engineering teams still need to choose the algorithm that fits their problem space.

The Retrospect Engineering team used this resource scheduling method to ensure more disparate data sources were backed up faster, vastly improving our customers' level of data protection.

In the book “Smarter Faster Better: The Transformative Power of Real Productivity”, Charles Duhigg highlights a  that those with a commitment culture are far more resilient than star cultures, engineering cultures, bureaucratic cultures, and autocratic cultures.

> “The only culture that was a consistent winner were the commitment firms. Hands down, a commitment culture outperformed every other type of management style in almost every meaningful way. 'Not one of the commitment firms we studied failed,’ said Baron. ‘None of them, which is amazing in its own right. But they were also the fastest companies to go public, had the highest profitability ratios, and tended to be leaner, with fewer middle managers, because when you choose employees slowly, you have time to find people who excel at self-direction.’ Employees at commitment firms wasted less time on internal rivalries because everyone was committed to the company, rather than to personal agendas. Commitment companies tended to know their customers better than other kinds of firms, and as a result could detect shifts in the market faster.”

Communication and collaboration drive these second-order benefits that make a commitment culture. Employee retention increases, so less time is lost in retraining and building team trust. Productivity increases from the shared sense of purpose, corporate strategy, and each person’s role in it.

I consider Retrospect to be a commitment culture. Most of the team has been together for over a decade, and a handful started at the company in the 1990s. I used to be Chief Operating Officer at Retrospect, so Sales, Marketing, Support, Operations, and Finance reported to me—around 25 people. Now, I manage the Engineering team. We’re a small, stable team, and we ship consistently good software, on schedule for the last seven years.

Creating a stable team is partly luck. We have a fantastic set of engineers with a collaborative approach. Equally important is consistent communication from management: with reports, with the Engineering team, with the product’s cross-functional team, and with the larger corporate structure. It injects a constant flow of good information into the team, reducing silos and uncertainty while building trust and increasing productivity.

For my team, I schedule biweekly 1-on-1s. It’s a 30-minute slot where we can discuss anything. People get a scheduled opportunity to ask any question that’s on their mind, and I get a scheduled time to discuss any small-scale (release schedule, task switching, interesting projects) or large-scale (performance, raises, promotions, hiring, firing, layoffs) events. If I need to change the schedule, I always add a reason for the change, so that people aren’t getting calendar notifications with no explanation. For your team, you might find a weekly cadence more necessary. Biweekly seems to be a good fit for us because we've worked together for years and we have a daily Engineering scrum.

Beyond Engineering, I have weekly 1-on-1s with other department managers. These are 1-hour slots where we discuss what’s going on in each team. The Sales head and I talk about what he’s hearing from the field, and I talk about Engineering plans to make sure our technical goals line up with Sales’ needs. The Support head and I review escalations and any technical documentation that they need. With these meetings, we stay in sync.

Years ago, we had a 1-hour weekly Product Team meeting with all of the departments. It was the main meeting for the entire team. There was a generic agenda that we didn't update. We banned laptops from the meeting because people were not paying attention outside of their own turn. One agenda topic was “Discuss escalations”. Due to the laptop ban, we literally printed out the escalation list with bug numbers and titles to discuss each, but also due to the laptop ban, no one had details because we only had titles. We realized it was not an effective meeting and replaced it with functional 1-on-1s and a monthly executive meeting.

Can you get by as a team without such periodic meetings? Yes. When Support started reporting to me, I asked the head how the previous manager had run their 1-on-1s. He said they never had one. We’ve had one every week since then. Without a periodic meeting, Support still took care of customers brilliantly, but we have a great Support team. I consider the meetings essential for issues to come up and ensure both teams are on the same page.

Engineering has a daily scrum. I’d call it a stand-up, but that would be a lie. First, we’re distributed, so we’re all on Zoom and seated. Second, the meetings never last less than ten minutes. Each person talks about what they did the day before and their plan for that day, but we drill into many tasks and address related topics. We record each one and send out a link to the group in case someone is off that day. The schedule works well for us.

We used to have weekly in-person Engineering meetings. They were one hour. Everyone talked about their progress, but as knowledge workers, engineers accomplish so much in a week that it’s difficult to remember and summarize the relevant details after a week.

We also have tried different forms of status reports to make up for the low fidelity of the weekly meeting. We’ve done daily status emails: failure, because no one else read them. We also tried a status wiki page: failure, again because no one read it. In all three cases, Engineering was less effective because the team exchanged and absorbed information only once a week. 

We switched from a 1-hour weekly Engineering meeting to a scrum three days a week when we had to ship a complicated feature with many moving parts. We needed to ensure the whole team stayed on the same page. Within a month, we all agreed that scrums were a great format and switched to a daily scrum. That was eight years ago.

Coincidentally, we switched from  around the same time. GitHub was transformational for the team: Git for its cheap branching and GitHub for its seamless review process. This tool switch enabled much more detailed scrums because code was being consistently reviewed.

Along with higher fidelity information, scrums offer much faster course correction for productivity. There were many instances in weekly Engineering meetings where a developer had been blocked on a task or going down the wrong path for four days. That’s four days of lost productivity. What if each engineer was blocked for an average of one day a week? Reclaiming that day would make your team 25% more productive. You’re effectively hiring 25% more just by ensuring every member on the team is not stuck.

Since scrums are daily, the team can use the meeting time to discuss other topics ad-hoc, such as new escalations from Support, release schedules, or product roadmap, and it’s a time to reinforce why we are shipping what we are shipping, bringing Engineering tasks back to what problems they solve for customers. I occasionally use the meeting as an opportunity to explain what other departments are doing, so that the Engineering team connects their work with other department's work. It’s never “us vs them”.

At a previous job, the CEO would send a monthly email after every good month, summarizing the wins and the revenue. When there was no email, everyone knew it was a bad month, but no one knew how bad, leading to gossip that might have been worse than the truth.

Since Retrospect, Inc. became private in 2011, we’ve had monthly company meetings. Before our office went virtual, they were stand-up meetings. No slides. Sales, Product Management, Marketing, Engineering, and Support would present. It was a great venue for people to ask questions and discuss how the company was doing. Sometimes, there were difficult topics to discuss, like a bad Sales quarter or even a layoff, but we’ve also been able to celebrate hiring, promotions, and great Sales quarters. Having a consistent meeting where every employee gets to hear from every department helps break down functional silos.

Even after acquisition and a move to a functional organization, we still hold monthly company meetings for Retrospect to make sure every department is on the same page and can ask questions in a group environment.

StorCentric acquired Retrospect in 2019, and since then, we transitioned into the larger functional organization with each team rolling up to its respective StorCentric department.

The StorCentric executive team holds monthly business reviews for each product line, so that each functional team is also on the same page about each product and the overall corporate strategy. We can see trends specific to the business unit, and it allows the team to drill down into specific problems.

> “A foolish consistency is the hobgoblin of little minds.”
- Ralph Waldo Emerson

We adjusted our Engineering meeting schedule repeatedly until we arrived at a format that worked well for the team. That was eight years ago, and most of the team that was there then is here now--a testament to our commitment culture. On the other hand, after the acquisition, we cancelled our monthly Retrospect executive team meeting after it was superseded by the StorCentric monthly business review.

Don’t hold a meeting or attend a meeting because it’s on your calendar. Meetings, from 1-on-1s to scrums to monthly all-hands, need to have a point, so be intentional about them. Each enables managers to maintain consistent communication with their staff, other departments, and their bosses.
Retrospect Engineering has always hosted its engineering tools on-premise. For source control, we've used CVS, Subversion, and now GitHub Enterprise. For issue tracking, we've used Bugzilla. Recently, we needed to move colos, translating into a possible week of downtime for the Engineering team.

I thought it would a great opportunity to try migrating our critical infrastructure to the cloud as a stop-gap solution, so over a weekend, I migrated our on-prem GitHub Enterprise and Bugzilla to Amazon AWS, available through AWS VPN Client. The entire deployment cost $300 for the two weeks we had it active without a day of downtime for the team.

Amazon AWS provides all of the services we needed to set up a private cloud with the necessary topology. The AWS building blocks: VPC, EC2, S3, and Route 53. The AWS Management Console Dashboard provides a single interface to the entire breadth of AWS's services. Note that some, like S3, provide a global view whereas others, like VPC, only allow per-region selection.

## 

Amazon Virtual Private Cloud (VPC) is the overarching services with the necessary components for building a private cloud.

I used  to set up the following:

* **VPC**: Private cloud instance with associated CIDR ranges.
* **NAT Gateway**: Allows private-subnet EC2 instances to connect to public subnet with an Elastic IP ()
* **Internet Gateway**: Allows public-subnet traffic to connect to internet.
* **Subnets**: Both private and public. EC2 instances on a private subnet within a VPC cannot access the internet without a NAT Gateway on a public subnet and an Internet Gateway on the public subnet.
* **Route Tables**: Associated with subnets to direct traffic.
* **Reachability Analyzer**: Analyzes whether one component can reach another component. Used this for understanding why an EC2 instance could not connect to the internet.
* **Client VPN Endpoint**: Uses OpenVPN client to connect endpoint to the private cloud. Followed . Had to set up new certificate in Amazon Certificate Manager.

Most of the setup was straight-forward, but one setting that took me a while to get right was the subnets with route tables. The private subnet needed to point all traffic to the NAT Gateway on the public subnet, while the public subnet needed to point all traffic to the Internet Gateway. That configuration allowed EC2 instances on the private subnet to connect to the internet.

## 

Amazon Elastic Compute Cloud (EC2) runs server instances for Windows, Linux, and macOS, including block storage, security groups, keypairs, and IP addresses.

We've been using EC2 for 11 years in various forms, and they now have an overwhelming number of options (396) to choose from for configuration, from general-purpose to compute-optimized to memory-optimized with x86 and ARM support. For our purposes, we used `t2` instances: Amazon Linux 2 on `t2.medium` for Bugzilla and GitHub Enterprise Server on `t2.xlarge` for GitHub. GitHub provides . Let's walk through the EC2 components:

* **EC2 Instances**: Runs servers.
* **EC2 AMIs**: These are server images. We used the standard  for LAMP and GitHub's provided AMI.
* **EBS**: Elastic Block Storage. These attach to EC2 instances as block-based volumes. Use  to format and mount on Linux.
* **Elastic IP**: Public-facing IP addresses. These are available both in VPC and EC2.

## 

Amazon Simple Storage Service (S3) is used to store objects in the cloud. This service was one of the first from AWS; Bezos described it as `malloc` for the internet.

We used S3 to host the GitHub and Bugzilla backups. We use S3 for everything, with 77 buckets and counting.

## 

Amazon Route 53 is used for DNS routing.

In addition to handling our website and mail DNS routing, we utilize it for giving private network instances a public DNS name, such as github.retrospect.com. Relying on a public DNS provider makes it simple to maintain and update, and it's a fast lookup compared to private lookups that we've used in the past. We used to use Zerigo for DNS until Zerigo had one too many outages. (Zerigo shut down within a year.) Migrating to Route 53 took about an hour, and the monthly bill went from $13/mo to $3/mo. Finally, abstracting away the IP address allowed us to seamlessly switch from colo to AWS to new colo without changing our Git development settings.

The actual migration was simple. I took the most recent backups from GitHub and Bugzilla, transferred them into S3, then transferred them into the Bugzilla instance. I used the Bugzilla server as the staging server for GitHub.

GitHub Enterprise has a GitHub repo for backup and restore utilities. After setting up the configuration file, we do backups with:

    ghe-backup
    tar zcvf github.tgz ./backup-utils

And you can do a restore with:

    tar xvf github.tgz
    ghe-restore github.retrospect.com

For Bugzilla, we simply tar/gzip the directory and dump the MySQL database:

    tar zcvf bugzilla.tgz /var/www/html/bugzilla
    mysqldump -u root -p bugzilla | gzip > bugzilla.sql.gz

We restore with:

    tar xvf bugzilla.tgz
    gunzip < bugzilla.sql.gz | mysql -u root -p bugzilla

One issue was GitHub's compression. I had been using a spare Mac VM to do GitHub backups. When I compressed it to transfer to S3, I used macOS's Archive Utility from the right-click menu. As it turns out, gzip didn't know what to make of that. I had to tar/gzip it again and transfer it to get it uncompressed on the EC2 instance. Another important note is not using "-h" for tar because it follows symlinks rather than preserving them, doubling the size of the GitHub backup due to the "current" symlink.

Another issue was Bugzilla's backup. A recent attachment broke the `mysqldump` export of the MySQL database. I had to delete the attachment from the MySQL `attach_data` table to get a working export. It was 2GB, going back 10 years.

The final issue was setting up Bugzilla itself. Installing the correct Perl CPAN modules is a pain, even with `yum` as a tool.

Migrating back from AWS to the new colo was just as easy, using the same steps in reverse.

There are three major cloud providers: Amazon AWS (at 33% market share), Microsoft Azure (at 15% market share), and Google Cloud Storage (at 5% market share). I used Amazon AWS because I already have Amazon AWS. Let's look at the services offered by the other two platforms:

For Microsoft Azure:

* Virtual Network for private cloud
* Virtual Machines for servers
* Blob Storage for object storage
* Azure DNS for DNS entries
* VPN Gateway for VPN service for endpoints into the private cloud. See .

For Google Cloud:

* VPC for private cloud
* Compute Engine for servers
* Cloud Storage for object storage
* Cloud DNS for DNS entries
* Cloud VPN for peer-to-peer network connectivity, but GCP does not offer an equivalent to AWS Client VPN Endpoint. There is a  for it.

Cloud is on-demand infrastructure. It took me a weekend--from googling "AWS VPN" to sending out VPN profiles to the team--to get the temporary environment set up. By migrating our source control and issue tracking database to Amazon AWS, the Engineering team didn’t miss a day of work while our servers were on a truck. The IT team at the other end set the environment up quickly once the hardware arrived, but we would have missed seven days of work without the AWS infrastructure.
Amazon AWS, Microsoft Azure, and Google Cloud are the three main global cloud providers and control 60% of the market, worth $129B in 2020, according to . Microsoft Azure, in particular, has significantly increased its cloud provider market share from 10% in 2017 to 20% in 2020. In June, we launched Retrospect Backup 18.1 with seamless integration with Microsoft Azure.

Microsoft Azure offers a wide variety of services, including virtual machines, virtual private networks, and blob (object) storage with object lock support. Retrospect Backup now integrates with all of these. You can perform a cloud deployment with Retrospect Backup or Retrospect Virtual into an Azure VM and connect it to your network with a site-to-site connection.

You can protect on-prem data in Azure Blob Storage and utilize Immutable Backups to protect that data against ransomware, and you can even protect the data in Azure Blob Storage in a different cloud or on-premise, adding an additional layer of data recovery.

We made a short video for the launch.

Learn more at .

With the addition of Azure to the wide range of cloud storage providers that Retrospect supports, customers can select the best cloud based on their unique business.

Retrospect Backup first added cloud storage support in 2016 with Amazon S3, Google Cloud Storage, Dropbox, and a dozen other regional providers. Since then, the Engineering team has shipped five major upgrades and many more minor updates, including smarter backup workflows for scale (ProactiveAI), new destinations for concurrent operations (Storage Groups), a hosted management service (Retrospect Management Console) and more. We have a  on our website.

Congratulations to the whole Retrospect team! We launched a massive release this spring with ransomware protection, security reporting with geo tracking, cloud data protection, cloud deployment, virtual improvements, and now Microsoft Azure integration. It meant juggling multiple, connected schedules, different bug milestones, and hundreds of pull requests, but the Engineering team shipped on schedule.

 is a web application that happens to include a website.

With Retrospect.com, visitors can learn about our data protection suite in eight languages. Customers can log in to the support portal to view their order history, license codes, and support tickets as well as access personalized upgrade options in the upgrade wizard. Partners can log into the partner portal for client order history, personalized sales campaigns and quotes, partner tools and collateral for their sales territory, trainings and printable certificates, and support tickets.

I built these CRM tools incrementally over the course of five years based on consistent communication with Sales and Support to address their pain points with customers and partners. Each tool has enabled Sales, Marketing, and Support to efficiently deliver deep customer experiences at a scale they could not handle without a far larger team.

I’ll first walk through the stack and feature set, and then I’ll discuss the implementation tradeoffs.

Let’s walk through the stack:

* Web framework: Ruby-on-Rails
* Underlying CRM: Salesforce
* CRM Integration: Restforce Ruby gem
* Authentication: Devise
* Database: Postgres
* Email: Twilio SendGrid
* Deployment: Salesforce Heroku
* Source Control: GitHub Enterprise

Retrospect is used by homes and businesses in over 100 countries, and the software is localized in six languages. To enable this global reach, the website is localized into English, German, Spanish, French, Italian, Japanese, Latin America Spanish, and Portuguese along with AU and UK English regions.

On any page of Retrospect.com, you can scroll to the footer and click one of ten language links, and you will be taken to the same page but in that language. The app automatically chooses the language based on your browser settings.

The actual pages in GitHub don’t contain English or Japanese. Instead, they have code to translate keys into text strings, based on the language you have selected. Retrospect.com includes over 10k strings, stored in dozens of YAML files and versioned by GitHub.

Even our knowledge base and documentation live in the website and support different languages through the same process, although both use the Asciidoc format instead of HTML/Erb or Markdown.

Read more about localization at scale in .

Customers can sign up for the support portal, and that portal account is automatically associated with their own Salesforce account based on their email. From the portal, customers can see the latest news along with pre-release information if they are part of our beta program. Along the top, they have easy access to their licenses, support tickets, and referrals.

The "Licenses" section shows their purchases including licenses and their upgrade options.

The "Support" section lists all of their support tickets. Customers can create new tickets, update them, and view the ticket history, including attachments.

Internally, we have used Salesforce Cases for tracking support tickets since 2011, but customers did not gave access to them until we created the support portal. A great benefit of continuing to use Salesforce as the backend is Support did not need to change their workflow.

With a self-service portal, customers are able to look up license keys and track support tickets, leveraging our existing Salesforce data and Support’s existing workflow.

Partners can sign up for the partner portal, and that portal account is automatically associated with their own Salesforce account based on their email. From the portal, partners can see the latest news including pre-release information, their certification status, and their regional sales rep's contact details. At the top, they have easy access to enablement tools, pipeline, client base, and support tickets.

As a CRM, the portal lists client details including purchase history, licenses, and upgrade options. Distributors can see their reseller network as well. For enablement, the portal has a variety of tools:

* Lead pipeline
* Deal registration for resellers and distributors
* Personalized email campaigns with embedded logo and contact information
* Partner collateral
* Pre-release product information
* Partner trainings
* Personalized training certificates to download

Our sales team is able to scale the number of partners they enable by utilizing this self-service portal to help resellers and MSPs all over the world take advantage of these personalized tools and incorporating our existing Salesforce database.

The upgrade wizard, available at , supports upgrade options, upgrade license fulfillment, annual support and maintenance (ASM) license fulfillment, and even recent upgrade fulfillment.

Each version of Retrospect has a new set of license keys for the many editions and add-ons included. Customers can type in a license code and view their upgrade options, but it supports far more as well.

Because of our channel business model, customers can purchase upgrades and support renewals without first validating that they own the software. They can buy a product and receive a license code for it, but the code does not license the product. Instead, the code tells us what they bought, and we verify that they are an existing customer before issuing a license fulfillment with a functioning license code.

Our Sales and Support team used to do this manually in Salesforce, creating new “zero-dollar” opportunities, adding products and licenses to them, and then sending an email fulfillment to the customer. Thousands of times.

The tool automates these manual tasks and saves thousands of hours per year for the company.

The upgrade wizard is not inside the support portal to make it more accessible. Sales and Support frequently direct customers and partners without portal accounts to the wizard, and the sign-in wall is an extra barrier. There is a clear trade off: ease-of-use versus potential piracy. You can validate a license without signing in, meaning any person could test prospective licenses until they found one without purchasing. In practice, the key space is enormous: 1m licenses per release out of 36^16 or 1 in a quintillion (8x10^18). If the issue ever arises, through Sales or Support, we can easily move it inside the portals.

When we wrote the first version of the website nine years ago, we did not have these components in mind. We just needed a website after we were spun out into a private company.

As a small team, we didn’t have a web developer in Marketing. We had engineers who knew web development. We could have created it in Wordpress, but Engineering wasn’t familiar with the tool. Wordpress felt like a CMS designed for Marketing, but we weren’t the target audience for it.

We knew the Ruby-on-Rails web framework, so the first iteration of the site used that. We needed to get the site off the ground, and Ruby-on-Rails was the quickest path.

But at a deeper level, we saw Wordpress’s accessibility to non-developers in Marketing as a limitation because of the constraints it imposed. Wordpress operated at a higher abstraction layer, and Engineering wanted to control more of the stack. Hosting Wordpress, backed by a database, was an issue too because we wanted a website versioned by GitHub. Maybe these tools existed for Wordpress at the time, but we didn’t know. 

We could have taken a modular approach: the website in Wordpress, sales data in Salesforce, support tickets through Zendesk, and partner portal though Force.com. That approach would enable Sales, Marketing, and Support to manage their tools themselves and depend on a third party rather than Engineering’s bandwidth. However, it would be far more expensive, less tailored to our specific needs, and less integrated with our other tools.

For example, I built an internal tool for order processing that saves significant time every day: . I could have built it without the rest of the tools, but those tools were the reason that we had a stack for integrating Salesforce and a web app with a sign-in wall.

I built the partner portal in about two weeks. There was no incremental cost, and with its Salesforce integration, it provides a far better experience than other solutions. Development speed is an important factor. 

However, long-term maintenance of homegrown tools is another factor to consider. In-house tools that teams depend on can prove to be very expensive if the developers leave and takes their domain knowledge with them.

At a previous company, a developer built a license activation server at Sales’ request to combat piracy. Three months after the developer moved on, the license server stopped working, and the entire company ground to a halt. Customers couldn’t use new licenses. It took two days for Engineering to diagnose and fix the issue. The root cause was the developer used their own credentials for various API calls, and the credentials were cleaned up after 90 days. Luckily, there wasn’t a worse bug. With a third-party service, you can offload the risks of institutional knowledge and technical debt to them. 

Finally, there is the very real opportunity cost of Engineering building custom tools for other departments. We thought that the calculus made sense for Retrospect because we were a self-funded private company with a small, technical team. As part of a larger organization, the payoff is less clear unless there is an official department dedicated to development and, equally important, maintenance.

Engineering can enable Sales, Marketing, and Support to deliver deep, scalable interactions with prospective buyers, customers, and partners, tailored to each departments’ needs.

But utilizing third-party services might make more sense in terms of projected revenue or savings versus price and in terms of long-term maintenance and technical debt. 

The question is whether the opportunity cost is worth the investment.
After 13 years and 177 posts, this blog is a bit long for a single scrolling index, so I added years to delineate the content as well as post count per year.

That involved a bit of programming in , a template language created by Shopify and used by Jekyll. Liquid does not expose support for hash creation, even though Jekyll can provide hashes like `site`. Fortunately, it supports `group_by_exp` expressions, available in v3.4+.

Below is the code I used:

I've been using Jekyll for eight years now. In 2013, I converted from Wordpress’s database-driven approach to Jekyll’s static HTML-generated approach: . It continues to work well.

And for those who use Jekyll, you can post Liquid as text in a post by using &#123;% raw %}&#123;% endraw %}.

Cloud storage provides a low-cost, scalable, offsite storage location for a multitude of corporate assets. Companies use cloud storage for all sorts of data, from website assets to corporate storage to unstructured data used for analysis.

Retrospect has used Amazon S3 for storing various assets for the last decade. Today, we store tens of terabytes of content on it: website assets, sales collateral, customer logs, Engineering builds, and, of course, corporate backups.

Cloud data storage is fantastic for its infinite supply, worldwide accessibility, and abstraction. It allows companies to move up the storage stack and not manage the physical bits of hardware that are required to store vast quantities of data–all for an affordable per-data-unit cost. Amazon S3 costs $0.023 per GB per month, and other providers like Wasabi and Backblaze B2 are even more affordable.

To achieve SLAs up to 11 9s like Amazon S3, cloud storage providers built redundancies into their infrastructure to protect your data from hardware failure. Objects are stored in multiple copies in a single zone and in multiple different zones. Two data centers can go offline, and you'll still have your data. Awesome.

Unless you delete a file accidentally. Or ransomware comprises your account and encrypts your data. Or a malicious insider deletes every bucket.

Working at a backup company, I hear stories about data loss a lot. Bad things happen. You can protect yourself against one type of data loss, only to be hit by another. Cloud data is no exception. 

Retrospect Backup can now protect your cloud data with Cloud Data Protection. Retrospect treats cloud storage as a first-class storage volume and integrates cloud volumes into the rest of Retrospect's deep feature set. Cloud Data Protection is included for an unlimited number of cloud volumes in Retrospect Backup's server-level editions on both Windows and Mac platforms, and Retrospect integrates cloud storage from all the major cloud storage providers: Amazon S3, Microsoft Azure Blob Storage, Google Cloud Storage, Wasabi, Backblaze B2, and MinIO.

I made a video covering common use cases and step-by-step walkthroughs for Retrospect Backup for Windows and Retrospect Backup for Mac:

Learn more at .

Retrospect Backup has a deep feature set, and Cloud Data Protection allows organizations to incorporate cloud data into their standard backup strategies. Let's walk through a number of facets:

* **On-Premise or Cloud**: You can back up the cloud volume to on-premise storage, back up the cloud data to a different cloud from a cloud server instance, or back up a local volume and restore it to the cloud. A cloud volume has the same level of protection and interoperability as a local volume.
* **Incremental-Forever Backups**: Retrospect uses incremental-forever backup technology to ensure each backup is as compact as possible while still providing point-in-time restore, and cloud volumes are included.
* **Scheduling**: Retrospect supports cloud volumes under scheduled scripts or policy-based scheduling called ProactiveAI, which prioritizes backups based on a backup window and availability across local volumes, NAS shares, servers, endpoints, email accounts, and now cloud volumes.
* **Filtering**: Retrospect includes a granular set of filtering options for companies to include only critical data or exclude extraneous data, based on volume information or file information. For instance, you can easily write a filter to only back up cloud data that's created in the last 90 days and less than 100MB.
* **Cloud Backup**: Retrospect can also use cloud storage as a destination for backups, with support for over 20 different cloud storage providers.
* **Replication**: In addition to backup, Retrospect can replicate between cloud volumes, local volumes, NAS shares, servers, endpoints, or any combination, with scheduling and filtering available.

Retrospect Backup's goal is to ensure your data is protected from the wide array of possibilities for data loss. With Cloud Data Protection in Retrospect Backup, you can back up and restore cloud storage with the same policy-driven workflows that you use for on-premise systems and storage.

Congratulations to the Retrospect Engineering team for shipping this excellent feature in such a deeply integrated fashion! With the continued migration to the cloud, data protection that is agnostic to volume type or location helps companies protect their data in the way that works for their business constraints.
Ransomware is a huge global threat to businesses around the world. The problem for companies is that their storage is always connected with full access for admins. When ransomware gets the administrative credentials, it has full access too. There is no policy to say that no one, not even the administrator, can change this file for a set amount of time.

Cloud Object Lock does just that. Because cloud storage providers like Amazon S3 control the API, they can add features like Object Lock. This lock is a retention policy for a specific version of a file that is locked from changes from every user, including the administrator. You can think of this as a virtual air-gap in the cloud because there is no way, barring to close the account, to delete that file before the retention date is passed.

Retrospect was one of the first data protection solutions to add ransomware protection using immutable backups: . Retrospect utilizes Object Lock technology in major cloud storage providers to set a retention policy for cloud backups to ensure no one, not even the root user, can delete them during the retention window.

Creating an immutable backup set with Retrospect Backup is easy. There is a single checkbox in the user interface to enable it and a number of days to specify:

However, there is a lot of functionality underneath that checkbox to create immutable backups. Let’s dive into the technical details.

Retrospect Backup uses ProactiveAI for policy-driven scheduling and forever-incremental backup technology to minimize backup sizes while ensuring a point-in-time restore. The first backup is a full backup and every subsequent backup is called an incremental backup. Those incremental backups depend on previous backups. If a file doesn’t change, it doesn’t get backed up again.

Ordinarily, this workflow is a fantastic combination of minimizing storage while providing a backup that can perfectly recreate a point-in-time snapshot of the volume being protected. But that changes if you’re concerned the previous backups might be deleted. If a file is no longer locked, it can be deleted maliciously. Retrospect Backup needs to create backups where any backup within the rolling window of immutability are fully contained point-in-time snapshots of the volume.

Retrospect Backup accounts for the rolling window in two ways:

* **File Matching**: Retrospect adjusts its file matching to take into account retention policy for a given backed up file. A file that is outside of the retention policy is no longer considered to be backed up, and Retrospect will back it up again.
* **ProactiveAI Scheduling**: ProactiveAI determines the next date the script will run and backs up any file that will fall out of the retention policy by that date with forever-incremental backup, predicting into the future to ensure the file is protected at all times.

The consequence of this change is Retrospect will back up any file that is not protected in an immutable backup. Let’s say you back up every week and you set the retention policy for 4 weeks. Retrospect will back up every file every four weeks, regardless of whether it changed, because it needs to keep those files in the ransomware protection’s rolling window.

This process ensures that customers always have immutable backups with complete point-in-time restores. There is never a time when a backup depends on an out-of-policy file while preserving forever-incremental backups.

There are two types of approaches from cloud storage providers: per-object policies and per-bucket policies. Per-object policies can be applied granularly to specific versions of an object at the time of creation, and they can vary within a bucket. Per-bucket policies are created for an entire bucket and are applied uniformly to every new version of any object in that bucket.

To compare with Retrospect:

* **Per-Object Policy**: You can create Backup Set A with an immutable retention policy of 2 days and Backup Set B with an immutable retention policy of 6 months in the same bucket, and the bucket does not need to have a bucket-wide policy.
* **Per-Bucket Policy**: You can only set a bucket-wide policy for immutable retention, so every new object is set to that retention period, regardless of what you have set in Retrospect.

Cloud storage providers with per-object policies are Amazon S3, Wasabi, Backblaze B2, MinIO, and Microsoft Azure Blob Storage (Preview - September 2021), while those with per-bucket policies are Google Cloud Storage and Microsoft Azure Blob Storage.

There are also different policy modes:

* **Compliance Mode**: The policy is time-based and enforced for every user, including administrators.
* **Governance Mode**: The policy is a legal hold, does not expire, and can be cancelled by a user with those permissions.

See  for more information.

Retrospect Backup uses Retention Mode for its immutable backups. When you create an immutable backup, there is no permission level that will allow you to delete that version of the backup files. The root account cannot delete them. The only way to delete them is to close the account.

Because there is a way to ultimately delete the files, it's important to use multi-factor authentication (MFA) for your root account on the cloud storage provider.

Let’s walk through the user interfaces for retention policy dates in the different cloud storage providers.

### Amazon S3

Below is Amazon S3’s Retention Mode UI.

You’ll see it specifies the mode, the "Retain Until Date", and the version of an object that you’re applying this to. Retrospect Backup does this step automatically when creating an immutable backup.

For Microsoft Azure Blob Storage and Google Cloud Storage, you will need to create the retention policy manually because they only support per-bucket policies.

Click on any object and scroll down on “Properties” to “Object Lock retention”.

### Wasabi

Click on any object and “File Details” appears.

### Backblaze B2

Click on any object’s blue link and “Details” appears.

### Cyberduck

Select any object and click “Info” then “Metadata”.

### Microsoft Azure Blob Storage

For a container, select “Access Policy”. Note: Per-object (blob) version locking in preview (September 2021)

### Google Cloud Storage

When viewing a list of files, see the “Retention expiration date” column.

### Viewing and Deleting Versions

One important nuance is how to view versions of a file. Only Amazon S3 and Cyberduck show versions. See below. Other interfaces choose to display a simplified version of the actual underlying content while preventing you from taking certain actions, like deletion.

One underlying feature is a delete marker. When you delete a object in a versioned bucket on Amazon S3, the file is not deleted. You are adding a delete marker as the next version of that file, and Amazon S3 understands it should not display that in the interface without "Show Versions" enabled.

Let’s look at the difference between deleting an object ("delete") and deleting an object version ("permanently delete"):

In Wasabi or Backblaze, you don’t see versions, even though they are there for buckets with Object Lock enabled. Wasabi won’t let you delete files through their interface, but if an attacker added a delete marker to your file using an API, the file would no longer show up in Wasabi. You would have to use Cyberduck or other API to see that the locked files were indeed still there.

I recorded a detailed video of the use cases and step-by-step walkthroughs on both Windows and Mac platforms as well as this technical deep dive into how Retrospect's ransomware protection works and how Object Lock is implemented across cloud storage providers.

Note that much of this content is also available on the Retrospect website, under  and [Technical Deep Dive on Ransomware Protection, Object Lock, and Immutable Backups
Ransomware](https://www.retrospect.com/en/support/kb/technical_deep_dive_on_ransomware_protection). It took me a bit to compile, but the broad overview and then technical details have really helped clarify people's understanding of ransomware, Object Lock, and Immutable backups–both inside the company and for our partners and customers.
Retrospect, Inc. was a bootstrapped startup of sorts. We were spun out in 2011, responsible for a product that was 22 years old, with around thirty people at the company. We had an accounting firm, Accretive (now Countsy), with a fantastic and helpful advisor there, and they maintained our financials in NetSuite. From 2016 until our acquisition in 2019, I was Chief Operating Officer (COO), so I was responsible for the financials and the cash flow.

The cash flow of a small company is composed of two pieces: Accounts Payable (AP) and Accounts Receivable (AR). AP is the list of vendors to whom we own money with our payment deadline. AR is the list of customers that owe us money with their payment deadline. Sales closes opportunities (bookings) and hands off booked orders to Operations; Operations fulfills those orders and bills customers (billings), transitioning those orders into AR and over to Finance; and finally, Finance follows up on AR and collects money from customers for those orders (collections). This back-office process is how product turns into cash, and, for us, it involved many manual steps.

The core component of a bootstrapped startup is cash in the bank. We needed to know our current financial position, predict the company's cash flow, and minimize past-due AR. Let's walk through how we did that with a simple spreadsheet.

I’m not referring to complicated machine learning models to predict cash flow. We needed basic spreadsheet-driven estimates for our expenses and our cash. We did not have a subscription service at that point, so we did not deal with annual recurring revenue (ARR) or subscription churn rate. Our focus was simply understanding our current business, selling software licenses online and in the channel as well as support contracts and collecting payment from customers.

AP was consistent for us every month. Countsy did a great job maintaining our NetSuite instance, so that I could drill into any department and see their expenses for the three years or the last quarter. For us, the largest expense was payroll. Beyond that, we had sales rep contractors, our European support team, our accounting team, and a long list of credit card charges. Every quarter, we had to account for quarterly bonuses as well as quarterly bills, such as Salesforce. When we grew our Sales staff, I knew how much those additional people were going to cost on a monthly basis. There were occasional out-of-the-blue bills, such as the yearly online store fee, but in general, our burn rate was easy to predict.

AR took more effort. Most of our customers had NET30 payment schedules, meaning that they had 30 days to pay us after we sent the invoice. However, when I started as COO, we had an AR forecast of one week, meaning we knew which customers were going to pay us what amount for the next seven days. Think about those two data points. Our customers had 30 days to pay us, and we only predicted payments a week out. We had better data than a one-week horizon.

To predict cash as far out as possible, I built an “AR Forecast” spreadsheet in Google Sheets. Using a report in NetSuite, I copied every customer’s open invoices with amount, purchase date, and payment terms into the spreadsheet, and I used formulas to note when the invoice was due. For each customer, I learned what their past payment schedule was and manually predicted around when they would pay us. The spreadsheet allowed me to extend our AR forecast from 1 week to 6 weeks as well as accurately track our total AR.

As a bootstrapped startup, tracking AR and predicting payments were critical to understanding our runway and spotting any hurdles.

In addition to only having a 1-week cash prediction, we had a huge past-due AR balance. According to NetSuite, many customers had unpaid invoices that were over 90 days old. Some open invoices were over a year old.

All customers had their own processes for paying vendors. Large distributors' Finance departments ran like clockwork, but they were still idiosyncratic. For instance, one consistently sent checks every week for all of the open invoices that had past NET30 that week, but the key phrase was “sent checks”. The checks were marked as sent in their system, but they always managed to arrive two weeks later. Adding up the times, the checks arrived six weeks after we sold the products: NET45, not NET30. Still, large customers were consistent. 

Smaller customers varied in terms of how prompt they were at paying. We had a number of small distributors that were great at paying on time. Others needed reminders. One change we made was ensuring there was an employee who was responsible for following up on past-due AR.

A past-due invoice did not necessarily mean that we hadn’t been paid. All customers need to submit their payment with remittance information, which lists what invoices the payment covers. Sometimes, remittance information is incorrect, leading to accounting not closing the correct open invoice. Other times, accounting has the correct information but has not yet updated the open invoice.

Our huge past-due AR balance was a mix of all three. Accounting only processed invoices every other week, so there were frequently unprocessed invoices. A subset of remittance information was incorrect, so while the customer thought the invoice was paid, our system did not. Working with customers resolved a long list of these that had built up over a couple years. Finally, a couple customers were consistently late at paying. Our diligent back-office person worked with them to figure out a payment schedule that we both agreed on.

By minimizing our past-due AR, I knew what our real AR was, so I could predict payments more accurately.

We didn’t need to worry about EBITDA–shorthand for earnings before interest, taxes, depreciation, and amortization. It’s a common accounting measure for a company’s financial health. As a bootstrapped startup, cash was critical, and EBITDA was only useful for our corporate taxes. We followed cash basis accounting (as opposed to accrual basis accounting), and we focused on profit in terms of cash to decide questions like hiring, bonuses, or layoffs.

Understanding a small company’s cash flow isn’t hard, but it does require patience and tools. NetSuite is a fantastic back office tool, but it’s not necessarily ground truth. We couldn’t simply click on “AR Report” and accept the sum at the bottom. The data in the report wasn’t accurate in multiple ways. Digging into the actual data, customer by customer and vendor by vendor, helped us see what our actual AR was, allowed us to predict our cash up to six weeks using a simple spreadsheet, and enabled us to manage the cash flow for our bootstrapped startup.

When I write a post and add an image, the image lives in the "files" folder. When Jekyll builds the static site locally, I can see the image inline. When I push the site to GitHub Pages, I can see the image online in the post with my web browser. But when I view the feed in an RSS reader (Reeder for me), no image.

The issue is the URL for the image is a relative path instead of an absolute path:

The RSS reader doesn't know what the base path is. But if I wrote it with the absolute URL format locally, I couldn't see it until I pushed the post live.

Web browsers handle relative URLs, but RSS readers are not consistent about it. You can add "xml:base" to your feed, but there is no guarantee that the RSS reader will observe it.

Below is a quick Jekyll hack to replace relative URLs for links and images with absolute URLs in `/feed.xml`, using Liquid's `replace` method:

Here is the code in the context of my entire `feed.xml` file:


Retrospect is a small company, so when we build new products or features, our goal is to find the intersection between what customers need and what our team can build, deliver, sell, and support.

In 2017, feedback from customers and partners frequently touched on how difficult it was to monitor and manage multiple instances of Retrospect Backup. Retrospect Backup is software that is installed on Windows and Macs, either servers or desktops, so administrators needed to log into each computer and look at the status in the application or configure email reporting for each instance. The process was painful and manual.

The Retrospect team talked about different solutions to this, including third-party monitoring service integration, better email reporting, or a web UI for each instance. Eventually, we agreed that the best long-term solution would be our own hosted service.

With a hosted service, customers and partners could log into a single web application and see an aggregated view of their entire backup infrastructure or, in the case of partners, all of their clients’ backup infrastructures through a single pane of glass. That one-sentence product pitch was the goal for , focused initially on monitoring and analytics and eventually on management.

Building a minimum viable product or MVP represents a compromise between Sales, Engineering, and Product Management. Sales is looking at competitors and wanting to ship the equivalent solution as soon as possible. Engineering is trying to architect and deploy a well-designed product. Product Management is representing the customer and how this product will address their pain points. Finding a balance between these competing views comes down to a well-defined product roadmap that satisfies all three perspectives and begins with the MVP.

The value of the MVP is shipping and feedback. Shipping an MVP means the Engineering team has built a foundation for the product with a small set of features along with a packaging and deployment process. Feedback means you have customers–internally and externally–who are interested enough to give you feedback to help you iterate toward a better future product.

In our case, we wanted to ship the MVP for a hosted service that a customer could sign into and see an aggregated view from multiple Retrospect Backup instances. That translated into the following requirements:

* **Hosted Service Platform**: Heroku
* **Database**: Postgres
* **Application Framework**: Ruby-on-Rails
* **Authentication**: Devise
* **Security**: SSL encryption
* **Data Processing**: SuckerPunch
* **Integration**: Retrospect Backup connection and JSON data
* **Dashboard**: Aggregated view of multiple instances

The actual customer use case is the final point because the others represent the foundation of the product. We couldn’t get data into the service without them. The product requirement for the dashboard was a set of useful statistics and charts, and we could build more after shipping and getting feedback.

For comparison, we added the following features in future releases after the MVP:

* **Scalable Data Processing**: Sidekiq
* **Detailed Instance Monitoring**: Backup Report, Sources List, Scripts List, Backup Sets List, Scripts List
* **Interactivity**: Pause/Stop Support for Activities and Script Creation

We started building in May 2018 and shipped the MVP in August 2018. After testing it in-house for a month, we released it to the public as a beta. It required the latest version of Retrospect Backup to connect and send analytics data.

When you’re building an MVP, you also need to acknowledge the tradeoffs. The problem with shipping an MVP is those early decisions build the foundation for the product, and as teams continue to build features on top of them, they also find it harder to justify the time commitment of changing them.

For us, we wanted to ship as soon as possible, which meant utilizing our existing knowledge of Ruby-on-Rails. We could have built the service as a Node.js application with React or Angular, but we weren’t as familiar with that approach. However, switching to a different application framework now would be a significant project because of the features we would need to replicate.

Moreover, we chose to have the API endpoint for the integration be served by the user-facing application. That design choice simplified the architecture in the beginning. We only needed one web instance running to both ingest data from Retrospect Backup and also serve the dashboard. However, as the service scaled, we needed to address that bottleneck.

MVPs are a fantastic approach to getting a product into customers’ hands and iterating based on their feedback, but your team needs to acknowledge the Engineering tradeoffs that come with that approach.

With the MVP shipped, we revisited the product roadmap. Sales wanted more management abilities to compete with other services. Product Management pushed for easier workflows for customers to use the service. Support pointed out issues that customers had. Engineering evaluated performance optimizations for scaling the service.

We’ve added a number of features since the MVP shipped, including the following:

* **Navigation**: Better navigation for organizations with managed organizations and multiple Retrospect Backup instances
* **Detailed Monitoring**: More insight into Retrospect Backup instances via Backup Report, Sources list, Scripts list, Backup Sets list, Scripts list
* **Interactivity**: Pause/Stop support for activities, shared scripts, and backup set/script creation
* **Status**: Detailed status for each Retrospect Backup instance’s service connectivity
* **Scalable Data Processing**: Sidekiq and Rails AutoScale

Web applications are particularly suited to MVPs and iteration based on feedback because the company controls the delivery. We only update Retrospect Backup every six months because customers have repeatedly told us they don’t want updates more frequently. However, with a web application, we can update the application far more frequently, and the customer is only aware if there are new features.

For Retrospect Management Console, we’re on deployment #396, so on average, we've shipped 2.5 times per week since Fall 2018.

Owning a hosted service allowed us to integrate and streamline the user experience for Retrospect Backup. In May 2021, we launched Retrospect Backup 18 with deeper integration with Retrospect Management Console.

When you download a trial of Retrospect Backup, the download link points to Retrospect Management Console. The service automatically creates an account for the embedded email address and license and then creates a personalized version of the application installer with license and Retrospect Management Console UUID included. After installation, Retrospect Backup automatically creates a public/private keypair and uploads it to the service, so that the service can bundle it in each client installer for that Retrospect instance.

In Retrospect Backup, the customer can copy a single download link for the Retrospect Backup Client agent installer and send it their entire company. When each employee downloads and installs it, their computer will be automatically added to the Retrospect Backup instance and start getting protected, without any administrative setup.

By leveraging Retrospect Management Console, the Engineering team was able to streamline the entire end-to-end process of trial download to automatic protection for an entire organization, requiring only a couple clicks from the administrator.

Building an MVP and then iterating based on feedback enabled the Retrospect Engineering team to ship an integrated backup service that helps companies easily onboard, protect, and manage their backup environment from a single pane of glass. 

Retrospect, Inc. incorporated ten years ago today: November 1, 2011. Retrospect Backup was originally owned by Dantz Development until Dantz was acquired by EMC in 2004. In 2010, Sonic Solutions acquired Retrospect Backup from EMC and then itself was acquired by Rovi. The leaders of Retrospect approached the management team at Rovi about the idea of spinning off the product, and Rovi agreed to a deal.

Thus, Retrospect, Inc. was born, focused solely on data protection. See our ) for more details. Eight years later, we were acquired by StorCentric. We’re part of a larger organization now, with ways to grow the business and protect more data.

While Retrospect, Inc. has only been around for a decade, Retrospect Backup has been protecting businesses for 32 years now. Retrospect Backup doesn’t collect analytics, but a couple years ago, I did a back-of-the-envelope calculation for how much data we protect: 100PB. In fact, we now have individual customers who have over 1PB protected by Retrospect Backup. For a long list of features we’ve shipped since 2011, see .

I’m extremely grateful to be a part of such a fantastic team and to work on a product that has helped so many recover from data loss. Thanks to everyone who is or has been a part of Retrospect!
 is Retrospect's hosted service where businesses can connect their instances of Retrospect Backup to aggregate, analyze, and manage their backup environments. See  for more details about why we built it.

Here is our current stack for Retrospect Management Console:

* Web framework: Ruby-on-Rails
* Background processing: Sidekiq
* Auto-Scaling: Rails Autoscale
* Authentication: Devise Ruby gem
* Database: Postgres
* Deployment: Salesforce Heroku
* Source control: GitHub Enterprise

Data requests are like ocean waves hitting a service. Ingestion is trivial for small waves, but the larger and more frequent they get, the harder it is to finish ingesting before the next wave. The service might recover for a bit, but then it gets knocked over again with an even larger wave. And of course, you can’t see the waves, so you have no warning.

As Retrospect Management Console has grown in usage, we hit two constraints: request number and request size. There were more requests, and more requests had a lot of data, up to 11MB.

In the beginning, we didn’t know what the problem was beyond Heroku telling us that the service was down. The memory consumption was far too much, and increasing the running instances (dynos) didn’t resolve the problem.

We initially focused on the memory consumption and lowered the maximum allowed amount. That ceiling temporarily resolved the 500 errors, but the service still went down with a big wave and didn’t recover without a couple manual restarts.

After investigating the problem, it was clear that our background processing framework, , was the bottleneck. It ran on background Ruby threads in the same instance and was built for sending the occasional email in the background, not processing millions of large data requests every day. We had chosen it initially to ship the MVP, but it was time to move on.

We considered switching to Amazon SES or a similar cloud service, but we settled on . Sidekiq was created for scalable background processing in Ruby. It ran background instances and handed off jobs to them. In fact, SuckerPunch was written as a lighter version of Sidekiq, so it fit well into our existing workflow.

We switched to Sidekiq in April 2021. The transition was five lines of code, and it immediately resolved our scaling issues in both directions. Since then, the background instances have processed 250 million requests at 1.1 million requests per day, including 20MB requests. 20MB is a lot of JSON backup data. The service has been able to handle 300 requests per second.

In addition to Sidekiq, we needed to automatically scale our web instances and worker instances up and down based on the amount of traffic coming in. If we underprovisioned, we would not be able to process the amount of data coming in. If we overprovisioned, we would be paying too much for the amount of data we needed to process.

Heroku offers a services for web instances, but to cover both types, we chose . Rails Autoscale automatically scales instances based on queue time for requests, and it has a number of options for scaling up and scaling down along with limiting the minimum and maximum number of instances.

It also has a nice usage page to show how much money it's saving you by not overprovisioning.

We've been thrilled by how Retrospect Management Console has grown from a product idea to solve customer problems into a shipping product that has handled 250 million requests since the summer. By monitoring the service and investigating each bottleneck in turn, the service has been able to scale up to meet customer needs, using excellent solutions like Sidekiq and Rails Autoscale.
Let's say you have a computing environment, and you need to protect its contents over time. You need a backup: a versioned snapshot of the systems for point-in-time restores. Any backup solution will be optimized for certain criteria–simplicity, completeness, flexibility, etc–but each one needs to perform backups. Let's walk through the three different types:

* **Full Backup**: This is a complete point-in-time snapshot of a volume. It takes up the most space.
* **Differential Backup**: This is a backup that only depends on one full backup. If you have two differential backups based on one full backup, each differential backup only depends on that one full backup and thus take up more space.
* **Incremental Backup**: This is a backup that depends on a full backup and a number of incremental backups. This is the smallest backup format you can have because it only takes up as much space as necessary to preserve point-in-time restores for each recovery point.

Retrospect Backup's core engine uses forever-incremental file-based backup technology to protect individual files and folders as well as full systems. Forever-incremental backup allows you to only back up file and system changes since the previous backup, minimizing backup time and storage costs.

When you restore, Retrospect will combine these incremental backups into a synthetic full backup, giving you a point-in-time restore for a set of files or a full system.

This customer-centric approach allows businesses to leverage the speed of file-based recovery, restoring a file to the Desktop or to the original location. IT administrators can even restore an entire folder structure based on settings like "Do not replace newer files", allowing IT to recover a large folder structure in place on the affected server or endpoint without determining whether files have been touched more recently than the backup.

Retrospect Backup's goal is to protect your entire environment the way you need it, giving you as much flexibility as possible.

Retrospect Backup leverages a number of components to optimize its forever-incremental file-based backup workflow:

* **System State**: Retrospect includes system state, application state, disk layout, and boot information to support disaster recovery scenarios. To capture an accurate backup of a Windows system, Retrospect utilizes Windows Volume Shadow Copy Service (VSS) to snapshot the system, so that with the other information, it can perform a full disaster recovery (DR), also bare-metal recovery (BMR).
* **Filtering**: Businesses need to be able to include or exclude files based on how critical they are to the business. Music and movies can be excluded while preserving the ability to perform a bare-metal recovery.
* **Destinations**: Retrospect supports a wide range of destinations–disk, NAS, tape, and cloud–and backups can be transferred between different storage locations.
* **Scheduling**: Organizations have their own unique business flow, and Retrospect allows them to schedule their data protection strategies based on their business needs.
* **Scripts**: Retrospect supports different types of actions on data, including backup, replication, archive, and transfers with support for block-level incremental backup (BLIB) and file-level deduplication for backup scenarios. Transfers allow customers to make a copy of a backup in another storage destination, with support for different destination types, filtering, and scheduling. Customers can make one backup of a source and then move it multiple places without touching the source again. Customers can even enable automatic backup healing by using a "Verify" script to auto-heal their backups. If any issues are found, Retrospect will back up the affected files in the next backup. It's a flexible approach that you can apply to each backup set on your own schedule to detect and fix integrity issues automatically.
* **ProactiveAI**: Retrospect's unique ProactiveAI policy-based scheduling enables businesses to intelligently protect their infrastructure based on policy window. Endpoints that go on and off the network will be protected as soon as they join, and with remote backup, remote workers are protected even outside of the office. See  for a technical deep dive.

The combination and deep integration of all of these components form Retrospect's backup stack and provide businesses with a flexible backup solution that extends to all major operating systems and includes application-level backup for Microsoft Exchange and Microsoft SQL Server. Customers can deploy Retrospect wherever it makes the most sense: on a VM, in the cloud, or on an old desktop.

An alternative backup method is image-based backup. Image-based backup uses the blocks of data on a volume to read and protect data at a system level.

Many enterprise data protection solutions use this approach. They focus on the entire volume, copying all used blocks on a volume for a full backup and then subsequent changed blocks for incremental backups. This approach allows customers to restore entire systems, but it loses the file-level granularity of file-based backup.

To allow better flexibility, a number of solutions also support file-level restore. This granular recovery lets businesses quickly choose a point-in-time recovery for a system, launch and attach to it, and then retrieve the individual file that they needed. This workflow enables IT administrators to quickly retrieve a lost file.

However, I don't know of any solutions that integrate file-level restore into the original system along with the ability to leave files that are newer in that folder structure, as Retrospect can.

Businesses use Retrospect's customer-centric approach to data protection to quickly restore individual files, large folders, or entire systems with incremental-forever file-based backup, system state snapshotting, filtering, scheduling, and deep integration with destinations.
Retrospect Backup is designed to be easy to use for non-technical customers, but it has a deep set of features for the technical subset. To understand its capabilities, customers need well-written, up-to-date documentation, in the form of user guides and knowledgebase articles in their language, with targeted content accessible as a URL. Retrospect has been around for 33 years and supports six languages, so achieving that goal is a significant ongoing DocOps/DevOps project.

In the past, we used Adobe FrameMaker for our user guides and a Salesforce Force.com Site for our knowledgebase. Using FrameMaker meant only someone with FrameMaker knowledge could update the user guides, typically an outside contractor. It was difficult to see what changed when updating the content, so the team had to trust spot-checking. Using Force.com meant Support could manage the knowledgebase content, but it wasn’t easy and had no search capabilities at the time. Moreover, language support for both simply multiplied the required tasks.

All these barriers led to fewer documentation updates, detracting from our customer experience and increased Support tickets from confused customers.

To simplify the process of keeping documentation current and adding new information, we updated our documentation DocOps/DevOps pipeline with a set of goals and the following tools:

* **Markup**: Documents need to be readable plain text with markup => *AsciiDoc and Markdown*
* **Source Control and WYSIWYG**: Documents need to be versioned, diffable, and editable by non-technical staff => *Git and GitHub*
* **Languages**: Documents need to be easily localized => *Gengo*
* **Deployment, Search, and Links**: Document generation needs to be automated and deployable with search and deep linking  => *Website, Prince, and Swiftype*

Markup languages like  and  are excellent at embedding sophisticated styles and layouts in plain text. AsciiDoc in particular was designed for generating books from markup, so it works well for both HTML generation and PDF generation. We use AsciiDoc for most parts of our documentation now, both user guides and KB articles, and mix in Markdown documents when it’s easier.

Converting Adobe FrameMaker to AsciiDoc was painful. The Retrospect Backup for Windows User’s Guide was 500 pages long, and the Retrospect Backup for Mac User’s Guide was 300 pages long. Each was localized into six languages: English, German, Spanish, French, Italian, and Japanese. First, we exported from FrameMaker to HTML, but the HTML was very messy. Next, we used many passes of regular expressions in Ruby to clean it up, and finally, we used  to convert from HTML to AsciiDoc.

Compared to the Adobe FrameMaker conversion, we had a far easier time ingesting the SFDC Article pages that populated our Force.com knowledgebase. We used a Ruby gem to read the objects in from the API and write them out to files with bits of Ruby to convert it into Markdown. There were a small subset of KB article that had already been localized, and those were saved as `kb_name.language.adoc` (i.e. `error_1101.ja.adoc`).

With all of our documentation now in a plain text format, we could easily add, edit, delete, and localize it.

 translates content within a few hours at low price point. We use them for all of our localization needs. Compared to our previous service, it has saved us thousands of dollars and months of waiting. Getting a string translated for a couple dollars in an hour by a professional sounded absurd to us before we discovered Gengo.

We switched to Git and GitHub almost a decade ago: . By adding our documentation to Git, we could easily version the content and diff any changes. We now have 402 KB articles in Git, and our user guides total 104k lines. When we recently added a new chapter in the user’s guide, it only took 10 minutes and became a GitHub pull request to be diff’d and reviewed. When Support writes up a new KB article, Engineering can add to the production website in about 10 minutes.

Moreover, GitHub provides an excellent WYSIWYG editor for AsciiDoc and Markdown files, with rich styles and preview available.

Using Git and GitHub allows us to integrate our documentation into our existing build and deployment pipeline while providing editor abilities to non-technical staff.

The documentation is now in plain text and versioned, but we still need to deploy it with search and links. Most companies use Wordpress for their websites, but we use Ruby-on-Rails. The tradeoff is extensibility. Ruby-on-Rails is a less common choice, so fewer web developers have experience with it compared to Wordpress. However, Rails lets us quickly build new functionality: .

For the knowledgebase, we built a new page with categories and links to all of the articles. To populate it, we wrote a simple Rails initializer to read in all of the articles in the directory and store their frontmatter in memory to be used later by the KB index page.

The user’s guides took more effort. We could easily build a section of the website for Documentation. The trick was handling the chapters well. We wanted to enable Support to link directly to specific sections of chapters without loading the entirety of the user’s guide in a single page. We manually build the chapters in the guide to allow custom sorting, but we dynamically build the subchapters by reading and parsing the AsciiDoc files within Rails. With both, we can display the any chapter along with a sidebar of the overall structure of the guide for context.

It took some time to work out the correct AsciiDoc syntax to integrate well with Rails via . For instance, every chapter needed the following frontmatter:

-
Containerization has been around for decades since chroot, but the technology has become ubiquitous as large tech companies have invested in platforms and tools to make containers easier to use and scale with tools like Kubernetes. While I’d been using containers with Heroku for years, I didn’t realize how pervasive they were until I read an  in 2016 about Google's Borg, Omega, and Kubernetes, especially this section:

> “The isolation and dependency minimization provided by containers have proved quite effective at Google, and the container has become the sole runnable entity supported by the Google infrastructure.”

Containers are how Google can scale services, by abstracting away the physical infrastructure and building orchestration tools to manage them, and they fit into a broader context with the rise of DevOps, handing Engineering an excellent building block for creating software delivery pipelines and deploying applications.

Containers are one level of abstraction up from virtual machines. Virtual machines (VMs) allow you to run an operating system in an isolated environment in a server, virtualizing the hardware, but containers allow you to run a single application within an isolated environment, virtualizing the operating system. Compared to virtual machines, containers are smaller, start faster, and consume fewer resources, so you can run more on a single physical hardware system while maintaining isolation between them.

You’ll find many diagrams like this one online, but it’s an oversimplification. Containers are so small and fast because they rely on the host OS’s kernel, but they actually need their own minimal guest OS as well to handle tasks like package management, configuration, and other miscellaneous OS features. The application's libraries and dependencies as well as the small guest OS are all wrapped up in “Libs”. As an example, the Alpine Linux OS is 5MB, and applications running on it start in less than a second.

 made containers accessible and has become very popular since its launch in 2013 with support for Windows, Mac, and Linux. Docker containers are instances of a Docker image, and each image is a stack of immutable layers, based on the commands of your `Dockerfile`. This layered immutability has multiple benefits, allowing you to only change the image based on new or changed commands and to store layers separately in Docker Hub, the hosted service for distributing public Docker images. Clients can download layers concurrently for performance.

Before the transition to Docker, our web app toolchain was a local macOS environment for development,  for source control, and  for deployment. Retrospect Engineering has used Heroku for web app deployment since 2011, starting with , our Ruby-on-Rails website, and then with , our hosted service for backup analytics. Heroku, now owned by Salesforce, has been a fantastic deployment target. Deploying a new version is as simple as pushing to a Git repository.

Instead of installing and managing upgrades for Ruby/Rails/Gems/Postgres/Redis on macOS, we manage them all within a Docker container running on our Mac development computers. We finally decided to migrate from a local macOS environment to Docker because of upgrade issues. Every Ruby/Rails/gem/macOS version update hit installation issues which we only resolved after finding a blog post for our specific combination of versions. Upgrades were painful enough that we postponed them, a sure sign that we needed a deeper toolchain upgrade.

Docker provided that next-generation environment, where upgrading a Ruby version was as simple as changing the base Docker image that we depend on. Every upgrade starts with a clean slate in the image, so there are far fewer conflicts between different versions or different packages. Using containers for development work increased the Engineering team’s efficiency.

The website is a Ruby-on-Rails environment with only a SQLite dependency for local development, so we use a simple Docker configuration file (`Dockerfile`). Here is our short file for the website:

Let’s walk through each chunk:
* **Base Image**: We use the official Ruby Alpine Linux image as the base image. Common tools like Ruby publish official images to Docker Hub.
* **APK**: Alpine Package Keeper (APK) is an extremely lightweight package manager. One of the reasons we need a base image inside the container is for the package management functionality, so that we can download the necessary libraries for our application’s environment.
* **Gemfiles**: We create the directory for our application, copy in the Gemfiles, run `bundle install` to install the required libraries, and finally remove both Gemfiles. This image is specifically for local development, so we want to bind mount to the host's application folder and not depend on any application code within the container.
* **Gemfile.lock**: We actually created a separate `Dockerfile` for running `bundle update` because it’s such an infrequent task to generate a new `Gemfile.lock`, and we wanted it to be an intentional action.
* **Rails Server**: We run the `rails server` command to start the application.

From Terminal, we use the `Dockerfile` to build the image. Then, to run an instance of the image, we use a bind mount, reference a Docker environment file, and  include `-it` to allow interactive debugging with the `pry-byebug` gem.

With that, we have the website running in a Docker container with the following features:

* **Local Development**: It’s accessible at `http://localhost:3000`.
* **Live Editing**: The bind mount means the container’s app folder references the app folder on the host under source control, so that we can do live editing and commit our changes as if the app was not running in a container.
* **Live Debugging**: The `-it` parameter enables `tty` mode for live interaction, which means we can use `debugger` for breakpointing.

You can also override the default command by passing a command at the end. For instance, to run `rails console`, use:

Instead of typing the whole line every time, we set up a shell alias in `~/.bash_profile`:

Retrospect Management Console is a Ruby-on-Rails web application that has two dependencies: Postgres and Redis. We needed all three services running and able to communicate with each other. This configuration required Docker Compose to handle the multiple microservices.

With Docker Compose, we can set up a `docker-compose.yml` file to describe the containers, dependencies, volumes, and configurations. Docker Compose even supports health checks to periodically verify that a service is operating correctly. 

For our `docker-entry.sh` file, we added logic to either run any pending Rails migrations if there is a database or re-create the database if it did not exist:

As with the website, we configured Docker to use the host system’s web application folder under source control via a bind mount.

To launch the containers, first build the images then bring the setup up.

Live debugging is one extra step. We needed `stdin_open: true` and `tty: true` entries under the `console` container in `docker-compose.yml`. Then, we needed to add `debugger` in the Rails app where we wanted to breakpoint. Finally, we needed to attach to the container after it launched.

To detach, type the escape sequence Ctrl+P then Ctrl+Q.

Since the Postgres database is now run in a container, we needed to modify our `psql` to access it. Fortunately, Docker makes it easy:

Finally, here are a couple more handy Docker commands:


Automation testing is significantly more complicated with Docker. You can’t just drive a browser because there is no screen. We needed to update our Rails Rspec and Capybara setup to use a headless environment for the integration tests. After testing different options, we settled on Selenium Grid with a Google Chrome container using Docker Compose, based on .

With this environment, RSpec can run through our 711 tests for the website and generate screenshots on the failures, covering the site and the embedded partner and support portal.

Here is the `docker-compose.test.yml` file for reference:

Here is a snippet from `rails_helper.rb` that works for us:

Getting the setup exactly right was tricky, but this is the code that works for us. We frequently referred to the  site as well.

The proof that the toolchain upgrade was a success came when we rolled out Docker to the wider web development team. Previously, setting up Ruby, Rails, and all of the gems was a multi-hour process with frequent web searches to fix environment issues. With Docker, the developers downloaded Docker, built the image with one Terminal command, and launched the multi-container application with a second command. It took five minutes.

The rise of ransomware, and ransomware-as-a-service more recently, poses a huge threat to businesses around the world with a projected $20B payout in 2021, a 100% year-over-year increase for the last four years. Ransomware is now a vast ecosystem with many different forms of attacks. Many attackers have their own versions of ransomware, and these are called variants. Each variant has the same purpose, but it uses a different mechanism or simply a different naming convention. The majority of ransomware variants and all of the top 10 forms for 2021 followed the same attack pattern: infiltrate a computer, encrypt its files, and then rename the files with a different extension. Businesses need to detect ransomware as early as possible to stop the threat and remediate those resources.

 in Retrospect Backup 18.5, launched last month, identifies changes in an environment that warrant the attention of the IT team. Administrators can tailor anomaly detection to their business's specific systems using customizable filtering and thresholds for each of their backup policies, and those anomalies are aggregated on Retrospect Management Console across the entire business's Retrospect Backup instances or a partner's client base with a notification area for responding to those anomalies.

Retrospect Backup detects all of the major ransomware variants using an algorithm that focuses on file metadata anomalies for behavior-based monitoring. According to Coveware, the top variants are always changing, with over 50% changing every quarter. The key to detection is combining technologies such as signature detection in processes with file-based irregularities. Using a multi-pronged defense, with immutable backups, anomaly detection, and other security layers, businesses will know when they're being attacked and will have the tools to remediate it and move on.

As a data protection solution, Retrospect Backup has a significant footprint in a business’s computer environment with visibility into endpoints, servers, NAS volumes, and even cloud storage. To detect anomalies, Retrospect Backup provides a per-policy option for filtering and threshold to decide whether or not certain file changes are an anomaly with options for notifications. Let’s walk through each:

* **Filtering**: Configure a filter to identify the files to observe. Retrospect lets administrators tailor this to file types, paths, dates, or specific attributes, and the built-in filter focuses on office documents, photos, and movies.
* **Threshold**: Set the threshold for the alert. If the percentage of files new or changed out of the total number of files matched by the filter is greater or equal to the threshold, Retrospect will create an anomaly event.
* **Notification**: Access notifications on Retrospect Management Console, receive them immediately in an email, and find them in the Execution History and Backup Report. Retrospect surfaces the notification for anomaly detection in the best place for an organization.

The diagram shows the volume being monitored as a whole, the subset of files that match the "Anomaly Detection" filter, and the files that are new or changed within that subset. Retrospect generates an alert if the percentage exceeds the threshold.

Customers can simply enable “Anomaly Detection” in each policy, select the appropriate filter, and assign a threshold. Retrospect takes care of the rest.

When an anomaly is detected, Retrospect can generate an email notification and surface the anomaly in the application and on Retrospect Management Console. It also provides API integration for businesses to tailor the workflow to their processes using Script Hooks, so the organizations can take further actions, like posting to a Slack channel or even stopping the backup until the problem is assessed.

Congratulations to the Retrospect Engineering team for releasing such a huge update! Retrospect Backup 18 included immutable backups for ransomware protection, and with this free update, we're extending that ransomware focus to anomaly detection.
More than a decade ago, Retrospect Engineering set up a daily build system for our Windows and Mac products. Once a day, a cronjob would kick off a task to build the latest version of our codebase and email the results to the team. The daily build process was a fantastic method for adding automation to the build process for having a repeatable build process, ensuring the main codebase compiled in production, and storing builds that we could use to narrow down any issue that arose during regression testing.

Since then, continuous integration (CI) workflows have become a common practice for Engineering teams. Continuous integration (CI) and continuous delivery (CD) are both processes to automate building, testing, and in the case of delivery, packaging and deploying a product to the final platform for customers. This DevOps automation creates a pipeline from codebase to customers, eliminating human bottlenecks and manual steps to increase the efficiency and performance of an engineering team.

](https://www.devops-research.com/research.html) group studied thousands of teams to understand the practices and metrics for high-performing teams vs low-performing teams, and continuous delivery is a fundamental building block for high-performing teams. By relying on an automation pipeline, engineering teams can ship more features faster and better tested while reducing burnout.

Recently, we decided it was time to upgrade our infrastructure to a continuous delivery workflow to improve our efficiency as an Engineering team. Let’s walk through the original homegrown build system, what our overall goals for automation were, and how we extended the homegrown build system into a continuous delivery pipeline with Jenkins to improve our delivery performance.

Since the mid-2000s, Retrospect has had a daily build system for Windows and Mac and a manual build for Linux.

On Windows, we’ve used Visual Build Pro to generate builds. Visual Build Pro checked out the source code from GitHub Enterprise, compiled the appropriate project, posted the build output to a NAS share, and emailed the Engineering team with the results. When we added automated testing, we updated Visual Build Pro to kick that off as well.

On Mac, we wrote our own build system. The first version was a shell script, run daily as a cron job. That lasted a couple years until we rewrote it in Ruby. The Ruby version has endured quite well. In the intervening years, we’ve added support for building different versions of the Mac application, running regressions with our automated test framework, signing them, notarizing them, and uploading the final artifacts to Amazon S3. The Ruby build script is an end-to-end build process for our Mac product.

On Linux, we have never made changes to the client agent frequently enough to justify automating the build process. We built it manually for every change.

Our homegrown system had elements of continuous integration, but there were many manual steps. Let’s walk through what we needed from a continuous delivery pipeline:

* **Continuous Delivery**: We wanted a single system that supported an end-to-end pipeline, starting when a pull request was merged, building, testing, packaging, deploying to Amazon S3 for download by customers, and notifying the Engineering team of the results.
* **On-Premise**: We use GitHub Enterprise, so we needed a system that could run on-premise and control the local testing infrastructure.
* **Cross Platform**: Retrospect has Windows, Mac, and Linux elements to build as well as cross-platform dependencies. We need to build a Mac artifact and then transfer it to Windows and vice versa. Moreover, we needed to check out our codebase at a single consistent point across platforms.
* **Build Commit with Status**: As a team, we are accustomed to having a build commit to master/main on GitHub that identifies the exact bits that were compiled and shipped, and the build commits keep version numbers and cross-platform artifacts up-to-date within the codebase. We wanted to consolidate the two commits that we currently had into a single commit and mark it with a status from the CD pipeline.

Our goal was to leverage our current build processes, knit them together into a single build pipeline, and extend that into continuous delivery.

Building a homegrown system has the fantastic advantage of doing exactly what it was coded for and the unfortunate downside of doing nothing else. The internal team pushes the functionality forward. We needed to leverage a third-party solution to achieve our CD goals with minimal commitment.

We looked at Jenkins, GoCD, Bamboo, CircleCI, and GitHub Actions; Jenkins was open source, worked on-premise, and seemed to be popular enough that we could google for help. Jenkins was designed to be a CI system with a plugin architecture, and it had thousands of plugins. We thought it made sense to try it first.

Overall, the setup process took about three weeks of one engineer’s time, spread out over three month. We have nine stages in our pipeline using three nodes (2 Windows, 1 Mac) to perform the following steps through a versioned Groovy script.

* **Auto Trigger**: We wanted the new system to start a new build when someone merged a pull request to master. As we use GitHub, its Webhooks were the natural choice. We originally set up GitHub Webhooks to only send Jenkins data on pull request changes, but because it didn’t actually include the master commit, Jenkins didn’t trigger the build. We had to send the push event from GitHub for Jenkins Git plugin to notify our pipeline. However, because the pipeline was triggered by pushes, the later build commit auto-triggered a new build, creating a feedback loop. We had to add special code to detect and suppress builds based on the build commit.
* **Delay**: We set a 30-minute delay before the pipeline starts in case an engineer merges several pull requests in quick succession (which did happen to us in practice).
* **Code Checkout**: We have two platforms that we’re building on, and we needed the codebase to be in the same state on both to avoid the race condition of building different commits on different platforms. We reduced this race condition from 45 minutes to 15 seconds by separating out the checkout stages and moving them to the beginning. We did experiment with checking out a specific Git hash, but that approach ran into issues when we tried to push a new build commit back to master.
* **Build Dependencies**: Retrospect for Windows includes a Mac binary and vice versa. The build pipeline needed to build the dependencies on both platforms and then copy them to the other platform via an Amazon S3 artifact staging folder.
* **Commit Version and Dependencies**: The team was accustomed to the version of the product and the above dependencies that they use in development being updated by the build system. In the past, we had the separate build systems commit these, but that approach would re-introduce the race condition from above. If one platform committed to master, the other platform would need to fetch that change or its push would fail, but because it was fetching a change, it could fetch too much. Either build commit push would fail if someone had merged another pull request, but we were trying to avoid silent bugs, rather than the build process failing. We consolidated the two platform commits into one by transferring all of the updated files from Windows onto Mac via the S3 artifact staging folder and then committing all of them.
* **Build Applications**: Each platform node would build Retrospect client agent and engine application as before, using the previous build systems.
* **Package Applications**: Jenkins was quite handy for creating the folder structure and moving files around for packaging. On Mac, we did need to code sign and notarize the binaries. These steps used to be manual, and the team really appreciated automating them.
* **Deploy Applications**: Our deployment process is quite straight-forward: upload them to S3 and mark as public.
* **Test Applications**: On Mac, the original build system continued to run automated testing on the finalized product. On Windows, Jenkins downloaded the finished binaries to a new node where it built the test framework and then ran it against the new binaries. Our tests are quite extensive, as we make backup software, so the tests actually extended the build pipeline from two hours to nine hours.
* **Email Team**: Jenkins sent an email at the end with the status of the build and linked (if successful) to the staged final products.

It's a long list, but it ticks off all of our requirements. Jenkins provides us with an infrastructure for managing multiple build systems on different platforms as nodes in a single pipeline, and its Groovy script support enables us to knit those build systems together into a DevOps process. 

The resulting Groovy script is 500 lines, and that count excludes the 2k-line Ruby file for building Mac and the extensive Visual Build Pro script on Windows. The option to pull the Groovy script from source code (GitHub Enterprise in our case) highlights the DevOps mindset of repeatability and automation.

While the team is really happy that Jenkins is working now, the setup process was not smooth. It took a significant amount of time to understand what Jenkins could provide us, how to connect what we needed into what it could deliver, and then codify that process in Groovy. Beyond that expertise barrier, Jenkins has bugs, like all software. We hit a number of issues where we eventually found open JIRA issues without resolution, so we worked around them.

CI/CD workflows are a necessary part of a high-performing Engineering team. While we had a good system for building and testing the software, we were missing the full continuous delivery pipeline. Like all Engineering projects, there was an opportunity cost associated with the infrastructure upgrade, but resolving this technical debt allows the teamn to focus on features rather than manual processes.

Thanks to the engineer who took on this project and overcame the hurdles we found in Jenkins to wrap up this project and to the entire team for helping out when needed! Moving to cross-platform continuous integration and continuous delivery had been a goal for years, so a huge shout-out to the Retrospect Engineering team for ticking it off this summer.

My friend pointed out an excellent article by Peter Norvig titled . Norvig is an Education Fellow at Stanford Institute for Human-Centered AI, co-authored “Artificial Intelligence: A Modern Approach”, the most popular AI textbook, and served as a director of research and search quality at Google.

This is Sudoku: fill in every blank square with a number between 1 and 9 such that every row, every column, and every 3x3 group has every digit.

I was a complete novice to Sudoku when I started this project. I had casually played it before, but my naive mental algorithm was a mix of constraint propagation and search, without any systematic method to it. In his article, Norvig walked through how to use constraint propagation and backtracking search to quickly solve any valid Sudoku puzzle. His Python version was able to do so in one second for the world’s hardest puzzles.

I skimmed Norvig’s general approach (summarized below) and then attempted to fill in the details myself with a Ruby implementation. The process took a couple iterations and pushed me to understand the problem space far better than I would have simply reading the article and Norvig’s Python implementation.

Let’s walk through the different challenges.

Sudoku has 10^21 potential solutions, and the minimum number of supplied values is 17. A researcher actually spent a year proving that no 16-value Sudoku puzzles existed, according to . Fittingly, Norvig’s example puzzle (pictured above) has 17 values.

Each square in the 9x9 grid can have a value between 1 and 9. These squares are grouped into "units": columns, rows, and 3x3 groups. Each unit has nine squares (peers), so each unit has one of every digit. If a unit already has a value assigned, like "4" in the first square (A1 in Sudoku parlance), then no other square can contain that value. By converting the grid that we see into representation of known and potential values (`A1->4, A2->12356789...`), we can evaluate each unit according to a set of constraints.

There are two constraints that we can use to reduce the space of possibilities:

* *Elimination*: If a square has only one value, eliminate that value from all other peers.
* *Assignment*: If a square is the only peer with a certain value, that value must belong to that square.

For every elimination, we can attempt an assignment, and for every assignment, we can attempt further eliminations, recursively narrowing the potential value space for the puzzle.

In practice, easy and medium difficulty Sudoku puzzles can be solved without search by simply running eliminations and assignments recursively.

Constraint propagation only gets us so far. After we’ve limited the potential values using eliminations and assignments, we need to search the problem space: choose a possible value for a square and see if it’s correct. A backtracking search leverages constraints to prune a given branch and backtrack as soon as the remaining possibilities on that branch are not valid. I used a backtracking depth-first search in my implementation.

Norvig wrote his implementation in Python. I wrote mine in Ruby. They’re both great, high-level languages, but they have important differences that only manifested when I tested difficult puzzles.

In particular, Python has lazy evaluation of generator functions. In `some()`, the expression `e` is not evaluated until it’s needed, so the first time it succeeds, it returns and does not evaluate the rest.

Ruby does not have lazy evaluation, so the function needs to check an instance variable instead to break out of the search.

Norvig picked an excellent puzzle as the example because there are only 17 initial numbers, compared to 36 in easy puzzles and 22 in hard ones. Moreover, the puzzle seems designed to trip up naive search algorithms. My early iterations failed on his example because squares with the smallest number of potential values needed the larger number assigned, and my code guessed the lowest one and could never recover after that. It was a great example that ensured I didn’t prematurely stop improving the algorithm.

However, only testing with one puzzle is not a good QA practice, so I built out a test framework with 60 puzzles of varying difficulty to ensure my implementation worked on a variety of puzzles beyond Norvig's example. To ease debugging, I added two functions for displaying the current state of the puzzle grid in Terminal: one for the actual values and one for the potential values. The visual aid helped debug issues with the search algorithm.


After I finished debugging my version, I ported Norvig’s Python implementation into Ruby to see how they compared. Norvig’s algorithm was vastly faster than mine was: from 3x faster (0.004 seconds vs 0.01 seconds) for easy puzzles to 178x faster for the hardest puzzle available (): 0.2 seconds vs 34 seconds.

The implementations were close for puzzles that only required constraint propagation, but mine was much slower at search for two reasons:

* *Overzealous Constraint Propagation*: My version wasted a tremendous amount of time performing constraint propagation on every square during every search, rather than focusing on the square being tested and its affected peers in each unit.
* *Wasted Data Conversions*: My version passed each grid to be tested as a string to the next search, forcing each run to recreate the potential values of each square in the grid rather than leaving the internal representation as a set of known/potential values for each square.

I preserved my Ruby approach along with my Ruby port of the Python version in a GitHub repo for comparison along with the `sudoku.csv` test framework and results: .

As an intersection between Product Management and Engineering, I thought productizing Norvig's algorithm into an iPhone app written in Swift would be a fun extension to this side project. Read more about the process in .

Before I started this side project, I had tried Sudoku a handful of times. Then, my friend sent me this excellent article by Peter Norvig titled , and I wrote about the algorithm in . Productizing that algorithm into an iPhone app seemed like a fun reason to learn Swift.

My original vision for this app was one screen: a camera view that automatically recognized a Sudoku puzzle, highlighted the numbers with Apple’s yellow box (like QR codes get), and solve it on the fly, superimposing the calculated numbers over the image in real time. I thought I could learn Swift, SwiftUI, and iOS’s image recognition. This is not that app.

Brainstorming is a fantastic avenue to thinking about what’s possible, but all projects, especially side projects like this one, are constrained by time and resources. SudokuAI is a far simpler app. It does have image recognition, but the process doesn’t involve floating yellow boxes. It solves puzzles in real time, but the result isn’t superimposed on a real-time image. SudokuAI also covers a couple more use cases than the original version did: creating a new Sudoku puzzle, using an image from Photos, or getting just a hint.

Most importantly, SudokuAI is shipping on the App Store now. It’s the most important aspect because without shipping, people don’t have the opportunity to try the app.

Shipping a product highlights the tension between Product Management and Engineering. Product Management wants to deliver the best experience to the customer, but Engineering is tasked with implementing the experience. Features that sound simple or fun to customers can translate into untenable requests for Engineering, like floating yellow boxes with superimposed numbers for someone who has never written Swift working on a side project. Delivering a new product requires understanding the needs of customers in addition to the resources of the team and finding the best intersection between them.

I iterated many times on the design both to simplify the interaction and to enable me to implement it.

My original vision for the app was derailed as soon as I looked into Apple’s documentation for image recognition. I thought that those yellow rectangles might be part of an API for VisionKit. They were not, and I wasn’t interested in building them. Moreover, the original version didn’t account for creating new puzzles, importing digital photos of puzzles, or getting a hint.

I pivoted to having a list of puzzles. The user could try a pre-loaded puzzle, create a new one, or import one from the camera or photo roll, and they could watch autoplay, get a hint, or solve it instantly. That list of use cases seemed reasonable and achievable. Still, those use cases could be surfaced with many different user interfaces.

I settled on buttons along the top of the list view for puzzle ingestion: plus, camera, photo. Toolbar items like these are a standard iOS style.

The puzzle interaction workflows were far harder to get right. I started with the music player analogy: back, play/pause, next.

Interacting with them wasn’t a great experience though. “Back” should always be a reset for the puzzle. “Next” doesn’t cover both use cases: hint and solve. I abandoned the music analogy and switched to words: “Solve”, “Hint”, and “AI” (for autoplay). Tapping “AI” switched the buttons to “Pause AI” and “Reset AI”.

That interaction felt awkward. I iterated on the words with “Solve” and “AI”. Tapping “Solve” switched to “Answer” and “Hint” with “Reset”. Tapping “AI” switched to “Reset” and “Pause” with a play rate slider. The slider would allow users to slow down or speed up autoplay, seeing what choices the algorithm made to proceed through the problem space.

That workflow was far too awkward. This app was supposed to be simple. I returned to the music player analogy with a small tweak: “Reset” (Back), “Play”/“Pause”, “Hint” (Next), “Solve” (End). 

These buttons addressed the use cases and were easy to implement, and they translated well to the high-level pitch for the app:

* *Solve Sudoku Instantly*
* *Tap for a Hint*
* *How to Solve Any Puzzle*

I got tired of adding puzzles manually into the app and added a toolbar item for importing a default set of puzzles: Norvig’s example along with the three hardest puzzles I could find.

* 
* 
* 
* 

The final addition to the app was the tutorial. The first question during beta testing was what exactly the app did, so I added the tutorial and a question mark button to bring up the Help section any time.

SudokuAI is not a complicated or flashy app, but it achieves what the goals were.

I’ve used Objective-C, Cocoa, and Interface Builder (NIBs/XIBs) for over a decade, writing 50k lines of code to create the user interface for Retrospect Backup for Mac, but Apple’s moved on to Swift. I had zero knowledge of Swift when I started this app.

The initial problem was learning enough Swift to implement the algorithm, but productizing R&D doesn’t just mean wrapping a UI around some research. Features like “Tap for a Hint” meant deciding how to store and surface the intermediate steps of the algorithm, and the “Play”/“Pause” buttons required the algorithm slow down and occasionally stop at the user’s request. Adding the plumbing for those user-facing features required extending the original algorithm to support new use cases.

Next, when should the app actually solve a puzzle? When the user hits “Solve”? When it’s created? The design choice depends on the engineering capabilities. Luckily, solving any known puzzle takes 1.2 seconds at maximum, about 1400 steps in the algorithm. Based on that, I created an upper bound, declaring “No Solution” if the algorithm exceeds 2000 steps in its search. Because the time was short enough, I opted to solve a puzzle for every change. By always having a solution or failing after a certain number of tries, the app could then display if there was a solution, providing great feedback to users who are editing a puzzle.

The UI is built in SwiftUI. SwiftUI is drastically different working with XIBs in Interface Builder. In fact, Apple EOL’d Interface Builder in 2011 and rolled it into Xcode before replacing it with SwiftUI. SwiftUI is declarative code (as opposed to imperative code), meaning you can write what you want, not exactly how you want it. The following code becomes a list with `PuzzleCell` as the table view’s cells and “SudokuAI” as the title.

With SwiftUI, I could easily write UI code (with only a few conditional statements) that worked on any iPhone and iPad on both portrait and landscape. Coming from Interface Builder, the experience was both transformative and incredibly frustrating: amazing because very little code can become a functioning UI and annoying because that UI might not be exactly what I wanted. Learning SwiftUI informed the app’s design by seeing what was easy and shippable.

Beyond solving the puzzle, the app needed to ingest puzzles, either by user creation or by import and image recognition. The goal was to do a reasonable job of recognizing a Sudoku puzzle in an image and extracting the numbers from it. `VNRecognizeTextRequest` in Apple’s VisionKit was a great API to accomplish this with minimal effort, but it took a number of tries to find the correct parameters. Thanks to  for making attribute testing easy for sample puzzle images.

For a given image, iOS’s VisionKit API provided a list of character guesses with their location in the image. The app needed to estimate the spacing between the recognized numbers, including where the puzzle started and stopped within the image, so the code first guesses the surrounding box size and then assigns each character to a square, adding blanks for spaces that contain no number. The algorithm works well if the user crops the image to only include the puzzle. To help the user correct any errors, I overlay the recognized puzzle on top of the image, so that the user can tap on each square to correct the number. The app uses the same workflow to create a new puzzle.

With camera support, photo roll integration, and puzzle creation, SudokuAI gives users a couple different workflows for ingesting puzzles. Solving puzzles instantly on any change allows the app to display a status like “Ready to Solve” or “44 hints left”. Letting users tap to change each square ensures any image recognition errors can be easily fixed. SwiftUI enables me to bundle these features into a simple UI.

 is available for free from the . It is compatible with iOS 15 and later on iPhone and iPad. The source code is available on GitHub at  under MIT license.

Despite its name, the underlying algorithm is simply constraint propagation and search, neither of which is AI (artificial intelligence) or ML (machine learning). SudokuAI just seemed like a catchy name.

  

ChatGPT and Midjourney are both examples of generative AI, a form of machine learning that lets users create artwork based on specific inputs, such as text or images. Reading about the possiblities of this technology doesn’t have the same impact as personally trying it. It’s amazing.

I first learned about Midjourney from Stratechery’s interview with its founder, David Holz, creator of LeapMagic. I had no idea that Midjourney was Discord’s most popular server or that Midjourney is one of the biggest consumers of GPUs in the cloud. The images are amazing. Type in text, and out comes a professional image.

Midjourney, released in July 2022, creates images in 60 seconds, solely through Discord. In the Stratechery interview, Holz talks about how the team was using Discord as an easy way to test the software and then realized how much fun it was, so they decided to make the tool available only through Discord. Midjourney uses a freemium business model, with a couple dozen free images and paid tiers.

OpenAI’s ChatGPT, released in November 2022, is also remarkable. It's a text chatbot, based on the GPT-3 large language model (LLM), and can respond to an impressive variety of questions. Ask it for an essay comparing communism and capitalism, and the essay is ready in less than a minute. Ask for a Python script to ingest a CSV file. Done. Ask for a sonnet about why ChatGPT is at capacity. It displays that on its status page.

Both services take text as an input, referred to as a prompt, and run it through an advanced machine learning model to output a generative AI result. To explore the two services, I thought it would be fun to create children's books, writing a prompt for ChatGPT so that the output could be fed directly into Midjourney for story artwork. I did one pass myself and then another with my family, and we printed all of them on Shutterfly as 6x6 Instant Books for $4 each.

Prompts are a very new concept, but there is already a term for iterating on them for better results: prompt engineering. My goal was to create a prompt for ChatGPT that would generate a story, and the story sentences would serve as the prompts for artwork in Midjourney. I went with two family favorites: birds and pandas.

> ChatGPT: “Write a seven-sentence children’s story titled “Falcon Finds Home” about a peregrine falcon that wakes up in a box, escapes, and flies home above the forest and stream to find its family and rainbow-colored nest, with each sentence written as a prompt.”
1. The peregrine falcon woke up inside a small box, feeling confused and alone.
2. ItThe falcon quickly realized it needed to escape from the box and began looking for a way out.
3. Using its powerful wings, the falcon broke through the top of the box and flew into the open air.
4. ItThe falcon soared above the forest, the trees a blur of green beneath it.
5. The falcon followed the babbling stream, knowing it would lead it home.
6. As Itthe falcon flew, a rainbow appeared in the sky, guiding the falcon towards its destination.
7. Finally, the falcon saw its colorful nest built high in a tree, and it knew it was home.

ChatGPT wrote the below story within ten seconds, including the numbering. I made slight alterations to the text to keep the context of the story for Midjourney in every line.

It's a story and a bit unnverving. I came up with the idea in thirty seconds, and I was able to create all of the assets for a book in around ten minutes, lay it out in Shutterfly in ten minutes, and ship it in a week for $20.

> ChatGPT: “Write a children’s story titled “Great Wall of Pandas” in seven sentences about a family of panda that must journey over the Great Wall of China to make it home to their bamboo forest.”

> The Great Wall of Pandas is a story about a family of pandas who live in a bamboo forest in China. One day, the mother panda, Peaches, and the father panda, Mr. Panda, decide to take their baby panda, Bao, on a journey to see the Great Wall of China. As they walk along the Great Wall, Bao the Panda asks his parents many questions about the history of the wall and how it was built to protect the country. Bao the Panda is so fascinated by the Great Wall that he wants to climb to the top, so with the help of his parents, he does just that. From the top of the wall, Bao the Panda sees a beautiful view of the country below. When it's time to go home, the family of pandas has a great journey back to their bamboo forest. Bao the Panda can't wait to tell all of his friends about his adventure and is excited for more adventures with his family.

Connecting ChatGPT to Midjourney to create visual stories from prompts creates an AI-driven creativity pipeline. With prompt engineering, one person can fuse words and art into a story in minutes instead of months.

My family was uninterested in ChatGPT, but Midjourney was magical. Type in words, and out comes an image. Visualizing ideas was far more interesting for the family than getting the computer to generate a story. They wanted to write their own stories and use Midjourney for the artwork.

They wrote two books, one about a fox and a panda and one about birds, and then we laid out the stories with the artwork from Midjourney on Shutterfly. In fact, printing 6x6 books for $4 with Shutterfly was as much of a hit as Midjourney.

ChatGPT and Midjourney aren't perfect. I generated many images with Midjourney where the panda had ten toes. ChatGPT repeatedly ignored my requests to use the name in each sentence or how to incorporate styles like "cinematic" or "sketch". Still, these are the state of what's available now. Those shortcomings will be resolved soon.
