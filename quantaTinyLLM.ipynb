{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantaTinyLLM Maker\n",
    "This notebook trains a custom character-level GPT model.\n",
    "\n",
    "### Instructions:\n",
    "1.  **Configure your settings** in the next cell.\n",
    "2.  **Run the Configuration cell.**\n",
    "3.  **Run the Training cell.** This will mount your Google Drive, load your data, and start training.\n",
    "4.  Once training is complete, a `checkpoint.pth` file will be saved to your Colab environment.\n",
    "5.  You can then run the **Inference cell** to see your model generate text, or download the checkpoint to use with `inference.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. CONFIGURATION ===\n",
    "# --- Dataset Settings ---\n",
    "# This path points to your training data file within your Google Drive.\n",
    "DATASET_PATH = \"/content/drive/MyDrive/MyLLM/training_dataset.txt\"\n",
    "CHECKPOINT_NAME = 'quanta_checkpoint.pth'\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "# Adjust these to change the size and architecture of your model.\n",
    "N_EMBD = 128       # The embedding dimension for each token.\n",
    "N_HEAD = 4         # The number of attention heads.\n",
    "N_LAYER = 12       # The number of transformer blocks (layers).\n",
    "BLOCK_SIZE = 128   # The maximum context length for predictions.\n",
    "DROPOUT = 0.1      # The dropout rate for regularization.\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "MAX_ITERS = 2000         # Total training iterations.\n",
    "LEARNING_RATE = 1e-3   # The learning rate for the optimizer.\n",
    "BATCH_SIZE = 32        # How many sequences to process in parallel.\n",
    "EVAL_INTERVAL = 100    # How often to evaluate the model and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. TRAINING ===\n",
    "# This cell contains all the code to define and train the model.\n",
    "\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "\n",
    "# --- Setup ---\n",
    "torch.manual_seed(1337)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_steps = []\n",
    "loss_values = []\n",
    "\n",
    "# --- Data Loading ---\n",
    "drive.mount('/content/drive')\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# --- Tokenizer ---\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# --- Data Splitting ---\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# --- Model Definition ---\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.sa = nn.MultiheadAttention(n_embd, n_head, bias=False, device=device, batch_first=True)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        y, _ = self.sa(y, y, y, attn_mask=None, need_weights=False, is_causal=True)\n",
    "        x = x + y\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout, device) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, block_size, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# --- Training Class ---\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, train_data, val_data):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.start_iter = 0\n",
    "        try:\n",
    "            print(f\"Checking for checkpoint: {CHECKPOINT_NAME}\")\n",
    "            checkpoint = torch.load(CHECKPOINT_NAME, map_location=device)\n",
    "            if (N_EMBD != checkpoint['n_embd'] or N_HEAD != checkpoint['n_head'] or N_LAYER != checkpoint['n_layer']):\n",
    "                raise ValueError(\"Hyperparameter mismatch.\")\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.start_iter = checkpoint['iter']\n",
    "            print(f\"Resuming training from iteration {self.start_iter}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found. Starting training from scratch.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Checkpoint could not be loaded. Starting from scratch. Error: {e}\")\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "        x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(EVAL_INTERVAL)\n",
    "            for k in range(EVAL_INTERVAL):\n",
    "                X, Y = self.get_batch(split)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits, loss = self.model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def run(self):\n",
    "        loss_steps.clear()\n",
    "        loss_values.clear()\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        start_time = time.time()\n",
    "        for iter in range(self.start_iter, MAX_ITERS):\n",
    "            if iter > 0 and iter % EVAL_INTERVAL == 0:\n",
    "                debug_loss = self.estimate_loss()\n",
    "                elapsed_time = time.time() - start_time\n",
    "                etc_seconds = (elapsed_time / (iter - self.start_iter + 1)) * (MAX_ITERS - iter)\n",
    "                print(f\"[{elapsed_time:.0f}s] step {iter}: train loss {debug_loss['train']:.4f}, val loss {debug_loss['val']:.4f} (ETC: {etc_seconds/60:.2f} min)\")\n",
    "                checkpoint = {\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'iter': iter,\n",
    "                    'vocab_size': vocab_size,\n",
    "                    'itos': itos,\n",
    "                    'n_embd': N_EMBD,\n",
    "                    'n_head': N_HEAD,\n",
    "                    'n_layer': N_LAYER,\n",
    "                    'block_size': BLOCK_SIZE,\n",
    "                    'dropout': DROPOUT\n",
    "                }\n",
    "                torch.save(checkpoint, CHECKPOINT_NAME)\n",
    "\n",
    "            xb, yb = self.get_batch('train')\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, loss = self.model(xb, yb)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_steps.append(iter)\n",
    "            loss_values.append(loss.log10().item())\n",
    "\n",
    "        print(f\"Training finished at iteration {MAX_ITERS}.\")\n",
    "        final_checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'iter': MAX_ITERS,\n",
    "            'vocab_size': vocab_size,\n",
    "            'itos': itos,\n",
    "            'n_embd': N_EMBD,\n",
    "            'n_head': N_HEAD,\n",
    "            'n_layer': N_LAYER,\n",
    "            'block_size': BLOCK_SIZE,\n",
    "            'dropout': DROPOUT\n",
    "        }\n",
    "        torch.save(final_checkpoint, CHECKPOINT_NAME)\n",
    "\n",
    "# --- Execution ---\n",
    "model = BigramLanguageModel(vocab_size, N_EMBD, N_HEAD, N_LAYER, BLOCK_SIZE, DROPOUT, device).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = Trainer(model, optimizer, train_data, val_data)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. INFERENCE ===\n",
    "# This cell can be run independently after a checkpoint has been saved.\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "if os.path.exists(CHECKPOINT_NAME):\n",
    "    print(f\"Loading checkpoint '{CHECKPOINT_NAME}' for inference...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_NAME, map_location=device)\n",
    "    \n",
    "    # Rebuild the tokenizer from the checkpoint\n",
    "    itos = checkpoint['itos']\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "    # Rebuild the model with the loaded hyperparameters\n",
    "    inference_model = BigramLanguageModel(\n",
    "        vocab_size=checkpoint['vocab_size'],\n",
    "        n_embd=checkpoint['n_embd'],\n",
    "        n_head=checkpoint['n_head'],\n",
    "        n_layer=checkpoint['n_layer'],\n",
    "        block_size=checkpoint['block_size'],\n",
    "        dropout=checkpoint['dropout'],\n",
    "        device=device\n",
    "    )\n",
    "    m_inference = inference_model.to(device)\n",
    "    m_inference.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m_inference.eval()\n",
    "\n",
    "    print(\"Model loaded. Generating text...\")\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(decode(m_inference.generate(context, block_size=checkpoint['block_size'], max_new_tokens=500)[0].tolist()))\n",
    "else:\n",
    "    print(f\"No checkpoint found named '{CHECKPOINT_NAME}'. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. PLOT LOSS ===\n",
    "plt.plot(loss_steps, loss_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}